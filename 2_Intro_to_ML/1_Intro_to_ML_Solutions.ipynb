{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83537d0-a528-4acf-810c-6556b920e183",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/2_Intro_to_ML/1_Intro_to_ML_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028fc6-114c-4d59-91a7-04725b8952b2",
   "metadata": {},
   "source": [
    "# Week 2 Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64833238-bd94-400d-b5d0-b70b5cf14b71",
   "metadata": {},
   "source": [
    "## What we will learn\n",
    "\n",
    "In this weeks colab we will introduce the Machine Learning library `scikit-learn` and practice some basic concepts of Machine Learning (ML), including:\n",
    "\n",
    "1. How to breakdown ML projects\n",
    "2. How to do classification with `scikit-learn`\n",
    "3. How to evaluate model performance\n",
    "4. How to do regression with `scikit-learn`\n",
    "5. How to do clustering with `scikit-learn`\n",
    "6. How to do dimensionality reduction with `scikit-learn`\n",
    "\n",
    "In the process you will also learn about:\n",
    "* Some classification algorithms, such as Nearest Neighbors, SVM, Decision Trees and Random Forest\n",
    "* Some regression algorithms, such as Linear Regression, and Nearest Neighbor Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1a21a-e9aa-4294-8ee4-d8582e899915",
   "metadata": {},
   "source": [
    "## 1. Intro [20 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f36176-c7d7-4c02-b125-647a774de8dc",
   "metadata": {},
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e46b4-a9e5-4d7a-9cf2-21f28924e6f0",
   "metadata": {},
   "source": [
    "**What is Machine Learning?**\n",
    "\n",
    "Teaching computers how to perform a task without having to explicitly program them to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9042a9-507b-4ce7-b84b-e9d3ac11c9e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> A computer program is said to learn from experience E with respect to some classes of task T and performance measure P if its performance can improve with E on T measured by P.\n",
    ">\n",
    "> M. T. Mitchell. 1997. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7b64e-af80-4b27-b33e-719acbf7e2ec",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad858-6ab4-4d26-b43f-e4125a76ab49",
   "metadata": {},
   "source": [
    "Butterfly recognition:\n",
    "\n",
    "* Task T: Classify images of British butterflies into different species\n",
    "* Performance measure P: percent of images that have been correctly classified\n",
    "* Training experience E: A database of butterfly images from museum collections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5ee4f-b5d8-4003-89b5-4a157fde2db2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**How does a computer program learn?**\n",
    "\n",
    "Using **data** to **parametrize** models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c21f57-3b9d-4a57-820b-4ad1d3e32163",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ML workflow\n",
    "\n",
    "Using ML for ecological inference is a multistep process. In practice it can generally be broken down into the following steps:\n",
    "\n",
    "* Data collection\n",
    "* Data preparation\n",
    "* Model training\n",
    "* Model evaluation\n",
    "* Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf2da2-9198-4319-ad47-3bbf30032304",
   "metadata": {},
   "source": [
    "![ml workflow](https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/2_Intro_to_ML/ml_workflow.png?raw=true)\n",
    "\n",
    "> Simplified workflow from: S. Amershi et al., [\"Software Engineering for Machine Learning: A Case Study,\"](https://ieeexplore.ieee.org/abstract/document/8804457) 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), Montreal, QC, Canada, 2019, pp. 291-300, doi: 10.1109/ICSE-SEIP.2019.00042."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe74165-d0f9-435f-8543-626a5b9d04a8",
   "metadata": {},
   "source": [
    "The previous figure serves as a guideline indicating the usual flow, but it is not uncommon to work simultaneously on multiple steps.\n",
    "\n",
    "Working in cycles is recommended. Model evaluation will inform if you need more data, clean the data further or change the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f676e3-aeda-4a7c-ae74-f36b68b7792f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Each step can be subdivided further:\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "* Design your ML goals\n",
    "* Determine what data (and ideally how much) you will need.\n",
    "* Collect data from multiple sources\n",
    "    * Field studies\n",
    "    * Open-source datasets\n",
    "    * Web scraping\n",
    "    * Citizen science\n",
    "    * Colaboration\n",
    "\n",
    "**Data Preparation**\n",
    "\n",
    "* Select feature set\n",
    "* Clean dataset, fix errors, decide what to do with missing values\n",
    "* Normalize variables\n",
    "* Annotate or label data\n",
    "* Split data into training and test datasets\n",
    "\n",
    "**Model Training**\n",
    "\n",
    "* Select an adequate ML model\n",
    "* Train model and validate\n",
    "* Finetune hyperparameters\n",
    "\n",
    "**Model Evaluation**\n",
    "\n",
    "* Compute metrics\n",
    "* Visualize predictions\n",
    "* Study failure cases\n",
    "* Identify weak spots\n",
    "* Compare with baselines\n",
    "\n",
    "**Make predictions**\n",
    "\n",
    "* Use model to process novel data\n",
    "* Use predictions to make ecological inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea068d60-1e20-409f-bdbe-a4138c5d65de",
   "metadata": {},
   "source": [
    "Now you will step through some practical examples of some of the steps listed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a53fb-b031-4d4d-ac25-7c3170709a3f",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6460da-8338-41a4-bb7e-69a4a4f8fb82",
   "metadata": {},
   "source": [
    "`scikit-learn` is a Python library for Machine Learning. \n",
    "\n",
    "It offers a wide array of algorithms for several ML tasks, and tools to setup ML pipelines.\n",
    "\n",
    "![scikit-learn](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/260px-Scikit_learn_logo_small.svg.png?20180808062052)\n",
    "\n",
    "You can learn about `scikit-learn` in its official [documentation](https://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dff65b-94df-44ba-84e1-1dead7bff9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn  # notice scikit-learn is imported as sklearn.\n",
    "\n",
    "# scikit-learn will be preinstalled in colab environments\n",
    "\n",
    "# print the currently installed scikit-learn version\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458f059-8d89-41c4-ade8-064cfeae9f6d",
   "metadata": {},
   "source": [
    "## 2. Classification [40 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb68d1-cc5e-472c-ba8e-c5c942d092bd",
   "metadata": {},
   "source": [
    "Suppose you need to automate the following task\n",
    "\n",
    "**Task**: Identify Iris flower species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c511f6-3040-4936-82fb-d46b3d5376d2",
   "metadata": {},
   "source": [
    "How would you describe this Iris flower?\n",
    "\n",
    "![iris](https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Iris_germanica_%28Purple_bearded_Iris%29%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg/470px-Iris_germanica_%28Purple_bearded_Iris%29%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg?20140528110728)\n",
    "\n",
    "* Colour?\n",
    "* Number of stripes?\n",
    "* Size?\n",
    "* Weight?\n",
    "* Environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973007c7-31bc-480c-96d4-6a9cdfa71220",
   "metadata": {},
   "source": [
    "**What are features?**\n",
    "\n",
    "They are numerical or categorical descriptors, attributes or traits of the object of study.\n",
    "\n",
    "> A **feature** is an individual measurable property or characteristic of a phenomenon\n",
    ">\n",
    "> *Bishop, Christopher (2006). Pattern recognition and machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7ae2a-895a-4522-98e9-95e2008cad63",
   "metadata": {},
   "source": [
    "In the case of Iris flowers, lets use sepal and petal length and width\n",
    "\n",
    "![iris sepal/petal length/width](https://ars.els-cdn.com/content/image/3-s2.0-B9780128147610000034-f03-01-9780128147610.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3ae91-fa27-4b70-bf2a-b36ae4e8a15b",
   "metadata": {},
   "source": [
    "Feature vector - Feature (descriptor):\n",
    "\n",
    "    x = (sepal_length, sepal_width, petal_length, petal_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b07eb8-820d-469e-9a44-ef1b3251b6e9",
   "metadata": {},
   "source": [
    "**Where to get Iris flower data?**\n",
    "\n",
    "A dataset of iris measurements is publicly available. Find a description [here](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "Lets load a dataset of Iris flower measurements using `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2e229-eaa6-4f96-bd56-b71b8fb9c06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scikit-learn offers toy datasets including the iris dataset.\n",
    "# load the dataset functions from scikit-learn.\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a2e7e-a7e7-4af3-9879-28232343f447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the iris dataset\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# extract the features table\n",
    "iris_data = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2ba57-e5fb-4a72-84a4-64a6dac8d3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the first rows\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c07ec2-0b7c-463f-b8cb-089581ebaa5c",
   "metadata": {},
   "source": [
    "**What is a classification task?**\n",
    "\n",
    "**Recall**: Supervised learning is one of the main approaches of Machine Learning. It consists of trying to predict a **target** variable using **features** as predictors.\n",
    "\n",
    "It is a supervised learning task where the target variable is a categorical variable, that is when trying to predict a class based on features.\n",
    "\n",
    "For the Iris dataset, given our feature vector **x**, can we predict the correct species (i.e. class label) **y**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80f71f-3ab8-4791-acfe-bef4ecd3a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the classification targets\n",
    "iris_target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf3053-3cd7-4e9c-944f-23bf74cc1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first values of iris_target\n",
    "iris_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76146ca7-c357-4b38-bba8-8ae187271fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows per target class\n",
    "iris_target.value_counts()\n",
    "\n",
    "# notice the target class is codified as a integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eb01f-6387-4658-b3de-ad51f809c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class names\n",
    "species_names = iris.target_names\n",
    "\n",
    "# print the correspondence between integer values and species names\n",
    "for index, name in enumerate(species_names):\n",
    "    print(f\"class {index} = {name}\")\n",
    "\n",
    "# map the target integer values to species names\n",
    "y_iris = iris_target.apply(lambda index: species_names[index])\n",
    "\n",
    "y_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ba705-ecb1-4b45-aa69-19ae573469bb",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ee27b-9da8-42e8-a914-61de12f31436",
   "metadata": {},
   "source": [
    "Select two features: petal length and petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d17d7e-9e42-47e8-a9c5-dad410ab880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = \"petal length (cm)\"\n",
    "feature_2 = \"petal width (cm)\"\n",
    "X_iris = iris_data[[feature_1, feature_2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee64f34-52a3-4260-851d-dd324e37a1c1",
   "metadata": {},
   "source": [
    "Each datapoint has some <span style=\"color: deepskyblue;\">features</span> and a <span style=\"color: coral;\">class label</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537015a3-b9bb-4a1a-9826-3b992183bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn and matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1394ef-699b-4f33-92c0-04e7cbd5d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot each data point (x = feature1, y = feature2) and the species in color\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "# select a single data point\n",
    "sample = iris_data.iloc[60]\n",
    "\n",
    "# add text to point to single data point\n",
    "ax.annotate(\n",
    "    f\"({feature_1}, {feature_2}), Species\",\n",
    "    (sample[feature_1] + 0.1, sample[feature_2] - 0.05),\n",
    "    xytext=(sample[feature_2] + 0.5, sample[feature_2] - 0.3),\n",
    "    fontsize=12,\n",
    "    arrowprops={\n",
    "        \"width\": 1,\n",
    "        \"headwidth\": 6,\n",
    "        \"headlength\": 6,\n",
    "        \"edgecolor\": \"black\",\n",
    "        \"facecolor\": \"black\",\n",
    "    },\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e30ab2-9a29-4f34-ad8d-efb85540eea9",
   "metadata": {},
   "source": [
    "Given a <span style=\"color: deepskyblue;\">new</span> datapoint, how can we determine its <span style=\"color: coral\">class</span>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cb68c-330e-4e87-891d-eb3f3c45ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new test point\n",
    "test_point = [3.8, 1.6]  # petal length cm, petal width cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd3cbb-5c66-413c-be9c-ebdcb14bea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot the species type in color\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "# plot the new test point\n",
    "ax.scatter(x=[test_point[0]], y=[test_point[1]], color=\"deepskyblue\")\n",
    "\n",
    "# add \"new\" text and arrow pointing at new test point\n",
    "ax.annotate(\n",
    "    \"New\",\n",
    "    (test_point[0] - 0.05, test_point[1] + 0.02),\n",
    "    xytext=(test_point[0] - 1, test_point[1] + 0.3),\n",
    "    fontsize=12,\n",
    "    color=\"deepskyblue\",\n",
    "    arrowprops={\n",
    "        \"width\": 1,\n",
    "        \"headwidth\": 6,\n",
    "        \"headlength\": 6,\n",
    "        \"edgecolor\": \"deepskyblue\",\n",
    "        \"facecolor\": \"deepskyblue\",\n",
    "    },\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a4929-07a9-4d49-8825-6a3add361b90",
   "metadata": {},
   "source": [
    "**Simple idea**: Assign the class of the nearest point in the dataset.\n",
    "\n",
    "How to find the nearest point in our dataset to a given test point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b96f0f-2e79-4181-a395-22b439f72653",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot the species type in color\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    "    s=50,\n",
    "    zorder=2,\n",
    ")\n",
    "\n",
    "# plot a line from the test point to each point in the dataset\n",
    "for _, flower in iris_data.iterrows():\n",
    "    ax.plot(\n",
    "        [test_point[0], flower[feature_1]],\n",
    "        [test_point[1], flower[feature_2]],\n",
    "        color=\"gray\",\n",
    "        linewidth=1,\n",
    "        alpha=0.5,\n",
    "        zorder=1,\n",
    "    )\n",
    "\n",
    "# plot the test point\n",
    "ax.scatter(\n",
    "    x=[test_point[0]],\n",
    "    y=[test_point[1]],\n",
    "    color=\"deepskyblue\",\n",
    "    s=100,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f3d95-0171-430f-bba1-54aa813c36b2",
   "metadata": {},
   "source": [
    "Compute **similarity** between two feature points.\n",
    "\n",
    "Use *Euclidean* distance (based on the pythagorean theorem)\n",
    "\n",
    "![euclidean distance](https://upload.wikimedia.org/wikipedia/commons/5/55/Euclidean_distance_2d.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9cd21-529e-48b8-9ce5-901b88455e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for math functions\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# implementation of distance between two points\n",
    "def compute_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698501d8-0099-40f2-adf9-ff9657c1ee03",
   "metadata": {},
   "source": [
    "And find the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd332796-2d9a-4190-b2ad-2ad8fd04251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distance from the test point to every example in the dataset\n",
    "distance_to_test_point = iris_data.apply(\n",
    "    lambda row: compute_distance(test_point, [row[feature_1], row[feature_2]]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# print the first results\n",
    "distance_to_test_point.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbc8af-fc14-4438-9a90-af615d2f0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the point in the dataset that is closest to the test point\n",
    "# and record its distance\n",
    "\n",
    "# get index where distance is minimized\n",
    "index_of_min_distance = distance_to_test_point.argmin()\n",
    "\n",
    "# retrieve the corresponding data point\n",
    "closest_point = iris_data.iloc[index_of_min_distance]\n",
    "\n",
    "# compute minimum distance\n",
    "distance_to_closest = distance_to_test_point.min()\n",
    "\n",
    "print(\"Distance to closest point: \", distance_to_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d1e7f-1ce3-45be-abb7-9ee12fd203d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dataset\n",
    "sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    ")\n",
    "\n",
    "# plot the test point as an 'x'\n",
    "sns.scatterplot(\n",
    "    x=[test_point[0]],\n",
    "    y=[test_point[1]],\n",
    "    marker=\"x\",\n",
    "    label=\"test point\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plot a ring around the nearest datapoint\n",
    "sns.scatterplot(\n",
    "    x=[closest_point[feature_1]],\n",
    "    y=[closest_point[feature_2]],\n",
    "    marker=\"o\",\n",
    "    label=\"nearest training point\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd226d-cc16-454b-8ee1-f17bb4cb00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the test point is the same class as the datapoint it is closest to\n",
    "predicted_species = y_iris[index_of_min_distance]\n",
    "print(f\"Predicted species: {predicted_species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba28ad7-2b79-4da6-be17-f320de20fdd4",
   "metadata": {},
   "source": [
    "**Summary: The nearest neighbor algorithm**\n",
    "\n",
    "1. Given a test point x\n",
    "2. Compute the distance between x and every other datapoint\n",
    "3. The class of x is set as the same as the closest datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae90ad0-8df1-498f-b65d-c682080d24e5",
   "metadata": {},
   "source": [
    "Here is a quick implementation of the nearest neighbor algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb3f25-7403-4221-9b2d-a4405cc3e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(test_point):\n",
    "    # compute the distance from the test point to every example in the dataset\n",
    "    distance = iris_data.apply(\n",
    "        lambda row: compute_distance(test_point, [row[feature_1], row[feature_2]]),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # get index where distance is minimum\n",
    "    index_of_min_distance = distance.argmin()\n",
    "\n",
    "    # find the point in the dataset that is closest to the test point\n",
    "    closest_point = iris_data.iloc[index_of_min_distance]\n",
    "\n",
    "    # assume the test point is the same class as the datapoint it is closest to\n",
    "    predicted_species = y_iris[index_of_min_distance]\n",
    "\n",
    "    return predicted_species, closest_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803839f-2409-463c-a9f9-007f8f5d4be8",
   "metadata": {},
   "source": [
    "Scikit-learn also provides an easy way of building Nearest Neighbor Classification Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfda6ad-605e-4284-a975-7b446fd843dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the KNeighborsClassifier from scikit learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create a model instance\n",
    "knn_model_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit model with the iris dataset\n",
    "knn_model_1.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509eadb-9f09-4f07-9e0a-4620f5dfdd71",
   "metadata": {},
   "source": [
    "The model can be used to make inference on new points. Lets try it out on a new test point, make sure we get the same results as before, and plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965239fc-e18d-449b-8abf-c75662eaa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it to predict the species of a test point\n",
    "test_point_2 = np.array([2.1, 0.7])\n",
    "\n",
    "predicted_species_2 = knn_model_1.predict(test_point_2.reshape(1, -1))[0]\n",
    "\n",
    "print(predicted_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23d030-9826-4b26-942f-d7f7a8af7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure it coincides with our algorithm\n",
    "predicted_species_2_ours, closest_point_2 = nearest_neighbour(test_point_2)\n",
    "print(f\"Predictions are equal = {predicted_species_2 == predicted_species_2_ours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19f56f-8a20-456f-a165-e54e947ae1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    ")\n",
    "\n",
    "# plot the test point as an 'x'\n",
    "sns.scatterplot(\n",
    "    x=[test_point_2[0]],\n",
    "    y=[test_point_2[1]],\n",
    "    marker=\"x\",\n",
    "    label=\"test point\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plot a ring around the nearest datapoint\n",
    "sns.scatterplot(\n",
    "    x=[closest_point_2[feature_1]],\n",
    "    y=[closest_point_2[feature_2]],\n",
    "    marker=\"o\",\n",
    "    label=\"nearest training point\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779a0ca-6da9-446b-a4d4-4bb87ce92da5",
   "metadata": {},
   "source": [
    "The model predicts some species to every point in feature space. \n",
    "\n",
    "**Decision regions** are formed by points that are assigned to the same species. \n",
    "\n",
    "The regions are separated by **decision boundaries** where the model is unsure what class to assign.\n",
    "\n",
    "The next cell contains some function definitions to plot decision regions and boundaries. You can safely ignore it, but make sure to run the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7b577-32a4-4ca5-b6ea-68912c5fa913",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title plot decision boundary function definition\n",
    "\n",
    "# IGNORE: here we define functions to plot the decision boundary.\n",
    "# Their implementation is not relevant.\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn.utils.validation import _is_arraylike, _num_features\n",
    "\n",
    "\n",
    "def _is_arraylike_not_scalar(array):\n",
    "    return _is_arraylike(array) and not np.isscalar(array)\n",
    "\n",
    "\n",
    "def _check_boundary_response_method(estimator, response_method):\n",
    "    has_classes = hasattr(estimator, \"classes_\")\n",
    "    if has_classes and _is_arraylike_not_scalar(estimator.classes_[0]):\n",
    "        msg = \"Multi-label and multi-output multi-class classifiers are not supported\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if has_classes and len(estimator.classes_) > 2:\n",
    "        if response_method not in {\"auto\", \"predict\"}:\n",
    "            msg = (\n",
    "                \"Multiclass classifiers are only supported when response_method is\"\n",
    "                \" 'predict' or 'auto'\"\n",
    "            )\n",
    "            raise ValueError(msg)\n",
    "        methods_list = [\"predict\"]\n",
    "    elif response_method == \"auto\":\n",
    "        methods_list = [\"decision_function\", \"predict_proba\", \"predict\"]\n",
    "    else:\n",
    "        methods_list = [response_method]\n",
    "\n",
    "    prediction_method = [getattr(estimator, method, None) for method in methods_list]\n",
    "    prediction_method = reduce(lambda x, y: x or y, prediction_method)\n",
    "    if prediction_method is None:\n",
    "        raise ValueError(\n",
    "            f\"{estimator.__class__.__name__} has none of the following attributes: \"\n",
    "            f\"{', '.join(methods_list)}.\"\n",
    "        )\n",
    "\n",
    "    return prediction_method\n",
    "\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    estimator,\n",
    "    X,\n",
    "    *,\n",
    "    grid_resolution=100,\n",
    "    eps=1.0,\n",
    "    plot_method=\"contourf\",\n",
    "    response_method=\"auto\",\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    ax=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if not grid_resolution > 1:\n",
    "        raise ValueError(\n",
    "            \"grid_resolution must be greater than 1. Got\" f\" {grid_resolution} instead.\"\n",
    "        )\n",
    "\n",
    "    if not eps >= 0:\n",
    "        raise ValueError(f\"eps must be greater than or equal to 0. Got {eps} instead.\")\n",
    "\n",
    "    possible_plot_methods = (\"contourf\", \"contour\", \"pcolormesh\")\n",
    "    if plot_method not in possible_plot_methods:\n",
    "        available_methods = \", \".join(possible_plot_methods)\n",
    "        raise ValueError(\n",
    "            f\"plot_method must be one of {available_methods}. \"\n",
    "            f\"Got {plot_method} instead.\"\n",
    "        )\n",
    "\n",
    "    num_features = _num_features(X)\n",
    "    if num_features != 2:\n",
    "        raise ValueError(f\"n_features must be equal to 2. Got {num_features} instead.\")\n",
    "\n",
    "    x0, x1 = _safe_indexing(X, 0, axis=1), _safe_indexing(X, 1, axis=1)\n",
    "\n",
    "    x0_min, x0_max = x0.min() - eps, x0.max() + eps\n",
    "    x1_min, x1_max = x1.min() - eps, x1.max() + eps\n",
    "\n",
    "    xx0, xx1 = np.meshgrid(\n",
    "        np.linspace(x0_min, x0_max, grid_resolution),\n",
    "        np.linspace(x1_min, x1_max, grid_resolution),\n",
    "    )\n",
    "\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        # we need to preserve the feature names and therefore get an empty dataframe\n",
    "        X_grid = X.iloc[[], :].copy()\n",
    "        X_grid.iloc[:, 0] = xx0.ravel()\n",
    "        X_grid.iloc[:, 1] = xx1.ravel()\n",
    "    else:\n",
    "        X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "\n",
    "    pred_func = _check_boundary_response_method(estimator, response_method)\n",
    "    response = pred_func(X_grid)\n",
    "\n",
    "    # convert classes predictions into integers\n",
    "    if pred_func.__name__ == \"predict\" and hasattr(estimator, \"classes_\"):\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.classes_ = estimator.classes_\n",
    "        response = encoder.transform(response)\n",
    "\n",
    "    if response.ndim != 1:\n",
    "        if is_regressor(estimator):\n",
    "            raise ValueError(\"Multi-output regressors are not supported\")\n",
    "\n",
    "        # TODO: Support pos_label\n",
    "        response = response[:, 1]\n",
    "\n",
    "    if xlabel is None:\n",
    "        xlabel = X.columns[0] if hasattr(X, \"columns\") else \"\"\n",
    "\n",
    "    if ylabel is None:\n",
    "        ylabel = X.columns[1] if hasattr(X, \"columns\") else \"\"\n",
    "\n",
    "    if plot_method not in (\"contourf\", \"contour\", \"pcolormesh\"):\n",
    "        raise ValueError(\"plot_method must be 'contourf', 'contour', or 'pcolormesh'\")\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    plot_func = getattr(ax, plot_method)\n",
    "\n",
    "    surface_ = plot_func(xx0, xx1, response.reshape(xx0.shape), **kwargs)\n",
    "\n",
    "    if xlabel is not None or not ax.get_xlabel():\n",
    "        xlabel = xlabel if xlabel is None else xlabel\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "    if ylabel is not None or not ax.get_ylabel():\n",
    "        ylabel = ylabel if ylabel is None else ylabel\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156232c-5df8-4cc9-a737-c89a6c370c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap([\"lightblue\", \"peachpuff\", \"palegreen\"])\n",
    "\n",
    "# plot the decision regions\n",
    "ax = plot_decision_boundary(knn_model_1, X_iris, cmap=cmap_light)\n",
    "\n",
    "# plot the decision boundary\n",
    "ax = plot_decision_boundary(\n",
    "    knn_model_1,\n",
    "    X_iris,\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    levels=[0, 1],\n",
    "    colors=\"black\",\n",
    ")\n",
    "\n",
    "# overlay data points from iris dataset\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247f41a-50a1-484c-bdd0-1acf7d826232",
   "metadata": {},
   "source": [
    "Now you will see some problems with the nearest neighbor algorithm. These are:\n",
    "\n",
    "1. Overconfidence\n",
    "2. Memory and Speed\n",
    "3. Sensitive to Noise\n",
    "4. Sensitive to changes in Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5e49a-069a-4b33-9242-0707e29553b9",
   "metadata": {},
   "source": [
    "**Overconfidence**\n",
    "\n",
    "When making inference on a point far from the training points, the model can be very confident about its prediction.\n",
    "\n",
    "Example: the following test point is much closer to any *virginica* point than any other species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445901d-b6f8-4668-97f4-a7fe8b9e533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new test point far from all other data points\n",
    "test_point_3 = [10, 8]\n",
    "\n",
    "_, closest_point_3 = nearest_neighbour(test_point_3)\n",
    "\n",
    "# plot training dataset\n",
    "sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    ")\n",
    "\n",
    "# plot the test point as an 'x'\n",
    "sns.scatterplot(\n",
    "    x=[test_point_3[0]],\n",
    "    y=[test_point_3[1]],\n",
    "    marker=\"x\",\n",
    "    label=\"test point\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plot a ring around the nearest datapoint\n",
    "sns.scatterplot(\n",
    "    x=[closest_point_3[feature_1]],\n",
    "    y=[closest_point_3[feature_2]],\n",
    "    marker=\"o\",\n",
    "    label=\"nearest training point\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a69ab5-85d5-4ff6-828f-70cd3483d8c9",
   "metadata": {},
   "source": [
    "Is this a reasonable behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b586c8-a65e-4d84-8a6c-aadb1c8c9013",
   "metadata": {},
   "source": [
    "**Memory and speed**\n",
    "\n",
    "The Nearest Neighbor model needs to store all points in the training dataset. \n",
    "\n",
    "Additionally, at inference it computes every distance from the test point to the training set points.\n",
    "\n",
    "For large datasets it becomes cumbersome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87480c8-24b5-4bb7-8803-2165085165ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a performance timer from the standard library\n",
    "from time import perf_counter\n",
    "\n",
    "test_point_4 = np.array([4, 4])\n",
    "\n",
    "\n",
    "def measure_nearest_neighbor_speed(n_samples, test_point=test_point_4):\n",
    "    # generate toy dataset with scikit-learn dataset functions.\n",
    "    X, y = datasets.make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,  # Two features\n",
    "        n_informative=2,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,  # Two classes\n",
    "    )\n",
    "\n",
    "    # create a Nearest Neighbor model\n",
    "    model = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\", n_jobs=1)\n",
    "\n",
    "    # measure fit time\n",
    "    start_fit = perf_counter()\n",
    "    model.fit(X, y)\n",
    "    fit_time = perf_counter() - start_fit\n",
    "\n",
    "    # measure prediction time\n",
    "    start_predict = perf_counter()\n",
    "    model.predict(test_point.reshape(1, -1))\n",
    "    predict_time = perf_counter() - start_predict\n",
    "\n",
    "    return fit_time, predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76203342-bcfa-4bae-9b3e-b434c522ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select different sizes of dataset.\n",
    "# logspace will return exponentially separated points.\n",
    "# 10, 100, 1000, ..., 10E8\n",
    "dataset_sizes = np.logspace(start=1, stop=8, num=8, dtype=np.int32)\n",
    "\n",
    "# measure time to fit and predict for each dataset size\n",
    "fit_times, predict_times = zip(\n",
    "    *[measure_nearest_neighbor_speed(n_samples) for n_samples in dataset_sizes]\n",
    ")\n",
    "\n",
    "# plot times\n",
    "plt.plot(dataset_sizes, fit_times, label=\"fit\")\n",
    "plt.plot(dataset_sizes, predict_times, label=\"predict\")\n",
    "\n",
    "# make axis logarithmic\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# add axis labels\n",
    "plt.ylabel(\"duration (s)\")\n",
    "plt.xlabel(\"dataset size (# samples)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c35bb-646a-4a97-b4e6-cf65ee4d6bda",
   "metadata": {},
   "source": [
    "**Noise sensitivity**\n",
    "\n",
    "If a single mistake is introduced in the dataset the **decision boundaries** can change drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031c043-c5a6-4103-8107-6785dbf203e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the species of the 134th element is virginica\n",
    "index = 134\n",
    "print(y_iris[index])\n",
    "\n",
    "# make a copy of the targets\n",
    "y_iris_corrupted = y_iris.copy()\n",
    "\n",
    "# and change the label of a single entry\n",
    "y_iris_corrupted[index] = \"versicolor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d500557-8389-4d43-b884-e91fb87dfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new NN model with corrupted labels\n",
    "knn_model_with_corrupted_data = KNeighborsClassifier(n_neighbors=1).fit(\n",
    "    X_iris, y_iris_corrupted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1c0ae-aee2-44f6-86fb-93bb28a44065",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# create a subplot on the left\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"Decision regions of original model\")\n",
    "\n",
    "# plot the decision regions and boundary of original model\n",
    "plot_decision_boundary(knn_model_1, X_iris, cmap=cmap_light, ax=ax1)\n",
    "plot_decision_boundary(\n",
    "    knn_model_1,\n",
    "    X_iris,\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax1,\n",
    "    levels=[0, 1],\n",
    "    colors=\"black\",\n",
    ")\n",
    "\n",
    "# overlay data points from iris dataset\n",
    "sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "# create a subplot on the right\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title(\"Decision regions of model with one corrupted label\")\n",
    "\n",
    "# plot the decision regions and boundary of model with corrupted data\n",
    "plot_decision_boundary(\n",
    "    knn_model_with_corrupted_data,\n",
    "    X_iris,\n",
    "    cmap=cmap_light,\n",
    "    ax=ax2,\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "    knn_model_with_corrupted_data,\n",
    "    X_iris,\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax2,\n",
    "    levels=[0, 1],\n",
    "    colors=\"black\",\n",
    ")\n",
    "\n",
    "# overlay data points from iris dataset\n",
    "sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris_corrupted,\n",
    "    style=y_iris_corrupted,\n",
    "    ax=ax2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6408-146c-450f-a2c7-3d3023bfd740",
   "metadata": {},
   "source": [
    "**Sensitivity to scale**\n",
    "\n",
    "So far we have been using **cm** as units for length.\n",
    "\n",
    "What happens if we change cm to meters for a single feature? \n",
    "\n",
    "How does this choice affect predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cc9d-fbc5-4b28-a616-98592a6fb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the feature array so that the first feature is in meters\n",
    "X_iris_2 = X_iris.copy()\n",
    "X_iris_2[\"petal length (m)\"] = X_iris[\"petal length (cm)\"] / 100\n",
    "X_iris_2 = X_iris_2.reindex(columns=[\"petal length (m)\", \"petal width (cm)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab03ee3-9044-46d6-887b-4ffbcb80feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with new feature array\n",
    "knn_model_2 = KNeighborsClassifier(n_neighbors=1).fit(X_iris_2, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a998c-485b-478b-8398-7e8d60d153c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wish to classify a new flower. These are the measurements of the flower with\n",
    "# two different units of measurements\n",
    "test_point_5_cm = np.array([2.7, 0.7])  # (petal length cm, petal width cm)\n",
    "test_point_5_m = np.array([0.027, 0.7])  # (petal length m, petal width cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0562c-ba99-4cb3-b6c5-0388f22d6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with cm\n",
    "predicted_species_cm = knn_model_1.predict(test_point_5_cm.reshape(1, -1))[0]\n",
    "\n",
    "# predict with meters\n",
    "predicted_species_m = knn_model_2.predict(test_point_5_m.reshape(1, -1))[0]\n",
    "\n",
    "print(\n",
    "    f\"Species predictions: with cm as units = {predicted_species_cm}, with m as units = {predicted_species_m}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea919e-7ec2-48f3-b92a-46c5508a0556",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "The nearest neighbor algorithm is intuitive and simple to implement. But \n",
    "\n",
    "* is very sensitive to errors and scaling (recall underfitting/overfitting discussion?)\n",
    "* requires lots of memory and computation\n",
    "\n",
    "What are some alternatives? \n",
    "\n",
    "Is there a way that involves less computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d403065-6c3f-46e3-b81a-630b1357d674",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26b421-320b-43b7-884b-ec7eae5a7346",
   "metadata": {},
   "source": [
    "Lets simplify our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b2bb6-62c9-47a3-845f-f5aa0dd8d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only data of the setosa and virginica species\n",
    "separable_data = iris_data[y_iris.isin([\"setosa\", \"versicolor\"])]\n",
    "\n",
    "# Select features and labels for the filtered dataset\n",
    "X_separable = separable_data[[feature_1, feature_2]]\n",
    "y_separable = y_iris[y_iris.isin([\"setosa\", \"versicolor\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88608cbb-09c6-4049-b98b-d72ae4d4c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data points\n",
    "sns.scatterplot(\n",
    "    data=separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_separable,\n",
    "    style=y_separable,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378921a-8edb-4f15-bf60-e826ae944df0",
   "metadata": {},
   "source": [
    "Data points from the different species are clearly separated. \n",
    "\n",
    "They can be separated by a line and new points can be classified depending on which side of the line they fall.\n",
    "\n",
    "In essence this is how a linear classifier operates. \n",
    "\n",
    "Here we will use the linear support vector classifier (SVC).\n",
    "\n",
    "See [here](https://scikit-learn.org/stable/modules/svm.html#svc) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24a225-6d3a-4034-9a7a-6d023c1c2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Linear SVC model from the SVM module in scikit-learn\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdd40a-87c6-4a86-a672-93cd1f565c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear support vector classifier (SVC) on the separable dataset\n",
    "linear_svc_separable = LinearSVC().fit(\n",
    "    X_separable,\n",
    "    y_separable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ceab1-3e57-46f0-9732-778ea40312ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision regions of the linear classifier\n",
    "ax = plot_decision_boundary(\n",
    "    linear_svc_separable,\n",
    "    X_separable,\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# plot the decision boundary of the linear classifier\n",
    "plot_decision_boundary(\n",
    "    linear_svc_separable,\n",
    "    X_separable,\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    colors=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_separable,\n",
    "    style=y_separable,\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8938b23-9a59-4608-89bf-16f40f466859",
   "metadata": {},
   "source": [
    "The linear model is small in memory and very fast.\n",
    "\n",
    "But in most cases it will not generate perfect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775844c5-9731-4730-865d-abe45e762f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only data of the versicolor and virginica species\n",
    "non_separable_data = iris_data[y_iris.isin([\"virginica\", \"versicolor\"])]\n",
    "\n",
    "X_non_separable = non_separable_data[[feature_1, feature_2]]\n",
    "y_non_separable = y_iris[y_iris.isin([\"virginica\", \"versicolor\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a70256-1560-4f51-8c06-6c1a23c24734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0692-d6ef-45bd-bd5e-4008e0de799b",
   "metadata": {},
   "source": [
    "No line cuts the two sets of points cleanly.\n",
    "\n",
    "When we fit a linear model prediction errors are unavoidable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91f1c2-c1b7-47a0-aade-46e41ac1f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear support vector classifier (SVC) on the separable dataset\n",
    "linear_svc_non_separable = LinearSVC().fit(\n",
    "    X_non_separable,\n",
    "    y_non_separable,\n",
    ")\n",
    "\n",
    "# plot the decision regions of the linear classifier\n",
    "ax = plot_decision_boundary(\n",
    "    linear_svc_non_separable,\n",
    "    X_non_separable,\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# plot the decision boundary of the linear classifier\n",
    "plot_decision_boundary(\n",
    "    linear_svc_non_separable,\n",
    "    X_non_separable,\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c93936-03cf-4248-af68-00fd33f278e2",
   "metadata": {},
   "source": [
    "Support vector machines can be non-linear by applying the so called kernel-trick. \n",
    "\n",
    "If interested in details check the [An Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4d9fd-d070-441d-b4ad-d8f5449ed9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the support vector classifier model from scikit-learn\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5ffd4-46d9-42cd-bdd0-8eea786424d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a non-linear support vector machine on the non-linearly-separable dataset\n",
    "non_linear_svc = SVC(C=100, gamma=10).fit(\n",
    "    X_non_separable,\n",
    "    y_non_separable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ecd5c-298e-4e23-b83c-076c0a6fa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision regions of the non-linear svm classifier\n",
    "ax = plot_decision_boundary(\n",
    "    non_linear_svc,\n",
    "    X_non_separable,\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# plot the decision boundary of the non-linear svm classifier\n",
    "plot_decision_boundary(\n",
    "    non_linear_svc,\n",
    "    X_non_separable,\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4eb25-34bd-464f-a72c-6c3af3a476b7",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4a1b6-7da9-459f-90f8-0660472e7b5c",
   "metadata": {},
   "source": [
    "**Another simple idea**: Use simple binary decisions to discriminate between points. Use a sequence or tree of decisions to bin a test point into its correct species.\n",
    "\n",
    "Example binary decision: whether the petal length ≤ 5.1 cm, or the petal width is ≤ 1.75 cm.\n",
    "\n",
    "Can be nested: If petal length > 5.2 cm, and petal width < 1.3 then predict setosa.\n",
    "\n",
    "Each decision splits feature space in two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24caa0-606a-4a6d-a53a-d1513b283366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dataset points\n",
    "ax = sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    ")\n",
    "\n",
    "# draw horizontal line at y = 1.75\n",
    "ax.axhline(1.75, color=\"gray\", linewidth=3, alpha=0.2)\n",
    "\n",
    "# draw vertical line at x = 4.95\n",
    "ax.axvline(4.95, color=\"blue\", linewidth=3, alpha=0.2, ymax=0.5)\n",
    "\n",
    "# add text to label regions\n",
    "ax.text(4.2, 2.15, f\"width >= 1.75\")\n",
    "ax.text(2.9, 1.5, f\"width < 1.75\\n& length < 4.95\")\n",
    "ax.text(5.5, 1.1, f\"width < 1.75\\n& length >= 4.95\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35493f25-b8dc-4f8a-846d-01a90c1b25d4",
   "metadata": {},
   "source": [
    "Decision trees for classification can be created algorithmically. \n",
    "\n",
    "Multiple algorithms are available, here we will use the default algorithm from `scikit-learn` (ID3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f5693-74d7-43db-914c-f7334231f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Decision Tree Classifier model from scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81889cfb-cebe-469d-ac78-27ef2b5ea302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Decision Tree classifier model\n",
    "decision_tree = DecisionTreeClassifier(max_depth=2, min_impurity_decrease=0.01)\n",
    "# check the scikit-learn documentation to see possible configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ab2e4-cf1a-4a2c-8276-a595c96690f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to dataset\n",
    "decision_tree.fit(\n",
    "    X_non_separable,\n",
    "    y_non_separable,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9e2e2-2874-4a88-9e0a-9c54b3c67133",
   "metadata": {},
   "source": [
    "Decision trees are easily interpretable as it is possible to understand the reason behind a model prediction.\n",
    "\n",
    "You can visualise the whole decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbded4b3-1b75-4c52-b37b-954591df573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plot_tree function from tree tools in scikit-learn\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# create a new figure\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# visualize the trained decision trees\n",
    "plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=[feature_1, feature_2],\n",
    "    class_names=decision_tree.classes_,\n",
    "    impurity=False,\n",
    "    label=\"root\",\n",
    "    rounded=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Add labels to decisions\n",
    "ax.text(0.43, 0.66, \"yes\")\n",
    "ax.text(0.73, 0.66, \"no\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29482d5-d1ee-4c93-9b61-6f45b180fc2c",
   "metadata": {},
   "source": [
    "Here is the region boundary plot for the fitted decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2010422-6e10-44ec-96d3-7e030ce0b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision boundary of the decision tree classifier\n",
    "ax = plot_decision_boundary(\n",
    "    decision_tree,\n",
    "    X_non_separable,\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# plot the decision boundary of the decision tree classifier\n",
    "plot_decision_boundary(\n",
    "    decision_tree,\n",
    "    X_non_separable,\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fbdb26-cc4a-44b2-a6c3-33b9368edb72",
   "metadata": {},
   "source": [
    "Decision trees are customizable in several ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10c8ab-ab93-4ac3-9f0c-ff41a2f55077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another tree with max_depth = 4\n",
    "decision_tree_2 = DecisionTreeClassifier(max_depth=4, min_impurity_decrease=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0054bab-b275-4ca7-9b49-59e4a8003eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to dataset\n",
    "decision_tree_2.fit(\n",
    "    X_non_separable,\n",
    "    y_non_separable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a8dbd-1389-4ca7-bf44-4d18efa62eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision region of the decision tree\n",
    "ax = plot_decision_boundary(\n",
    "    decision_tree_2,\n",
    "    X_non_separable,\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# plot the decision boundary of the decision tree\n",
    "plot_decision_boundary(\n",
    "    decision_tree_2,\n",
    "    X_non_separable,\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef8efe-a73a-4c2e-909a-185bde135146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd244a40-806a-4478-ba47-b3692cd79e8a",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "\n",
    "Research what is a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740779e-2abe-496f-9580-11767672c08b",
   "metadata": {},
   "source": [
    "*Solution*\n",
    "\n",
    "Random Forest models are **ensemble** models.\n",
    "\n",
    "This means that it combines many independently trained models to make better predictions.\n",
    "\n",
    "Random Forest models are built from many decision trees.\n",
    "\n",
    "Each decision tree is train on a subset of the whole dataset, and with a subset of all the features.\n",
    "\n",
    "The final decision is taken by majority voting.\n",
    "\n",
    "![random forest figure](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39035aa0-0468-4b7d-b2c0-c6f8e4aa7871",
   "metadata": {},
   "source": [
    "Build a random forest classifier with scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbc5d9-5280-4782-aea6-a8865e9e0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest Classifier from scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf63bb9-9af6-4d55-a83d-73a8ecbc4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model instance\n",
    "# use 100 decision trees to build the forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e0fd2-4a16-4d67-89a6-f92055a97c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the random forest to the data\n",
    "random_forest.fit(\n",
    "    X_non_separable,\n",
    "    y_non_separable,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a581a-ee12-457a-a2d1-9a5c51ee62e2",
   "metadata": {},
   "source": [
    "Plot its decision boundary and regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec95065-4a98-417d-a714-ea44a813c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision region of the decision tree\n",
    "ax = plot_decision_boundary(\n",
    "    random_forest,\n",
    "    X_non_separable,\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# plot the decision boundary of the decision tree\n",
    "plot_decision_boundary(\n",
    "    random_forest,\n",
    "    X_non_separable,\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0.45],  # changed contour level\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_non_separable,\n",
    "    style=y_non_separable,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1f79d-a1ce-47d5-89d5-652b98107260",
   "metadata": {},
   "source": [
    "Go to this [website](http://cs.stanford.edu/people/karpathy/svmjs/demo/demoforest.html) and play with RF parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360f277-78ae-47c5-9dd5-fece89164db8",
   "metadata": {},
   "source": [
    "## 3. Model evaluation [30 min]\n",
    "\n",
    "You have seen multiple models for Iris flower classification.\n",
    "\n",
    "Which model is the best fit?\n",
    "\n",
    "How can we be confident about the predictions of a model, or evaluate its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8829f-8c4b-4842-a251-248a55ef6c61",
   "metadata": {},
   "source": [
    "### Training and test split\n",
    "\n",
    "We could use the training data to count the number of correct and erroneous predictions.\n",
    "\n",
    "However this is a bad choice, as the Nearest Neighbor will always have 0 errors (can you see why?). \n",
    "\n",
    "In general, some models are very flexible and can fit any dataset.\n",
    "\n",
    "Others are rigid - like the linear SVM - and will not perfectly fit all datasets.\n",
    "\n",
    "Using the training data will not provide a clear picture of prediction accuracy for new points.\n",
    "\n",
    "**Solution**: Split the dataset into two parts: one for **training** another for evaluation or **testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0d74f-9d2a-4b5a-9bf8-9d2627ee049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the train_test_split function from scikit-learn module for model selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32a55f-89ce-481f-a1e8-d4259d0c802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset and labels into test and train\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris,\n",
    "    y_iris,\n",
    "    test_size=0.3,  # test dataset is 30% of all data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da46279-eda9-4bc4-bf39-2a4193013208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the dataset\n",
    "sns.scatterplot(data=iris_data, x=feature_1, y=feature_2, hue=y_iris)\n",
    "\n",
    "# with circles around the training set\n",
    "sns.scatterplot(\n",
    "    data=X_iris_train,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    "    label=\"train set\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5417b92-0470-4c62-8932-507bbbe1bb8e",
   "metadata": {},
   "source": [
    "Splits are usually done randomly to avoid selection bias.\n",
    "\n",
    "Often random sampling can introduce imbalances to both training and test dataset.\n",
    "\n",
    "If a dataset contains 10 points of class A, and 100 points of class B, then random sampling is likely to bin all class A points into a single split.\n",
    "\n",
    "In this case other sampling methods, such as stratified random sampling, are a better approach (such as `scikit-learn` [Stratified Shuffle Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn-model-selection-stratifiedshufflesplit))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bf89f-d0f2-40ed-8598-4ad1c9b0aa8d",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43991440-f3ff-4499-862c-4150d270fb71",
   "metadata": {},
   "source": [
    "There are many measures of performance.\n",
    "\n",
    "Accuracy, which is percentage of correct predictions, is commonly used for classification.\n",
    "\n",
    "Other metrics will provide different information on the model's performace.\n",
    "\n",
    "See the list of [classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) available in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e9955-607d-4268-821c-cc19ec7acfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new Nearest Neighbor model\n",
    "knn_model_3 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit with train split\n",
    "knn_model_3.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "# predict on the test data\n",
    "iris_test_predictions = knn_model_3.predict(X_iris_test)\n",
    "\n",
    "# compare to ground truth\n",
    "is_correct = iris_test_predictions == y_iris_test\n",
    "\n",
    "# print first results\n",
    "is_correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96c6fa-c6fd-46ca-ba9b-1ea46e7e27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "n_correct_predictions = is_correct.sum()\n",
    "accuracy = n_correct_predictions / len(X_iris_test)\n",
    "\n",
    "print(f\"Nearest Neighbor model accuracy = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943a722-2438-4102-9fe2-6a5a4a47e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn provides an easy way of evaluating models\n",
    "score = knn_model_3.score(X_iris_test, y_iris_test)\n",
    "\n",
    "print(f\"Model score = {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5a188-a82e-4887-bbc9-521f8bc1540c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "To evaluate a model:\n",
    "1. Split dataset into train and test\n",
    "2. Fit model with training data\n",
    "3. Select relevant performance metrics\n",
    "4. Evaluate with test data\n",
    "\n",
    "This is done succinctly with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ea229-4365-4618-b034-4317c241838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset and labels into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris,\n",
    "    y_iris,\n",
    "    test_size=0.3,\n",
    ")\n",
    "\n",
    "# create model\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit model with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model with test data\n",
    "# scikit learn has preselected accuracy as the relevant metric\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Model score = {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb23cc-d2e5-4355-820d-cd4ad5d1e395",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "\n",
    "Compute the accuracy score of all previous classification models on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000907e3-4815-4abc-8766-19fd45c16574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97158940-a253-4637-a7e2-a34d365507c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all models that will be evaluated\n",
    "model_list = [\n",
    "    KNeighborsClassifier(n_neighbors=1),\n",
    "    LinearSVC(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(max_depth=4),\n",
    "    RandomForestClassifier(),\n",
    "]\n",
    "\n",
    "# create a list to store model evaluations\n",
    "evaluations = []\n",
    "\n",
    "# evaluate each model\n",
    "for model in model_list:\n",
    "    # fit model to training data\n",
    "    model.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "    # compute accuracy of fitted model using test data\n",
    "    accuracy = model.score(X_iris_test, y_iris_test)\n",
    "\n",
    "    # store results in evaluations list\n",
    "    evaluations.append({\"model\": str(model), \"accuracy\": accuracy})\n",
    "\n",
    "# group evaluation results into pandas dataframe\n",
    "evaluations = pd.DataFrame(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c046986-fee6-4686-83d6-634bd4de8a04",
   "metadata": {},
   "source": [
    "Which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e60f1e-2ca6-4fc0-8365-3a08c8d76558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model evaluation table ordered by accuracy\n",
    "evaluations.sort_values(\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20932348-8e69-46bc-bd31-d04e7cff5780",
   "metadata": {},
   "source": [
    "Does the answer change if you use a different dataset split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c033be1-3612-44b7-99ae-32672f2ec0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new split\n",
    "# the train_test_split function will randomly select a split\n",
    "# running the function again will provide a different split\n",
    "X_iris_train_2, X_iris_test_2, y_iris_train_2, y_iris_test_2 = train_test_split(\n",
    "    X_iris,\n",
    "    y_iris,\n",
    "    test_size=0.3,\n",
    ")\n",
    "\n",
    "# create new model instances\n",
    "model_list = [\n",
    "    KNeighborsClassifier(n_neighbors=1),\n",
    "    LinearSVC(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(max_depth=4),\n",
    "    RandomForestClassifier(),\n",
    "]\n",
    "\n",
    "# create a list to store model evaluations\n",
    "evaluations_2 = []\n",
    "\n",
    "# evaluate each model\n",
    "for model in model_list:\n",
    "    # fit model to training data\n",
    "    model.fit(X_iris_train_2, y_iris_train_2)\n",
    "\n",
    "    # compute accuracy of fitted model using test data\n",
    "    accuracy = model.score(X_iris_test_2, y_iris_test_2)\n",
    "\n",
    "    # store results in evaluations list\n",
    "    evaluations_2.append({\"model\": str(model), \"accuracy\": accuracy})\n",
    "\n",
    "# group evaluation results into pandas dataframe\n",
    "evaluations_2 = pd.DataFrame(evaluations_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb7bf7-ed93-4578-9fb1-bb1ab501ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model evaluation table ordered by accuracy\n",
    "evaluations_2.sort_values(\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a8700-c2dd-4a49-8441-a1c1dd5346bb",
   "metadata": {},
   "source": [
    "Research what is cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230b28d-d11f-4f13-a709-9505e687cd97",
   "metadata": {},
   "source": [
    "Taken from [wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics)):\n",
    "\n",
    "> Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations.\n",
    "\n",
    "> The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset.\n",
    "\n",
    "> In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.\n",
    "\n",
    "\n",
    "\n",
    "![cross validation figure](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1920px-K-fold_cross_validation_EN.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac56af-a558-42c0-bb49-01118e472386",
   "metadata": {},
   "source": [
    "Implement cross-validation with scikit-learn (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c9e88-5032-4664-a80b-95ab7dc08bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cross_val_score from scikit-learn\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b015e6-b48e-47b9-a240-529eb03c3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model\n",
    "for model in model_list:\n",
    "    # compute accuracy for multiple splits using scikit-function\n",
    "    # do 5-fold cross-validation\n",
    "    scores = cross_val_score(model, X_iris, y_iris, cv=5)\n",
    "    \n",
    "    # print the mean accuracy and the standard deviation\n",
    "    print(f\"{str(model)} accuracy mean = {scores.mean():.1%}, std = {scores.std():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650c6a1-d257-4fb8-ba1c-36c5d95abafa",
   "metadata": {},
   "source": [
    "## 4. Regression [30 min]\n",
    "\n",
    "When the target of supervised learning is a numerical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d74bf3-91c2-4bf2-9c80-20627f780131",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "* Trying to predict future $CO_2$ levels for the next decade. Here the feature vector **x** = year and target variable **y** = $CO_2$ levels.\n",
    "\n",
    "![historic atmospheric co2 data](https://research.noaa.gov/Portals/0/EasyGalleryImages/1/864/co2_data_mlo.png)\n",
    "\n",
    "(taken from [NOAA research news](https://research.noaa.gov/article/ArtMID/587/ArticleID/2764/Coronavirus-response-barely-slows-rising-carbon-dioxide), Monday, June 7, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c108b-75db-4549-8f0a-7cd754e79b08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751c4e0-6d12-4fd0-ada7-63eec0fcf137",
   "metadata": {},
   "source": [
    "Let use scikit-learn to generate synthetic data for a regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d0374-e314-4b8b-bcd3-ed10f11b122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import make_regression function from scikit-learn datasets module\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# generate a random dataset for regression with some noise and 200 points\n",
    "X_reg, y_reg = make_regression(n_features=1, noise=10, n_samples=200)\n",
    "\n",
    "# use seaborn to generate a scatterplot\n",
    "sns.scatterplot(x=X_reg.flatten(), y=y_reg);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862cdcf-b3b2-4e89-adc2-aa15711bc9d5",
   "metadata": {},
   "source": [
    "A linear regression model assumes that there is a **linear** relation between the features and the target variable\n",
    "\n",
    "$$ {\\bf y} = m {\\bf x} + b $$\n",
    "\n",
    "The parameters $m$ (slope) and $b$ (bias) that best \"fit\" the data points can be found algorithmically.\n",
    "\n",
    "How good a model fits the data is determined by minimizing some **loss** or error.\n",
    "\n",
    "In the case of the linear model, the loss is measured by the Mean Squared Error (MSE), but we won't delve into details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c6f3c-b53f-4329-a383-70e048ea1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Linear Regression model from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit a linear model to the example data\n",
    "linear_reg_1 = LinearRegression().fit(X_reg, y_reg)\n",
    "\n",
    "# plot the datapoints. x = features, y = target value\n",
    "ax = sns.scatterplot(x=X_reg.flatten(), y=y_reg)\n",
    "\n",
    "x_min = X_reg.min()\n",
    "x_max = X_reg.max()\n",
    "\n",
    "# generate a prediction using the linear model on the example data\n",
    "pred = linear_reg_1.predict([[x_min], [x_max]])\n",
    "\n",
    "# plot the predicted line\n",
    "ax.plot([x_min, x_max], pred, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"x = Features\")\n",
    "ax.set_ylabel(\"y = Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec5dc9-86cc-464c-ac42-888b63cad733",
   "metadata": {},
   "source": [
    "Once fitted, a prediction for points outside the dataset is computed with the same formula.\n",
    "\n",
    "$$ {\\bf y_{pred}} = m {\\bf x_{test}} + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9a54d-018f-45ad-bc08-686949484eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new test point at x = 2\n",
    "test_point = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e40db-5685-4f02-a7ca-cf72b9e9b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the linear model to predict its target value\n",
    "predicted_value = linear_reg_1.predict([test_point])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd68e5f-3bfe-4e45-b2d4-2930cab34fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the datapoints. x = features, y = target value\n",
    "ax = sns.scatterplot(x=X_reg.flatten(), y=y_reg)\n",
    "\n",
    "# plot the predicted line\n",
    "ax.plot([x_min, x_max], pred, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# draw a point at the test point with its predicted value\n",
    "plt.scatter(test_point, [predicted_value], color=\"red\")\n",
    "\n",
    "y_min = pred.min()\n",
    "\n",
    "# draw a vertical arrow from the x-axis at x = test_point to its predicted value\n",
    "ax.arrow(\n",
    "    2,\n",
    "    y_min,\n",
    "    0,\n",
    "    predicted_value - y_min,\n",
    "    color=\"red\",\n",
    "    head_width=0.1,\n",
    "    head_length=10,\n",
    "    length_includes_head=True,\n",
    ")\n",
    "\n",
    "# draw a horizontal arrow from the point (x, y) = (test_point, predicted_value) to\n",
    "# the y-axis at y = predicted_value\n",
    "ax.arrow(\n",
    "    test_point[0],\n",
    "    predicted_value,\n",
    "    -test_point[0] + x_min,\n",
    "    0,\n",
    "    color=\"red\",\n",
    "    head_width=10,\n",
    "    head_length=0.1,\n",
    "    length_includes_head=True,\n",
    ")\n",
    "\n",
    "# add the linear formula to the plot\n",
    "ax.text(-1, 50, \"y = mx + b\")\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"x = Features\")\n",
    "ax.set_ylabel(\"y = Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e7bce-352b-4288-bedd-fdcac275be13",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f6d8b-9398-4ad5-ac71-a814f1e621f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another dataset\n",
    "# here target_ideal is a nonlinear function of x\n",
    "X_reg_2 = np.arange(0, 100, 2.0)\n",
    "y_reg_2_ideal = np.sin(X_reg_2 / 10) + (X_reg_2 / 50) ** 2\n",
    "\n",
    "# add some noise to our target variable target_ideal\n",
    "y_reg_2 = y_reg_2_ideal + np.random.normal(size=len(y_reg_2_ideal)) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35311f62-4176-460e-bbb4-cd22eec36c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatter points (x = features, y = target)\n",
    "ax = sns.scatterplot(x=X_reg_2, y=y_reg_2)\n",
    "\n",
    "# add title\n",
    "ax.set_title(\"Non linear dataset\")\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"x = Features\")\n",
    "ax.set_ylabel(\"y = Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a41f23-f310-4b82-a1b0-0e01b83542c8",
   "metadata": {},
   "source": [
    "As with classification, linear models are very rigid and will produce bad predictions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eedc55b-47b0-4243-b70b-07d978f30b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit linear model\n",
    "linear_reg_2 = LinearRegression()\n",
    "linear_reg_2.fit(X_reg_2.reshape(-1, 1), y_reg_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823741f-191b-492e-8bc5-101214eeb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a range of test points\n",
    "test_points = np.linspace(0, 100, 1000)\n",
    "linear_reg_pred_2 = linear_reg_2.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "# plot dataset\n",
    "sns.scatterplot(x=X_reg_2, y=y_reg_2)\n",
    "\n",
    "# plot the fitted linear model\n",
    "sns.lineplot(x=test_points, y=linear_reg_pred_2, color=\"red\", label=\"linear regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1efbd0-b842-4206-9b09-fddf3a8cded0",
   "metadata": {},
   "source": [
    "How else can we predict target value of a test point using features?\n",
    "\n",
    "**Simple idea revisited**: Use the nearest neighbor's target value as a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da270c-994f-433b-97ca-33eae28dc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Nearest Neighbor Regression model from scikit-learn\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64199a0a-0447-4f00-9f7c-1c29c8541498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit nearest neighbor model\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_reg.fit(X_reg_2.reshape(-1, 1), y_reg_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448b22d-e81c-4c61-85c0-0138ea20fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a range of test points\n",
    "test_points = np.linspace(0, 100, 1000)\n",
    "knn_reg_pred_2 = knn_reg.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "# plot dataset\n",
    "sns.scatterplot(x=X_reg_2, y=y_reg_2)\n",
    "\n",
    "# plot fitted nearest neighbor model\n",
    "sns.lineplot(x=test_points, y=knn_reg_pred_2, color=\"red\", label=\"nearest neighbor\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843571b1-7647-4e25-86e8-376784f52bb4",
   "metadata": {},
   "source": [
    "Nearest neighbor regression suffers from the same problems as nearest neighbor classification:\n",
    "    \n",
    "* Sensitive to noise\n",
    "* Heavy on computation and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096a98d-e9ff-4869-a125-b7874c805fda",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa980005-fdef-4c5f-855e-948d253c0c0a",
   "metadata": {},
   "source": [
    "Similarly the Random Forest model can be adapted for regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefeb78-188b-49e1-8866-139756809646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Nearest Neighbor Regression model from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d1b26-f151-43a7-a082-5a2df2f6dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest regression model\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_reg_2.reshape(-1, 1), y_reg_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6a943-92ad-4ff9-bac6-e0ae490d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a range of test points\n",
    "test_points = np.linspace(0, 100, 1000)\n",
    "rf_reg_pred_2 = rf_reg.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "# plot the fitted linear model and random forest\n",
    "sns.scatterplot(x=X_reg_2, y=y_reg_2)\n",
    "\n",
    "sns.lineplot(x=test_points, y=rf_reg_pred_2, color=\"red\", label=\"random forest\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9c37-f7f2-4aed-a0e0-cbf648d2436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "sns.scatterplot(x=X_reg_2, y=y_reg_2, color=\"black\", label=\"train dataset\", zorder=4)\n",
    "\n",
    "# plot the all fitted models\n",
    "sns.lineplot(x=test_points, y=linear_reg_pred_2, label=\"random forest\")\n",
    "sns.lineplot(x=test_points, y=knn_reg_pred_2, label=\"nearest neighbors\")\n",
    "sns.lineplot(x=test_points, y=rf_reg_pred_2, label=\"linear\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0025b02-bdb8-4961-b9c0-f0e8fa148a65",
   "metadata": {},
   "source": [
    "**Which is the best predictive model?**\n",
    "\n",
    "**How to evaluate regression models?**\n",
    "\n",
    "Similar procedure as classification but different metric.\n",
    "\n",
    "How to measure good fit?\n",
    "\n",
    "One option is to use **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i = 1}^{n} (y_{true} - y_{pred})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703ed90-58f7-4ac4-a77e-65b01cf15bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "ax = sns.scatterplot(x=X_reg_2, y=y_reg_2)\n",
    "\n",
    "# plot fitted line\n",
    "sns.lineplot(x=test_points, y=linear_reg_pred_2, color=\"red\", label=\"linear regression\")\n",
    "\n",
    "# use linear model to predict on original dataset\n",
    "y_reg_2_linear_pred = linear_reg_2.predict(X_reg_2.reshape(-1, 1))\n",
    "\n",
    "# plot errors\n",
    "for x, y_true, y_pred in zip(X_reg_2, y_reg_2, y_reg_2_linear_pred):\n",
    "    ax.plot([x, x], [y_true, y_pred], alpha=0.5, color=\"black\", linewidth=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf973945-fa1e-4175-baa3-ad63ac4cad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the array of differences in prediction and true value\n",
    "error = y_reg_2 - y_reg_2_linear_pred\n",
    "\n",
    "# compute the square of each error\n",
    "squared_error = error ** 2\n",
    "\n",
    "# compute the mean\n",
    "MSE = squared_error.mean()\n",
    "\n",
    "print(f\"MSE of linear model on training dataset: {MSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28d190-f4f4-452a-b849-1dd2c0b183cc",
   "metadata": {},
   "source": [
    "Scikit-learn implements MSE and offers multiple regression metrics.\n",
    "\n",
    "Each metric has its benefits and pitfalls. Choice depends on use case.\n",
    "\n",
    "Visit this [site](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) to see available regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157127b9-2e28-4d05-a1f2-b973db38ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn provides tools for easy computation of MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# use scikit-learn function to compute MSE\n",
    "MSE = mean_squared_error(y_true=y_reg_2, y_pred=y_reg_2_linear_pred)\n",
    "\n",
    "print(f\"Score of linear model: {MSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad1c42-86c4-4074-9705-bba59816a054",
   "metadata": {},
   "source": [
    "Split the dataset into train and test to make a fair comparison between different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dbc67-6035-4bb4-aae5-962a89449fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target into train and test. Test is 30% of all data.\n",
    "X_reg_2_train, X_reg_2_test, y_reg_2_train, y_reg_2_test = train_test_split(\n",
    "    X_reg_2, y_reg_2, test_size=0.3\n",
    ")\n",
    "\n",
    "# iterate over model types\n",
    "for model in [\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(n_neighbors=1),\n",
    "    RandomForestRegressor(),\n",
    "]:\n",
    "    # fit the model to training data\n",
    "    model.fit(X_reg_2_train.reshape(-1, 1), y_reg_2_train)\n",
    "\n",
    "    # use fitted model to predict in test data\n",
    "    y_pred = model.predict(X_reg_2_test.reshape(-1, 1))\n",
    "\n",
    "    # compute MSE using the predictions and ground truth\n",
    "    mse = mean_squared_error(y_true=y_reg_2_test, y_pred=y_pred)\n",
    "\n",
    "    print(f\"{str(model):>34} mse = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce21ae2-68c0-4ad0-be74-946471a73fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and test points\n",
    "sns.scatterplot(\n",
    "    x=X_reg_2_train,\n",
    "    y=y_reg_2_train,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    label=\"train\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X_reg_2_test,\n",
    "    y=y_reg_2_test,\n",
    "    color=\"black\",\n",
    "    marker=\"x\",\n",
    "    label=\"test\",\n",
    ")\n",
    "\n",
    "# iterate over model types\n",
    "for model in [\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(n_neighbors=1),\n",
    "    RandomForestRegressor(),\n",
    "]:\n",
    "    # fit the model to training data\n",
    "    model.fit(X_reg_2_train.reshape(-1, 1), y_reg_2_train)\n",
    "\n",
    "    # generate predictions in range of data points\n",
    "    test_points = np.linspace(0, 100, 1000)\n",
    "    y_pred = model.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "    # plot predicted line\n",
    "    sns.lineplot(x=test_points, y=y_pred, label=str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b84b2-6676-4ac8-ac34-ca6702cc0607",
   "metadata": {},
   "source": [
    "**What if number of features > 1?**\n",
    "\n",
    "In most cases multiple features are used for prediction.\n",
    "\n",
    "That is the same as saying feature vectors are multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b41456-6d6f-4923-9774-817181eb63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a synthetic dataset for regression with 2 features\n",
    "X_2D, y_2D = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_targets=1,\n",
    ")\n",
    "\n",
    "# visualise with seaborn\n",
    "# plot points at (x = feature 1, y = feature 2)\n",
    "# use the target variable to determine point size and colour\n",
    "grid = sns.relplot(\n",
    "    x=X_2D[:, 0],\n",
    "    y=X_2D[:, 1],\n",
    "    size=y_2D,\n",
    "    sizes=(40, 400),\n",
    "    alpha=0.5,\n",
    "    hue=y_2D,\n",
    ")\n",
    "\n",
    "# add axis labels\n",
    "grid.ax.set_xlabel(\"feature 1\")\n",
    "grid.ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bce6a3-8c72-48b2-8d80-eee6a55fc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test\n",
    "X_2D_train, X_2D_test, y_2D_train, y_2D_test = train_test_split(\n",
    "    X_2D,\n",
    "    y_2D,\n",
    "    test_size=0.3,\n",
    ")\n",
    "\n",
    "# fit a Nearest Neighbor Regression model to training data\n",
    "knn_reg_2D = KNeighborsRegressor(n_neighbors=1).fit(X_2D_train, y_2D_train)\n",
    "\n",
    "# create a mesh of points\n",
    "XX, YY = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
    "\n",
    "# predict on each point in mesh\n",
    "knn_2D_predictions = knn_reg_2D.predict(np.c_[XX.flatten(), YY.flatten()])\n",
    "\n",
    "# compute the min and max value of predictions\n",
    "vmin, vmax = knn_2D_predictions.min(), knn_2D_predictions.max()\n",
    "\n",
    "# select colormap\n",
    "# see available colormaps at https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "cmap = \"plasma\"\n",
    "\n",
    "# plot\n",
    "ax = plt.pcolormesh(\n",
    "    XX,\n",
    "    YY,\n",
    "    knn_2D_predictions.reshape(XX.shape),\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    cmap=cmap,\n",
    ")\n",
    "\n",
    "# create a color bar to indicate mapping between columns and target values\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "# add label to color bar\n",
    "cbar.set_label(\"target\")\n",
    "\n",
    "# plot training data as small round points\n",
    "plt.scatter(\n",
    "    X_2D_train[:, 0],\n",
    "    X_2D_train[:, 1],\n",
    "    c=y_2D_train,\n",
    "    s=20,\n",
    "    edgecolor=\"black\",\n",
    "    label=\"train\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    cmap=cmap,\n",
    ")\n",
    "\n",
    "# compute predictions at test points\n",
    "y_2D_test_pred = knn_reg_2D.predict(X_2D_test)\n",
    "\n",
    "# compute prediction absolute error\n",
    "error = np.abs(y_2D_test - y_2D_test_pred)\n",
    "\n",
    "# plot test data as large square markers\n",
    "# color squares using true value of target variable\n",
    "# use absolute error to determine square size\n",
    "plt.scatter(\n",
    "    X_2D_test[:, 0],\n",
    "    X_2D_test[:, 1],\n",
    "    s=error,\n",
    "    c=y_2D_test,\n",
    "    marker=\"s\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"test\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    cmap=cmap,\n",
    "    sizes=(20, 100),\n",
    ")\n",
    "\n",
    "# add legend to figure\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b86424-9c9b-43e2-b37c-c383283f5234",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "\n",
    "Checkout the [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f8378-ac79-4bb0-b739-b2ce60eee72c",
   "metadata": {},
   "source": [
    "Load the features and target variables from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2a6ed-d26b-4280-bc10-9208ae23ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_diabetes function from scikit-learn\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# load the full diabetes data\n",
    "diabetes = load_diabetes(as_frame=True)\n",
    "\n",
    "# extract features\n",
    "X_diabetes = diabetes.data\n",
    "\n",
    "# extract target variable\n",
    "y_diabetes = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7afb69-dc66-4261-b8f9-b6500886e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first rows of the diabetes feature table\n",
    "X_diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dff3d-a85f-41b7-9955-8979a6c161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first entries of the target variable\n",
    "y_diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2d906-e3a3-4103-a916-0efc4602ff11",
   "metadata": {},
   "source": [
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33572be9-98cf-431d-979e-4769a230404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(\n",
    "    X_diabetes,\n",
    "    y_diabetes,\n",
    "    test_size=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0103b2-360d-4991-bedc-6c914f154741",
   "metadata": {},
   "source": [
    "Select any regression model(s) of your choice and fit to train data.\n",
    "\n",
    "Evaluate model fit with test data and MSE (and other metrics of your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854cab93-3d43-4d1b-82d3-3f4f6a3a0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will also compute the R2 score\n",
    "# import r2_score from scikit-learn\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# create three regression models in list\n",
    "regression_model_list = [\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(n_neighbors=1),\n",
    "    RandomForestRegressor(),\n",
    "]\n",
    "\n",
    "# fit and evaluate each model in list\n",
    "for model in regression_model_list:\n",
    "    # use train dataset to fit model\n",
    "    model.fit(X_diabetes_train, y_diabetes_train)\n",
    "    \n",
    "    # predict over the test set\n",
    "    y_pred = model.predict(X_diabetes_test)\n",
    "    \n",
    "    # compute MSE with scikit-learn function\n",
    "    mse = mean_squared_error(y_true=y_diabetes_test, y_pred=y_pred)\n",
    "    \n",
    "    # compute R2\n",
    "    r2 = r2_score(y_true=y_diabetes_test, y_pred=y_pred)\n",
    "    \n",
    "    # print results\n",
    "    print(f\"{str(model)} MSE = {mse:.2f} R2 = {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74447f3e-fd3a-4dae-850f-8edbf98b4492",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction [20 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139803c-3453-4251-a02b-8d2d8589578f",
   "metadata": {},
   "source": [
    "Often the data we collect can be very **high dimensional**. e.g. D > 1000\n",
    "\n",
    "This poses a problem as it is difficult to visualize anything greater than 3 dimensions.\n",
    "\n",
    "We can **project** this data down to a lower dimension. P << D, where P is typically 2 or 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a06b9-42d1-437e-86e4-17df9b7a9084",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "One of the simplest approach is to do a linear projection.\n",
    "\n",
    "**PCA** is a linear projection that aligns with the directions of highest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7f659-9f87-4f90-8206-707ba28dddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full iris dataset has 4 features\n",
    "iris_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfc7f0-b0cc-4f9e-b3ee-d4c0d7f8a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafead0-a78d-43c7-98d3-050489b919b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2-dimensional PCA projection\n",
    "pca_model = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1238f-c6ca-4b46-bb8c-5d31d419527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the 4-dimensional iris dataset into 2-d points\n",
    "projected_iris = pca_model.fit_transform(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaba0cd-cbe4-47a1-948e-27f4e31b424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot projected points\n",
    "# use species to color points\n",
    "ax = sns.scatterplot(\n",
    "    x=projected_iris[:, 0],\n",
    "    y=projected_iris[:, 1],\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"PCA component 1\")\n",
    "ax.set_ylabel(\"PCA component 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5fdb7-dc08-469e-b3f4-5c9fc84f56f9",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b1a69-280e-4780-8aac-0d070ab54842",
   "metadata": {},
   "source": [
    "As a final example let explore a dataset of digits.\n",
    "\n",
    "Each data point is an grayscale image of a handwritten digit.\n",
    "\n",
    "The images are 8x8 pixels, so in total each point has 64 features.\n",
    "\n",
    "In this case a feature is the grayscale value of a single pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d318e-e983-4227-993c-7130887d2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_digits function from scikit-learn datasets module\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# load digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# extract data and target values\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# select a single data point\n",
    "# reshape to original 8x8 array\n",
    "digit = X_digits[0].reshape(8, 8)\n",
    "\n",
    "# use matplotlib to show image\n",
    "plt.imshow(digit, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12aa9c-0efd-4edc-8081-d8e2a160289b",
   "metadata": {},
   "source": [
    "**How to visualize the whole dataset?**\n",
    "\n",
    "Use dimensionality reduction\n",
    "\n",
    "Lets try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65752d57-f479-4426-8629-35271684fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to project to 2 dimensions\n",
    "pca_digits = PCA(n_components=2).fit_transform(X_digits)\n",
    "\n",
    "# do a scatterplot, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=pca_digits[:, 0],\n",
    "    y=pca_digits[:, 1],\n",
    "    hue=y_digits,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e1d2c-0360-4abb-8c8b-edba92a06039",
   "metadata": {},
   "source": [
    "Some digits seem to cluster.\n",
    "\n",
    "Still, there is a lot of overlap.\n",
    "\n",
    "Lets try a different projection method.\n",
    "\n",
    "Now we will use a non-linear projection called **t-SNE**.\n",
    "\n",
    "Checkout the [paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) where t-SNE was introduced, or this amazing [blog](https://distill.pub/2016/misread-tsne/) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590e82c-8b92-4352-86c1-a796e369df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import t-SNE mapping from scikit-learn\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f67b5-2606-437e-a5f4-d338aa404d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TSNE to project to 2 dimensions\n",
    "tsne_digits = TSNE(\n",
    "    n_components=2,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ").fit_transform(X_digits)\n",
    "\n",
    "# do a scatterplot, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=tsne_digits[:, 0],\n",
    "    y=tsne_digits[:, 1],\n",
    "    hue=y_digits,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f72a4-2cdc-4e71-93b2-ef522613576c",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "\n",
    "Checkout the [wine dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset).\n",
    "\n",
    "Load the wine dataset with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee112e-329a-4ad4-96c7-589daa1ba8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_wine function from scikit-learn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# load all data\n",
    "wine = load_wine(as_frame=True)\n",
    "\n",
    "# extract the feature data\n",
    "X_wine = wine.data\n",
    "\n",
    "# extract the target variable (wine type)\n",
    "y_wine = wine.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144c1b2-bb64-4e83-914b-9b403c16888c",
   "metadata": {},
   "source": [
    "Use PCA and t-SNE projections on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b5b57-0e43-4a05-ade7-9d3bc106e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to project to 2 dimensions\n",
    "pca_wine = PCA(\n",
    "    n_components=2,\n",
    ").fit_transform(X_wine)\n",
    "\n",
    "# use TSNE to project to 2 dimensions\n",
    "tsne_wine = TSNE(\n",
    "    n_components=2,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ").fit_transform(X_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46dbba-8d79-40ac-bdf2-3a74f8b5288e",
   "metadata": {},
   "source": [
    "Do a scatterplot with the results and use the target wine class to colour points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f346298-2dc4-4623-9a91-edef954c08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a scatterplot of PCA projection,color points by digit\n",
    "sns.scatterplot(\n",
    "    x=pca_wine[:, 0],\n",
    "    y=pca_wine[:, 1],\n",
    "    hue=y_wine,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695b97c-4ae6-40b5-b86f-fe7ef3b4c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a scatterplot of t-SNE projection, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=tsne_wine[:, 0],\n",
    "    y=tsne_wine[:, 1],\n",
    "    hue=y_wine,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71e8c9-d4a9-445d-8473-ddb682292cfc",
   "metadata": {},
   "source": [
    "Are points from different wine classes separated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7cf4a-7da8-4dfa-82fe-20dfaaa47f0e",
   "metadata": {},
   "source": [
    "Not very much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f367341-adcb-4146-a8e8-b1db3faa96bc",
   "metadata": {},
   "source": [
    "Install the [umap-learn](https://umap-learn.readthedocs.io/en/latest/index.html) library and test the UMAP (Uniform Manifold Approximation and Projection) algorithm. (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6917184-6c3a-412d-9fd0-8aeb1b753c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699bfce-f74c-4991-b30d-0f260b275aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import UMAP projector from umap package\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eae53-98b6-40a0-8db1-b942faf98fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project to 2 dimensions using UMAP\n",
    "umap_wine = UMAP(\n",
    "    n_components=2,\n",
    ").fit_transform(X_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b34235-071b-4b76-95a4-3c1a7dfbbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a scatterplot of UMAP projection, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=umap_wine[:, 0],\n",
    "    y=umap_wine[:, 1],\n",
    "    hue=y_wine,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6c898-04e3-4638-b847-bec79403f411",
   "metadata": {},
   "source": [
    "The most dimensionallity reduction methods are very sensitive to scaling.\n",
    "\n",
    "Projection results are often dominated by features with larger values.\n",
    "\n",
    "Therefore it is common to standardize the variables before using dimensionallity reduction methods.\n",
    "\n",
    "A standardized variable has mean 0 and standard deviation of 1.\n",
    "\n",
    "Here is the formula to standardize a variable:\n",
    "\n",
    "$$\\hat{x} = \\frac{x - mean(x)}{std(x)}$$\n",
    "\n",
    "Scikit-learn provides standarization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f662c-2cc0-4133-92ad-8c93557357d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the StandardScaler from scikit-learn preprocessing module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize the wine features\n",
    "X_diabetes_std = StandardScaler().fit_transform(X_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e4ce7-bdc5-4927-bd28-60a2b5e6dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the standardized features with PCA\n",
    "pca_wine_std = PCA(\n",
    "    n_components=2,\n",
    ").fit_transform(X_diabetes_std)\n",
    "\n",
    "# do a scatterplot of PCA projection, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=pca_wine_std[:, 0],\n",
    "    y=pca_wine_std[:, 1],\n",
    "    hue=y_wine,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9602eb-4bfb-412c-b8c8-634c4063bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the standardized features with t-SNE\n",
    "umap_wine_std = TSNE(\n",
    "    n_components=2,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ").fit_transform(X_diabetes_std)\n",
    "\n",
    "# do a scatterplot of t-SNE projection, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=umap_wine_std[:, 0],\n",
    "    y=umap_wine_std[:, 1],\n",
    "    hue=y_wine,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3accb-f0a9-458b-8179-b25b0036d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the standardized features with UMAP\n",
    "umap_wine_std = UMAP(\n",
    "    n_components=2,\n",
    ").fit_transform(X_diabetes_std)\n",
    "\n",
    "# do a scatterplot of UMAP projection, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=umap_wine_std[:, 0],\n",
    "    y=umap_wine_std[:, 1],\n",
    "    hue=y_wine,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba5224-ac26-401d-8827-08f09b460b8d",
   "metadata": {},
   "source": [
    "## 6. Clustering [20 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6fd89-081a-479f-b98d-27a31d0c6790",
   "metadata": {},
   "source": [
    "**What if we don't have any labels?**\n",
    "\n",
    "Often our data contains some **structure**.\n",
    "\n",
    "* Features from different classes might be separated (**separability**)\n",
    "\n",
    "* Similar objects might have similar features (**smoothness**)\n",
    "\n",
    "Often we wish to find groupings or patterns in our data. This is called **clustering**.\n",
    "\n",
    "Datapoints in the same **cluster** are deemed to be similar under some measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b81ff-fc5b-4646-a6cd-ab0094f77d3c",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "There are many algorithms for clustering. Here you will use k-means clustering.\n",
    "\n",
    "Scikit-learn has a [collection of clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html#clustering), including k-means.\n",
    "\n",
    "If interested, checkout an [explanation](https://www.youtube.com/watch?v=4b5d3muPQmA) of the k-means clustering algorithm or an [interactive simulation](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e24dfa-835c-4614-8a8a-f5ac94bf26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import make_blobs function in scikit-learn datasets module\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate synthetic dataset made up of 5 blobs\n",
    "X_blobs, y_blobs = make_blobs(n_features=2, n_samples=4000, centers=5)\n",
    "\n",
    "# plot synthetic dataset\n",
    "ax = sns.scatterplot(x=X_blobs[:, 0], y=X_blobs[:, 1])\n",
    "\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60386f0b-6adb-4f69-a1fd-9870d428bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means clustering model from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create a new K-means clustering model.\n",
    "# specify 5 wanted clusters\n",
    "kmeans_model_1 = KMeans(n_clusters=5)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model_1.fit(X_blobs)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_blobs_pred = kmeans_model_1.predict(X_blobs)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_blobs[:, 0], y=X_blobs[:, 1], hue=y_blobs_pred)\n",
    "\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aec484-6933-4ec6-b7b1-ee95d4b23067",
   "metadata": {},
   "source": [
    "Clustering performance will depend on clustering parameters, choice of algorithm and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdef2f-442a-4313-9ca0-10bc3a2aa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with 3 clusters\n",
    "kmeans_model_2 = KMeans(n_clusters=3)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model_2.fit(X_blobs)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_blobs_pred = kmeans_model_2.predict(X_blobs)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_blobs[:, 0], y=X_blobs[:, 1], hue=y_blobs_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bb4f7-4ff7-4891-945e-38b85c897906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# repeat with other dataset\n",
    "X_circles, y_circles = make_circles(factor=0.2, n_samples=4000, noise=0.1)\n",
    "\n",
    "# create K means with 2 clusters\n",
    "kmeans_model_3 = KMeans(n_clusters=2)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model_3.fit(X_circles)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_pred = kmeans_model_3.predict(X_circles)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_circles[:, 0], y=X_circles[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a7a47-e7a9-467b-b0f2-b7db290050e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with DBscan algorithm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# create DBSCAN model\n",
    "dbscan_model = DBSCAN(eps=0.15)\n",
    "\n",
    "# fit to dataset\n",
    "y_pred = dbscan_model.fit_predict(X_circles)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_circles[:, 0], y=X_circles[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58822a2-72bc-47a6-9aed-4e99270184a1",
   "metadata": {},
   "source": [
    "### **Exercise**\n",
    "\n",
    "Use K-means clustering on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853e17a-cb3f-45f5-8f0b-74a7ab60c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a K-means model fit iris dataset\n",
    "kmeans_model_4 = KMeans(n_clusters=3)\n",
    "\n",
    "# fit to iris dataset and predict clusters\n",
    "kmeans_iris_clusters = kmeans_model_4.fit_predict(X_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39105e7d-3065-4c89-bcce-7ea0ccd2f2a7",
   "metadata": {},
   "source": [
    "Can you recover the species separation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68350299-8c8a-4925-a9c7-79ff2239ecf6",
   "metadata": {},
   "source": [
    "We can make a contingency table using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d491e92-2ee0-49d7-9a4c-94ad5339c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function to create contingency table from scikit-learn\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "# compute contingency table using true labels and clustering predicted labels\n",
    "matrix = contingency_matrix(labels_true=y_iris, labels_pred=kmeans_iris_clusters)\n",
    "\n",
    "# make it into a pandas dataframe for better display\n",
    "pd.DataFrame(matrix, index=species_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90b5e6-fa70-4336-a8c7-fd57fd131a9b",
   "metadata": {},
   "source": [
    "Also, we can scatterplot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc428488-12d7-4293-b0a2-aa1088eb99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatterplot of iris dataset\n",
    "# use the predicted clusters for color\n",
    "# use the true species for marker style\n",
    "sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=kmeans_iris_clusters,\n",
    "    style=y_iris,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a687d-a6a1-43f0-8725-bfc97d608d86",
   "metadata": {},
   "source": [
    "Notice that the clusters found by K-means are very similar to the species grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90e730-d3d5-4d95-aaf6-ae7a2fb9ddb7",
   "metadata": {},
   "source": [
    "Research Affinity Propagation clustering and compare to K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce163c48-d46e-4b2c-a2c4-bd08516a743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680e4d9-20ea-4d97-bfb8-c1b1b5be3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Affinity Propagation model fit iris dataset\n",
    "aff_prop_model = AffinityPropagation()\n",
    "\n",
    "# fit to iris dataset and predict clusters\n",
    "aff_prop_iris_clusters = aff_prop_model.fit_predict(X_iris)\n",
    "\n",
    "# compute contingency table using true labels and clustering predicted labels\n",
    "matrix = contingency_matrix(labels_true=y_iris, labels_pred=aff_prop_iris_clusters)\n",
    "\n",
    "# make it into a pandas dataframe for better display\n",
    "pd.DataFrame(matrix, index=species_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e01c4-4f88-43e6-ab9c-602bae6f1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatterplot of iris dataset\n",
    "# use the predicted clusters for color\n",
    "# use the true species for marker style\n",
    "sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=aff_prop_iris_clusters,\n",
    "    style=y_iris,\n",
    "    palette=\"tab20\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d39150-d0ab-4e88-afaa-9c80d06091af",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7e01c-067c-47f5-9cae-5ad1459f9682",
   "metadata": {},
   "source": [
    "**Which algorithm to choose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e80a3-b8ff-4e18-8ce3-a07c94f38ff6",
   "metadata": {},
   "source": [
    "Short answer: It depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4ba4e-a002-4062-970f-a4fd085ab423",
   "metadata": {},
   "source": [
    "> The “No Free Lunch” Theorem argues that, without having substantive information about the modeling problem, there is no single model that will always do better than any other model. Because of this, a strong case can be made to try a wide variety of techniques, then determine which model to focus on.\n",
    ">\n",
    "> — Pages 25-26, Applied Predictive Modeling, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143b484-315c-4e02-8159-723891cf5495",
   "metadata": {},
   "source": [
    "No silver bullet, but often for classification it is sensible to first try a Support Vector Machine or Random Forest.\n",
    "\n",
    "This will give you an idea of how separable your data is. The next step is to try different features, and perhaps even collect more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214a328-0379-4c71-aad6-2e1d96e1b07f",
   "metadata": {},
   "source": [
    "**How much data do I need?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86aaa1-6a7f-4e03-a9f0-046d1592afc5",
   "metadata": {},
   "source": [
    "Short answer: It depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c65baf-518c-4a61-a70c-6e2c59c94029",
   "metadata": {},
   "source": [
    "It depends on how easy it is for your classifier to separate your data. \n",
    "\n",
    "Some problems are relatively easy and don’t require lots of data, others such as species identification in images can require 10,000s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci",
   "language": "python",
   "name": "sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
