{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83537d0-a528-4acf-810c-6556b920e183",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/2_Intro_to_ML/1_Intro_to_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028fc6-114c-4d59-91a7-04725b8952b2",
   "metadata": {},
   "source": [
    "# Week 2 Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64833238-bd94-400d-b5d0-b70b5cf14b71",
   "metadata": {},
   "source": [
    "## What we will learn\n",
    "\n",
    "In this weeks colab we will introduce the Machine Learning library `scikit-learn` and practice some basic concepts of Machine Learning (ML), including:\n",
    "\n",
    "1. How to breakdown ML projects\n",
    "2. How to do classification with `scikit-learn`\n",
    "3. How to evaluate model performance\n",
    "4. How to do regression with `scikit-learn`\n",
    "5. How to do clustering with `scikit-learn`\n",
    "6. How to do dimensionality reduction with `scikit-learn`\n",
    "\n",
    "In the process you will also learn about:\n",
    "* Some classification algorithms, such as Nearest Neighbors, SVM, Decision Trees and Random Forest\n",
    "* Some regression algorithms, such as Linear Regression, and Nearest Neighbor Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f36176-c7d7-4c02-b125-647a774de8dc",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e46b4-a9e5-4d7a-9cf2-21f28924e6f0",
   "metadata": {},
   "source": [
    "**What is Machine Learning?**\n",
    "\n",
    "Teaching computers how to perform a task without having to explicitly program them to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9042a9-507b-4ce7-b84b-e9d3ac11c9e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> A computer program is said to learn from experience E with respect to some classes of task T and performance measure P if its performance can improve with E on T measured by P.\n",
    ">\n",
    "> M. T. Mitchell. 1997. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7b64e-af80-4b27-b33e-719acbf7e2ec",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad858-6ab4-4d26-b43f-e4125a76ab49",
   "metadata": {},
   "source": [
    "Butterfly recognition:\n",
    "\n",
    "* Task T: Classify images of British butterflies into different species\n",
    "* Performance measure P: percent of images that have been correctly classified\n",
    "* Training experience E: A database of butterfly images from museum collections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5ee4f-b5d8-4003-89b5-4a157fde2db2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**How does a computer program learn?**\n",
    "\n",
    "Using **data** to **parametrize** models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c21f57-3b9d-4a57-820b-4ad1d3e32163",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ML workflow\n",
    "\n",
    "Using ML for ecological inference is a multistep process. In practice it can generally be broken down into the following steps:\n",
    "\n",
    "* Data collection\n",
    "* Data preparation\n",
    "* Model training\n",
    "* Model evaluation\n",
    "* Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf2da2-9198-4319-ad47-3bbf30032304",
   "metadata": {},
   "source": [
    "![ml workflow](images/ml_workflow.png)\n",
    "\n",
    "> Simplified workflow from: S. Amershi et al., [\"Software Engineering for Machine Learning: A Case Study,\"](https://ieeexplore.ieee.org/abstract/document/8804457) 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), Montreal, QC, Canada, 2019, pp. 291-300, doi: 10.1109/ICSE-SEIP.2019.00042."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe74165-d0f9-435f-8543-626a5b9d04a8",
   "metadata": {},
   "source": [
    "The previous figure serves as a guideline indicating the usual flow, but it is not uncommon to work simultaneously on multiple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f676e3-aeda-4a7c-ae74-f36b68b7792f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Each step can be subdivided further:\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "* Design your ML goals\n",
    "* Determine what data (and ideally how much) you will need.\n",
    "* Collect data from multiple sources\n",
    "    * Field studies\n",
    "    * Open-source datasets\n",
    "    * Web scraping\n",
    "    * Citizen science\n",
    "    * Colaboration\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "* Select feature set\n",
    "* Clean dataset, fix errors, decide what to do with missing values\n",
    "* Normalize variables\n",
    "* Annotate or label data\n",
    "* Split data into training and test datasets\n",
    "\n",
    "### Model Training\n",
    "\n",
    "* Select an adequate ML model\n",
    "* Train model and validate\n",
    "* Finetune hyperparameters\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "* Compute metrics\n",
    "* Visualize predictions\n",
    "* Study failure cases\n",
    "* Identify weak spots\n",
    "* Compare with baselines\n",
    "\n",
    "### Make predictions\n",
    "\n",
    "* Use model to process novel data\n",
    "* Use predictions to make ecological inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea068d60-1e20-409f-bdbe-a4138c5d65de",
   "metadata": {},
   "source": [
    "Now you will step through some practical examples of some of the steps listed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a53fb-b031-4d4d-ac25-7c3170709a3f",
   "metadata": {},
   "source": [
    "## 2. Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6460da-8338-41a4-bb7e-69a4a4f8fb82",
   "metadata": {},
   "source": [
    "`scikit-learn` is a Python library for Machine Learning. It offers a wide array of algorithms for several ML tasks and tools to setup ML pipelines.\n",
    "\n",
    "![scikit-learn](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/260px-Scikit_learn_logo_small.svg.png?20180808062052)\n",
    "\n",
    "You can learn about `scikit-learn` in its official [documentation](https://scikit-learn.org/stable/index.html).\n",
    "\n",
    "The development team published a paper on the design of the package wich makes an interesting read\n",
    "\n",
    "> Pedregosa, Fabian, et al. [\"Scikit-learn: Machine learning in Python.\"](https://arxiv.org/abs/1201.0490) the Journal of machine Learning research 12 (2011): 2825-2830."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dff65b-94df-44ba-84e1-1dead7bff9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn  # Notice scikit-learn is abbreviated as sklearn.\n",
    "\n",
    "# Scikit-learn will be preinstalled in colab environments\n",
    "\n",
    "# Print the currently installed scikit-learn version\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458f059-8d89-41c4-ade8-064cfeae9f6d",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb68d1-cc5e-472c-ba8e-c5c942d092bd",
   "metadata": {},
   "source": [
    "Suppose you need to automate the following task\n",
    "\n",
    "**Task**: Identify Iris flower species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c511f6-3040-4936-82fb-d46b3d5376d2",
   "metadata": {},
   "source": [
    "How would you describe this Iris flower?\n",
    "\n",
    "![iris](https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Iris_germanica_%28Purple_bearded_Iris%29%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg/470px-Iris_germanica_%28Purple_bearded_Iris%29%2C_Wakehurst_Place%2C_UK_-_Diliff.jpg?20140528110728)\n",
    "\n",
    "* Colour?\n",
    "* Number of stripes?\n",
    "* Size?\n",
    "* Weight?\n",
    "* Environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973007c7-31bc-480c-96d4-6a9cdfa71220",
   "metadata": {},
   "source": [
    "**What are features?**\n",
    "\n",
    "They are numerical or categorical descriptors, attributes or traits of the object of study.\n",
    "\n",
    "> A **feature** is an individual measurable property or characteristic of a phenomenon\n",
    ">\n",
    "> *Bishop, Christopher (2006). Pattern recognition and machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7ae2a-895a-4522-98e9-95e2008cad63",
   "metadata": {},
   "source": [
    "In the case of Iris flowers, lets use sepal and petal length and width\n",
    "\n",
    "![iris sepal/petal length/width](https://ars.els-cdn.com/content/image/3-s2.0-B9780128147610000034-f03-01-9780128147610.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3ae91-fa27-4b70-bf2a-b36ae4e8a15b",
   "metadata": {},
   "source": [
    "Feature vector - Feature (descriptor):\n",
    "\n",
    "    x = (sepal_length, sepal_width, petal_length, petal_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b07eb8-820d-469e-9a44-ef1b3251b6e9",
   "metadata": {},
   "source": [
    "**Where to get Iris flower data?**\n",
    "\n",
    "The problem of Iris species identification was famously studied in the paper\n",
    "\n",
    "> Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "\n",
    "The dataset is publicly available. Find a description [here](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "Lets load a dataset of Iris flower measurements using `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2e229-eaa6-4f96-bd56-b71b8fb9c06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scikit-learn offers toy datasets including the iris dataset.\n",
    "# Load the dataset functions from scikit-learn.\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a2e7e-a7e7-4af3-9879-28232343f447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the iris dataset. It is a pandas DataFrame\n",
    "iris = datasets.load_iris(as_frame=True).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2ba57-e5fb-4a72-84a4-64a6dac8d3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the first rows\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfd39d-f2ad-46b1-aa05-8a8bec6c93b4",
   "metadata": {},
   "source": [
    "**Recall**: Supervised training is one of the main approaches of Machine Learning.\n",
    "\n",
    "It consists of trying to predict a **target** variable using **features** as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c07ec2-0b7c-463f-b8cb-089581ebaa5c",
   "metadata": {},
   "source": [
    "**What is a classification task?**\n",
    "\n",
    "It is a supervised learning task where the target variable is a categorical variable, that is when trying to predict a class based on features.\n",
    "\n",
    "\n",
    "For the Iris dataset, given our feature vector **x**, can we predict the correct species (i.e. class label) **y**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80f71f-3ab8-4791-acfe-bef4ecd3a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also load the species labels using scikit-learn\n",
    "y = datasets.load_iris(as_frame=True).target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf3053-3cd7-4e9c-944f-23bf74cc1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first values of y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76146ca7-c357-4b38-bba8-8ae187271fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows per target class\n",
    "y.value_counts()\n",
    "# Notice the target class is codified as a integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eb01f-6387-4658-b3de-ad51f809c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names\n",
    "species_names = datasets.load_iris(as_frame=True).target_names\n",
    "\n",
    "# Print the correspondence between integer values and species names\n",
    "for index, name in enumerate(species_names):\n",
    "    print(f\"class {index} = {name}\")\n",
    "\n",
    "# Map the target integer values to species names\n",
    "y_species = y.apply(lambda index: species_names[index])\n",
    "\n",
    "y_species.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c7328-37b6-452c-9318-ac7eb4f7b6d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Exercise**: Explore the dataset (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ba705-ecb1-4b45-aa69-19ae573469bb",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ee27b-9da8-42e8-a914-61de12f31436",
   "metadata": {},
   "source": [
    "Select two features: petal length and petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d17d7e-9e42-47e8-a9c5-dad410ab880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = \"petal length (cm)\"\n",
    "feature_2 = \"petal width (cm)\"\n",
    "x = iris[[feature_1, feature_2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee64f34-52a3-4260-851d-dd324e37a1c1",
   "metadata": {},
   "source": [
    "Each datapoint has some <span style=\"color: deepskyblue;\">features</span> and a <span style=\"color: coral;\">class label</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537015a3-b9bb-4a1a-9826-3b992183bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn and matplotlib to make some plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1394ef-699b-4f33-92c0-04e7cbd5d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot each data point (x = feature1, y = feature2) and the species in color\n",
    "ax = sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    "    style=y_species,\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "# select a single data point\n",
    "sample = iris.iloc[60]\n",
    "\n",
    "# Add text to point to single data point\n",
    "ax.annotate(\n",
    "    text=f\"({feature_1}, {feature_2}), Species\",\n",
    "    xy=(sample[feature_1] + 0.1, sample[feature_2] - 0.05),\n",
    "    xytext=(sample[feature_2] + 0.5, sample[feature_2] - 0.3),\n",
    "    fontsize=12,\n",
    "    arrowprops={\n",
    "        \"width\": 1,\n",
    "        \"headwidth\": 6,\n",
    "        \"headlength\": 6,\n",
    "        \"edgecolor\": \"black\",\n",
    "        \"facecolor\": \"black\",\n",
    "    },\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e30ab2-9a29-4f34-ad8d-efb85540eea9",
   "metadata": {},
   "source": [
    "Given a <span style=\"color: deepskyblue;\">new</span> datapoint, how can we determine its <span style=\"color: coral\">class</span>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cb68c-330e-4e87-891d-eb3f3c45ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new test point\n",
    "test_point = [3.8, 1.6]  # Petal length cm, Petal width cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd3cbb-5c66-413c-be9c-ebdcb14bea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot the species type in color\n",
    "ax = sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    "    style=y_species,\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "# plot the new test point\n",
    "ax.scatter(x=[test_point[0]], y=[test_point[1]], color=\"deepskyblue\")\n",
    "\n",
    "# add \"new\" text and arrow pointing at new test point\n",
    "ax.annotate(\n",
    "    text=\"New\",\n",
    "    xy=(test_point[0] - 0.05, test_point[1] + 0.02),\n",
    "    xytext=(test_point[0] - 1, test_point[1] + 0.3),\n",
    "    fontsize=12,\n",
    "    color=\"deepskyblue\",\n",
    "    arrowprops={\n",
    "        \"width\": 1,\n",
    "        \"headwidth\": 6,\n",
    "        \"headlength\": 6,\n",
    "        \"edgecolor\": \"deepskyblue\",\n",
    "        \"facecolor\": \"deepskyblue\",\n",
    "    },\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a4929-07a9-4d49-8825-6a3add361b90",
   "metadata": {},
   "source": [
    "**Simple idea**: Assign the class of the nearest point in the dataset.\n",
    "\n",
    "How to find the nearest point in our dataset to a given test point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b96f0f-2e79-4181-a395-22b439f72653",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot the species type in color\n",
    "ax = sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    "    style=y_species,\n",
    "    s=50,\n",
    "    zorder=2,\n",
    ")\n",
    "\n",
    "# plot a line from the test point to each point in the dataset\n",
    "for _, flower in iris.iterrows():\n",
    "    ax.plot(\n",
    "        [test_point[0], flower[feature_1]],\n",
    "        [test_point[1], flower[feature_2]],\n",
    "        color=\"gray\",\n",
    "        linewidth=1,\n",
    "        alpha=0.5,\n",
    "        zorder=1,\n",
    "    )\n",
    "\n",
    "# plot the test point\n",
    "ax.scatter(x=[test_point[0]], y=[test_point[1]], color=\"deepskyblue\", s=100, zorder=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f3d95-0171-430f-bba1-54aa813c36b2",
   "metadata": {},
   "source": [
    "Compute **similarity** between two feature points.\n",
    "\n",
    "Use *Euclidean* distance (based on the pythagorean theorem)\n",
    "\n",
    "![euclidean distance](https://upload.wikimedia.org/wikipedia/commons/5/55/Euclidean_distance_2d.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9cd21-529e-48b8-9ce5-901b88455e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy for math functions\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Implementation of distance between two points\n",
    "def compute_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698501d8-0099-40f2-adf9-ff9657c1ee03",
   "metadata": {},
   "source": [
    "And find the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd332796-2d9a-4190-b2ad-2ad8fd04251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distance from the test point to every example in the dataset\n",
    "distance = iris.apply(\n",
    "    lambda row: compute_distance(test_point, [row[feature_1], row[feature_2]]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# print the first results\n",
    "distance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbc8af-fc14-4438-9a90-af615d2f0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the point in the dataset that is closest to the test point\n",
    "# and record its distance\n",
    "closest_point = iris.iloc[distance.argmin()]\n",
    "distance_to_closest = distance.min()\n",
    "print(\"Distance to closest point: \", distance_to_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d1e7f-1ce3-45be-abb7-9ee12fd203d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    ")\n",
    "\n",
    "# plot the test point as an 'x'\n",
    "sns.scatterplot(\n",
    "    x=[test_point[0]],\n",
    "    y=[test_point[1]],\n",
    "    marker=\"x\",\n",
    "    label=\"test point\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plot a ring around the nearest datapoint\n",
    "sns.scatterplot(\n",
    "    x=[closest_point[feature_1]],\n",
    "    y=[closest_point[feature_2]],\n",
    "    marker=\"o\",\n",
    "    label=\"nearest training point\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd226d-cc16-454b-8ee1-f17bb4cb00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the test point is the same class as the datapoint it is closest to\n",
    "predicted_species = y_species[distance.argmin()]\n",
    "print(f\"Predicted species: {predicted_species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba28ad7-2b79-4da6-be17-f320de20fdd4",
   "metadata": {},
   "source": [
    "**Summary: The nearest neighbor algorithm**\n",
    "\n",
    "1. Given a test point x\n",
    "2. Compute the distance between x and every other datapoint\n",
    "3. The class of x is set as the same as the closest datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae90ad0-8df1-498f-b65d-c682080d24e5",
   "metadata": {},
   "source": [
    "Here is a quick implementation of the nearest neighbor algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb3f25-7403-4221-9b2d-a4405cc3e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(test_point):\n",
    "    # compute the distance from the test point to every example in the dataset\n",
    "    distance = iris.apply(\n",
    "        lambda row: compute_distance(test_point, [row[feature_1], row[feature_2]]),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Get index where distance is minimum\n",
    "    index_of_min_distance = distance.argmin()\n",
    "\n",
    "    # find the point in the dataset that is closest to the test point\n",
    "    closest_point = iris.iloc[index_of_min_distance]\n",
    "\n",
    "    # assume the test point is the same class as the datapoint it is closest to\n",
    "    predicted_species = y_species[index_of_min_distance]\n",
    "\n",
    "    return predicted_species, closest_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803839f-2409-463c-a9f9-007f8f5d4be8",
   "metadata": {},
   "source": [
    "Scikit-learn also provides an easy way of building Nearest Neighbor Classification Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfda6ad-605e-4284-a975-7b446fd843dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNeighborsClassifier from scikit learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create a model instance\n",
    "nn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit it to the iris dataset\n",
    "nn_model.fit(x, y_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509eadb-9f09-4f07-9e0a-4620f5dfdd71",
   "metadata": {},
   "source": [
    "The model can be used to make inference on new points. Lets try it out on a new test point, make sure we get the same results as before, and plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965239fc-e18d-449b-8abf-c75662eaa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it to predict the species of a test point\n",
    "test_point_2 = np.array([2.1, 0.7])\n",
    "\n",
    "predicted_species = nn_model.predict(test_point_2.reshape(1, -1))[0]\n",
    "\n",
    "print(predicted_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23d030-9826-4b26-942f-d7f7a8af7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it is the same result as our algorithm\n",
    "predicted_species_ours, closest_point_2 = nearest_neighbour(test_point_2)\n",
    "print(f\"Predictions are equal = {predicted_species == predicted_species_ours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19f56f-8a20-456f-a165-e54e947ae1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "ax = sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    ")\n",
    "\n",
    "# plot the test point as an 'x'\n",
    "sns.scatterplot(\n",
    "    x=[test_point_2[0]],\n",
    "    y=[test_point_2[1]],\n",
    "    marker=\"x\",\n",
    "    label=\"test point\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plot a ring around the nearest datapoint\n",
    "sns.scatterplot(\n",
    "    x=[closest_point_2[feature_1]],\n",
    "    y=[closest_point_2[feature_2]],\n",
    "    marker=\"o\",\n",
    "    label=\"nearest training point\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779a0ca-6da9-446b-a4d4-4bb87ce92da5",
   "metadata": {},
   "source": [
    "The model predicts some species to every point in feature space. \n",
    "\n",
    "**Decision regions** are formed by points that are assigned to the same species. \n",
    "\n",
    "The regions are separated by **decision boundaries** where the model is unsure what class to assign.\n",
    "\n",
    "The next cell contains some function definitions to plot decision regions and boundaries. You can safely ignore it, but make sure to run the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7b577-32a4-4ca5-b6ea-68912c5fa913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title plot decision boundary definition\n",
    "# IGNORE: here we define functions to plot the decision boundary.\n",
    "# Their implementation is not relevant.\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn.utils.validation import _is_arraylike, _num_features\n",
    "\n",
    "\n",
    "def _is_arraylike_not_scalar(array):\n",
    "    return _is_arraylike(array) and not np.isscalar(array)\n",
    "\n",
    "\n",
    "def _check_boundary_response_method(estimator, response_method):\n",
    "    has_classes = hasattr(estimator, \"classes_\")\n",
    "    if has_classes and _is_arraylike_not_scalar(estimator.classes_[0]):\n",
    "        msg = \"Multi-label and multi-output multi-class classifiers are not supported\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if has_classes and len(estimator.classes_) > 2:\n",
    "        if response_method not in {\"auto\", \"predict\"}:\n",
    "            msg = (\n",
    "                \"Multiclass classifiers are only supported when response_method is\"\n",
    "                \" 'predict' or 'auto'\"\n",
    "            )\n",
    "            raise ValueError(msg)\n",
    "        methods_list = [\"predict\"]\n",
    "    elif response_method == \"auto\":\n",
    "        methods_list = [\"decision_function\", \"predict_proba\", \"predict\"]\n",
    "    else:\n",
    "        methods_list = [response_method]\n",
    "\n",
    "    prediction_method = [getattr(estimator, method, None) for method in methods_list]\n",
    "    prediction_method = reduce(lambda x, y: x or y, prediction_method)\n",
    "    if prediction_method is None:\n",
    "        raise ValueError(\n",
    "            f\"{estimator.__class__.__name__} has none of the following attributes: \"\n",
    "            f\"{', '.join(methods_list)}.\"\n",
    "        )\n",
    "\n",
    "    return prediction_method\n",
    "\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    estimator,\n",
    "    X,\n",
    "    *,\n",
    "    grid_resolution=100,\n",
    "    eps=1.0,\n",
    "    plot_method=\"contourf\",\n",
    "    response_method=\"auto\",\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    ax=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if not grid_resolution > 1:\n",
    "        raise ValueError(\n",
    "            \"grid_resolution must be greater than 1. Got\" f\" {grid_resolution} instead.\"\n",
    "        )\n",
    "\n",
    "    if not eps >= 0:\n",
    "        raise ValueError(f\"eps must be greater than or equal to 0. Got {eps} instead.\")\n",
    "\n",
    "    possible_plot_methods = (\"contourf\", \"contour\", \"pcolormesh\")\n",
    "    if plot_method not in possible_plot_methods:\n",
    "        available_methods = \", \".join(possible_plot_methods)\n",
    "        raise ValueError(\n",
    "            f\"plot_method must be one of {available_methods}. \"\n",
    "            f\"Got {plot_method} instead.\"\n",
    "        )\n",
    "\n",
    "    num_features = _num_features(X)\n",
    "    if num_features != 2:\n",
    "        raise ValueError(f\"n_features must be equal to 2. Got {num_features} instead.\")\n",
    "\n",
    "    x0, x1 = _safe_indexing(X, 0, axis=1), _safe_indexing(X, 1, axis=1)\n",
    "\n",
    "    x0_min, x0_max = x0.min() - eps, x0.max() + eps\n",
    "    x1_min, x1_max = x1.min() - eps, x1.max() + eps\n",
    "\n",
    "    xx0, xx1 = np.meshgrid(\n",
    "        np.linspace(x0_min, x0_max, grid_resolution),\n",
    "        np.linspace(x1_min, x1_max, grid_resolution),\n",
    "    )\n",
    "\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        # we need to preserve the feature names and therefore get an empty dataframe\n",
    "        X_grid = X.iloc[[], :].copy()\n",
    "        X_grid.iloc[:, 0] = xx0.ravel()\n",
    "        X_grid.iloc[:, 1] = xx1.ravel()\n",
    "    else:\n",
    "        X_grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
    "\n",
    "    pred_func = _check_boundary_response_method(estimator, response_method)\n",
    "    response = pred_func(X_grid)\n",
    "\n",
    "    # convert classes predictions into integers\n",
    "    if pred_func.__name__ == \"predict\" and hasattr(estimator, \"classes_\"):\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.classes_ = estimator.classes_\n",
    "        response = encoder.transform(response)\n",
    "\n",
    "    if response.ndim != 1:\n",
    "        if is_regressor(estimator):\n",
    "            raise ValueError(\"Multi-output regressors are not supported\")\n",
    "\n",
    "        # TODO: Support pos_label\n",
    "        response = response[:, 1]\n",
    "\n",
    "    if xlabel is None:\n",
    "        xlabel = X.columns[0] if hasattr(X, \"columns\") else \"\"\n",
    "\n",
    "    if ylabel is None:\n",
    "        ylabel = X.columns[1] if hasattr(X, \"columns\") else \"\"\n",
    "\n",
    "    if plot_method not in (\"contourf\", \"contour\", \"pcolormesh\"):\n",
    "        raise ValueError(\"plot_method must be 'contourf', 'contour', or 'pcolormesh'\")\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    plot_func = getattr(ax, plot_method)\n",
    "\n",
    "    surface_ = plot_func(xx0, xx1, response.reshape(xx0.shape), **kwargs)\n",
    "\n",
    "    if xlabel is not None or not ax.get_xlabel():\n",
    "        xlabel = xlabel if xlabel is None else xlabel\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "    if ylabel is not None or not ax.get_ylabel():\n",
    "        ylabel = ylabel if ylabel is None else ylabel\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156232c-5df8-4cc9-a737-c89a6c370c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap([\"lightblue\", \"peachpuff\", \"palegreen\"])\n",
    "\n",
    "# Plot the decision regions\n",
    "ax = plot_decision_boundary(nn_model, x, cmap=cmap_light)\n",
    "\n",
    "# Plot the decision boundary\n",
    "ax = plot_decision_boundary(\n",
    "    nn_model,\n",
    "    x,\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    levels=[0, 1],\n",
    "    colors=\"black\",\n",
    ")\n",
    "\n",
    "# Overlay data points from iris dataset\n",
    "ax = sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    "    style=y_species,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247f41a-50a1-484c-bdd0-1acf7d826232",
   "metadata": {},
   "source": [
    "Now you will see some problems with the nearest neighbor algorithm. These are:\n",
    "\n",
    "1. Overconfidence\n",
    "2. Memory and Speed\n",
    "3. Sensitive to Noise\n",
    "4. Sensitive to changes in Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5e49a-069a-4b33-9242-0707e29553b9",
   "metadata": {},
   "source": [
    "**Overconfidence**\n",
    "\n",
    "When making inference on a point far from the training points, the model can be very confident about its prediction.\n",
    "\n",
    "Example: the following test point is much closer to any *virginica* point than any other species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445901d-b6f8-4668-97f4-a7fe8b9e533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test point far from all other data points\n",
    "test_point_3 = [10, 8]\n",
    "\n",
    "_, closest_point_3 = nearest_neighbour(test_point_3)\n",
    "\n",
    "# plot training dataset\n",
    "sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    ")\n",
    "\n",
    "# plot the test point as an 'x'\n",
    "sns.scatterplot(\n",
    "    x=[test_point_3[0]],\n",
    "    y=[test_point_3[1]],\n",
    "    marker=\"x\",\n",
    "    label=\"test point\",\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "# plot a ring around the nearest datapoint\n",
    "sns.scatterplot(\n",
    "    x=[closest_point_3[feature_1]],\n",
    "    y=[closest_point_3[feature_2]],\n",
    "    marker=\"o\",\n",
    "    label=\"nearest training point\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a69ab5-85d5-4ff6-828f-70cd3483d8c9",
   "metadata": {},
   "source": [
    "**Question**: Is this a reasonable behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b586c8-a65e-4d84-8a6c-aadb1c8c9013",
   "metadata": {},
   "source": [
    "**Memory and speed**\n",
    "\n",
    "The Nearest Neighbor models needs to store all points in the training dataset. \n",
    "\n",
    "Additionally, at inference it computes every distance from the test point to the training set points.\n",
    "\n",
    "For large datasets it becomes infeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87480c8-24b5-4bb7-8803-2165085165ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a performance timer from the standard library\n",
    "from time import perf_counter\n",
    "\n",
    "test_point_4 = np.array([4, 4])\n",
    "\n",
    "\n",
    "def measure_nn_speed(n_samples, test_point=test_point_4):\n",
    "    # Generate toy dataset with scikit-learn dataset functions.\n",
    "    X, y = datasets.make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,  # Two features\n",
    "        n_informative=2,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,  # Two classes\n",
    "    )\n",
    "\n",
    "    # Create a NN model\n",
    "    model = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\", n_jobs=1)\n",
    "\n",
    "    # Measure fit time\n",
    "    start_fit = perf_counter()\n",
    "    model.fit(X, y)\n",
    "    fit_time = perf_counter() - start_fit\n",
    "\n",
    "    # Measure prediction time\n",
    "    start_predict = perf_counter()\n",
    "    model.predict(test_point.reshape(1, -1))\n",
    "    predict_time = perf_counter() - start_predict\n",
    "\n",
    "    return fit_time, predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76203342-bcfa-4bae-9b3e-b434c522ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select different sizes of samples. Logspace will return exponentially separated\n",
    "# points. 10, 100, 1000, ...\n",
    "n_samples_options = np.logspace(start=1, stop=8, num=8, dtype=np.int32)\n",
    "\n",
    "# Measure time to fit and predict for each dataset size\n",
    "fit_time, predict_time = zip(\n",
    "    *[measure_nn_speed(n_samples) for n_samples in n_samples_options]\n",
    ")\n",
    "\n",
    "# Plot times\n",
    "plt.plot(n_samples_options, fit_time, label=\"fit\")\n",
    "plt.plot(n_samples_options, predict_time, label=\"predict\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"duration (s)\")\n",
    "plt.xlabel(\"dataset size (# samples)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c35bb-646a-4a97-b4e6-cf65ee4d6bda",
   "metadata": {},
   "source": [
    "**Noise sensitivity**\n",
    "\n",
    "If a single mistake is introduced in the dataset the **decision boundaries** can change drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031c043-c5a6-4103-8107-6785dbf203e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The species of the 134th element is virginica\n",
    "index = 134\n",
    "print(y_species[index])\n",
    "\n",
    "# Make a copy of the targets\n",
    "y_species_corrupted = y_species.copy()\n",
    "\n",
    "# And change the label of a single entry\n",
    "y_species_corrupted[index] = \"versicolor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d500557-8389-4d43-b884-e91fb87dfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit new NN model with corrupted labels\n",
    "nn_model_with_corrupted_data = KNeighborsClassifier(n_neighbors=1).fit(\n",
    "    x, y_species_corrupted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1c0ae-aee2-44f6-86fb-93bb28a44065",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Create a subplot on the left\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"Decision regions and boundary of original model\")\n",
    "\n",
    "# Plot the decision regions and boundary of original model\n",
    "plot_decision_boundary(nn_model, x, cmap=cmap_light, ax=ax1)\n",
    "\n",
    "plot_decision_boundary(\n",
    "    nn_model,\n",
    "    x,\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax1,\n",
    "    levels=[0, 1],\n",
    "    colors=\"black\",\n",
    ")\n",
    "\n",
    "# Overlay data points from iris dataset\n",
    "sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species,\n",
    "    style=y_species,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "# Create a subplot on the right\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "\n",
    "ax2.set_title(\"Decision regions and boundary of model trained with corrupted labels\")\n",
    "\n",
    "# Plot the decision regions and boundary of model with corrupted data\n",
    "plot_decision_boundary(nn_model_with_corrupted_data, x, cmap=cmap_light, ax=ax2)\n",
    "\n",
    "plot_decision_boundary(\n",
    "    nn_model_with_corrupted_data,\n",
    "    x,\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax2,\n",
    "    levels=[0, 1],\n",
    "    colors=\"black\",\n",
    ")\n",
    "\n",
    "# Overlay data points from iris dataset\n",
    "sns.scatterplot(\n",
    "    data=iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_species_corrupted,\n",
    "    style=y_species_corrupted,\n",
    "    ax=ax2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6408-146c-450f-a2c7-3d3023bfd740",
   "metadata": {},
   "source": [
    "**Sensitivity to scale**\n",
    "\n",
    "So far we have been using **cm** as units for length.\n",
    "\n",
    "What happens if we change cm to meters for a single feature? \n",
    "\n",
    "How does this choice affect predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cc9d-fbc5-4b28-a616-98592a6fb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the feature array so that the first feature is in meters\n",
    "x2 = x.copy()\n",
    "x2[\"petal length (m)\"] = x2[\"petal length (cm)\"] / 100\n",
    "x2 = x2.reindex(columns=[\"petal length (m)\", \"petal width (cm)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab03ee3-9044-46d6-887b-4ffbcb80feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with new feature array\n",
    "nn_model_2 = KNeighborsClassifier(n_neighbors=1).fit(x2, y_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a998c-485b-478b-8398-7e8d60d153c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wish to classify a new flower. These are the measurements of the flower with\n",
    "# two different units of measurements\n",
    "test_point_5_cm = np.array([2.7, 0.7])  # (petal length cm, petal width cm)\n",
    "test_point_5_m = np.array([0.027, 0.7])  # (petal length m, petal width cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0562c-ba99-4cb3-b6c5-0388f22d6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with cm\n",
    "predicted_species_cm = nn_model.predict(test_point_5_cm.reshape(1, -1))[0]\n",
    "\n",
    "# Predict with meters\n",
    "predicted_species_m = nn_model_2.predict(test_point_5_m.reshape(1, -1))[0]\n",
    "\n",
    "print(\n",
    "    f\"Species predictions: with cm as units = {predicted_species_cm}, with m as units = {predicted_species_m}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea919e-7ec2-48f3-b92a-46c5508a0556",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "The nearest neighbor algorithm is intuitive and simple to implement. But \n",
    "\n",
    "* is very sensitive to errors and scaling (recall underfitting/overfitting discussion?)\n",
    "* requires lots of memory and computation\n",
    "\n",
    "What are some alternatives? \n",
    "\n",
    "Is there a way that involves less computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d403065-6c3f-46e3-b81a-630b1357d674",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b2bb6-62c9-47a3-845f-f5aa0dd8d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only data of the setosa and virginica species\n",
    "separable_dataset = iris[y_species.isin([\"setosa\", \"versicolor\"])]\n",
    "separable_labels = y_species[y_species.isin([\"setosa\", \"versicolor\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88608cbb-09c6-4049-b98b-d72ae4d4c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points\n",
    "sns.scatterplot(\n",
    "    data=separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=separable_labels,\n",
    "    style=separable_labels,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378921a-8edb-4f15-bf60-e826ae944df0",
   "metadata": {},
   "source": [
    "Data points from the different species are clearly separated. In fact, they can be separated by a line, and new points can be classified depending\n",
    "on which side of the line they fall.\n",
    "\n",
    "This is what a linear classifier does, in essence. Here we will use the linear support vector classifier (SVC). See [here](https://scikit-learn.org/stable/modules/svm.html#svc) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24a225-6d3a-4034-9a7a-6d023c1c2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Linear SVC model from the SVM module in scikit-learn\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72378ea-507a-41a8-949c-f03855e175d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear support vector classifier (SVC) on the separable dataset\n",
    "clf = LinearSVC().fit(\n",
    "    separable_dataset[[feature_1, feature_2]],\n",
    "    separable_labels,\n",
    ")\n",
    "\n",
    "# Plot the decision regions of the linear classifier\n",
    "ax = plot_decision_boundary(\n",
    "    clf,\n",
    "    separable_dataset[[feature_1, feature_2]],\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# Plot the decision boundary of the linear classifier\n",
    "plot_decision_boundary(\n",
    "    clf,\n",
    "    separable_dataset[[feature_1, feature_2]],\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    colors=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=separable_dataset,\n",
    "    x=separable_dataset[feature_1],\n",
    "    y=separable_dataset[feature_2],\n",
    "    hue=separable_labels,\n",
    "    style=separable_labels,\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8938b23-9a59-4608-89bf-16f40f466859",
   "metadata": {},
   "source": [
    "This linear model is small in memory and very fast.\n",
    "\n",
    "But in most cases will not generate perfect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775844c5-9731-4730-865d-abe45e762f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only data of the versicolor and virginica species\n",
    "non_separable_species = [\"virginica\", \"versicolor\"]\n",
    "non_separable_dataset = iris[y_species.isin(non_separable_species)]\n",
    "non_separable_labels = y_species[y_species.isin(non_separable_species)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a70256-1560-4f51-8c06-6c1a23c24734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=non_separable_labels,\n",
    "    style=non_separable_labels,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0692-d6ef-45bd-bd5e-4008e0de799b",
   "metadata": {},
   "source": [
    "No line will cut the two sets of points cleanly.\n",
    "\n",
    "When we fit a linear model prediction errors are unavoidable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91f1c2-c1b7-47a0-aade-46e41ac1f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear support vector classifier (SVC) on the separable dataset\n",
    "clf = LinearSVC().fit(\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    non_separable_labels,\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the decision regions of the linear classifier\n",
    "ax = plot_decision_boundary(\n",
    "    clf,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# Plot the decision boundary of the linear classifier\n",
    "plot_decision_boundary(\n",
    "    clf,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=non_separable_labels,\n",
    "    style=non_separable_labels,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c93936-03cf-4248-af68-00fd33f278e2",
   "metadata": {},
   "source": [
    "Support vector machines can be non-linear by applying the so called kernel-trick. If interested in details check the [An Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4d9fd-d070-441d-b4ad-d8f5449ed9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the support vector classifier model from scikit-learn\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e8d8a-f132-4874-b41d-23de0d238fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a non-linear support vector machine on the non-linearly-separable dataset\n",
    "clf = SVC(C=100, gamma=10).fit(\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    non_separable_labels,\n",
    ")\n",
    "\n",
    "# Plot the decision regions of the linear classifier\n",
    "ax = plot_decision_boundary(\n",
    "    clf,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# Plot the decision boundary of the linear classifier\n",
    "plot_decision_boundary(\n",
    "    clf,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=non_separable_labels,\n",
    "    style=non_separable_labels,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4eb25-34bd-464f-a72c-6c3af3a476b7",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4a1b6-7da9-459f-90f8-0660472e7b5c",
   "metadata": {},
   "source": [
    "**Another simple idea**: Use simple binary decisions to discriminate between points. Use a sequence or tree of decisions to bin a test point into its correct species.\n",
    "\n",
    "Example binary decision: whether the petal length  5.1 cm, or the petal width is  1.75 cm.\n",
    "\n",
    "Can be nested: If petal length > 5.2 cm, and petal width < 1.3 then predict setosa.\n",
    "\n",
    "Each decision splits feature space in two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24caa0-606a-4a6d-a53a-d1513b283366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset points\n",
    "ax = sns.scatterplot(\n",
    "    data=non_separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=non_separable_labels,\n",
    "    style=non_separable_labels,\n",
    ")\n",
    "\n",
    "# Draw horizontal line at y = 1.75\n",
    "ax.axhline(1.75, color=\"gray\", linewidth=3, alpha=0.2)\n",
    "\n",
    "# Draw vertical line at x = 4.95\n",
    "ax.axvline(4.95, color=\"blue\", linewidth=3, alpha=0.2, ymax=0.5)\n",
    "\n",
    "# add text to label regions\n",
    "ax.text(4.2, 2.15, f\"width >= 1.75\")\n",
    "ax.text(2.9, 1.5, f\"width < 1.75\\n& length < 4.95\")\n",
    "ax.text(5.5, 1.1, f\"width < 1.75\\n& length >= 4.95\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35493f25-b8dc-4f8a-846d-01a90c1b25d4",
   "metadata": {},
   "source": [
    "Decision trees for classification can be created algorithmically. \n",
    "\n",
    "Multiple algorithms are available, here we will use the default algorithm from `scikit-learn` (ID3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f5693-74d7-43db-914c-f7334231f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Decision Tree Classifier model from scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124c301-c081-4b52-ac2c-551dc0d6609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree classifier model\n",
    "# Check the scikit-learn documentation to see possible configurations\n",
    "tree_model = DecisionTreeClassifier(max_depth=2, min_impurity_decrease=0.01)\n",
    "\n",
    "# Fit to dataset\n",
    "tree_model.fit(\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    non_separable_labels,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbded4b3-1b75-4c52-b37b-954591df573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot_tree function from tree tools in scikit-learn\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Create a new figure\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Visualize the trained decision trees\n",
    "plot_tree(\n",
    "    tree_model,\n",
    "    feature_names=[feature_1, feature_2],\n",
    "    class_names=tree_model.classes_,\n",
    "    impurity=False,\n",
    "    label=\"root\",\n",
    "    rounded=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Add labels to decisions\n",
    "ax.text(0.43, 0.66, \"yes\")\n",
    "ax.text(0.73, 0.66, \"no\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2010422-6e10-44ec-96d3-7e030ce0b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary of the decision tree classifier\n",
    "ax = plot_decision_boundary(\n",
    "    tree_model,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# Plot the decision boundary of the decision tree classifier\n",
    "plot_decision_boundary(\n",
    "    tree_model,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=non_separable_labels,\n",
    "    style=non_separable_labels,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93415a72-3e20-4267-a3d5-8835a1243690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another tree with max_depth = 4\n",
    "tree_model = DecisionTreeClassifier(max_depth=4, min_impurity_decrease=0.01)\n",
    "\n",
    "# Fit to data\n",
    "tree_model.fit(\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    non_separable_labels,\n",
    ")\n",
    "\n",
    "# Fit to dataset\n",
    "tree_model.fit(\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    non_separable_labels,\n",
    ")\n",
    "\n",
    "# Plot the decision boundary of the linear classifier\n",
    "ax = plot_decision_boundary(\n",
    "    tree_model,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    cmap=ListedColormap([\"lightblue\", \"peachpuff\"]),\n",
    ")\n",
    "\n",
    "# Plot the decision boundary of the linear classifier\n",
    "plot_decision_boundary(\n",
    "    tree_model,\n",
    "    non_separable_dataset[[feature_1, feature_2]],\n",
    "    plot_method=\"contour\",\n",
    "    levels=[0],\n",
    "    ax=ax,\n",
    "    colors=\"black\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Overlay the dataset points\n",
    "sns.scatterplot(\n",
    "    data=non_separable_dataset,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=non_separable_labels,\n",
    "    style=non_separable_labels,\n",
    "    ax=ax,\n",
    "    zorder=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef8efe-a73a-4c2e-909a-185bde135146",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ccff5-f746-49cd-99a2-539f3bac1adf",
   "metadata": {},
   "source": [
    "### **Exercise**: (10 min)\n",
    "\n",
    "Research what is a random forest.\n",
    "\n",
    "Build random forest classifier with scikit learn. \n",
    "\n",
    "Plot decision boundary and regions. \n",
    "\n",
    "Go to this [website](http://cs.stanford.edu/people/karpathy/svmjs/demo/demoforest.html) and play with RF parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360f277-78ae-47c5-9dd5-fece89164db8",
   "metadata": {},
   "source": [
    "## 4. How to evaluate your model?\n",
    "\n",
    "You have seen multiple models for Iris flower classification.\n",
    "\n",
    "Which model is the best fit?\n",
    "\n",
    "How can we be confident about the predictions of a model, or evaluate its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8829f-8c4b-4842-a251-248a55ef6c61",
   "metadata": {},
   "source": [
    "### Training and test split\n",
    "\n",
    "We could use the training data to count the number of correct and erroneous predictions.\n",
    "\n",
    "However this is a bad choice, as the Nearest Neighbor will always have 0 errors (can you see why?). \n",
    "\n",
    "In general, some models are very flexible and can fit any dataset.\n",
    "\n",
    "Others are rigid - like the linear SVM - and will not perfectly fit all datasets.\n",
    "\n",
    "Using the training data will not provide a clear picture of prediction accuracy for new points.\n",
    "\n",
    "**Solution**: Split the dataset into two parts: one for **training** another for evaluation or **testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0d74f-9d2a-4b5a-9bf8-9d2627ee049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from scikit-learn module for model selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32a55f-89ce-481f-a1e8-d4259d0c802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset and labels into test and train. Test dataset is 30% of all data\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y_species, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da46279-eda9-4bc4-bf39-2a4193013208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the dataset\n",
    "sns.scatterplot(data=iris, x=feature_1, y=feature_2, hue=y_species)\n",
    "\n",
    "# with circles around the training set\n",
    "sns.scatterplot(\n",
    "    data=train_x,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    "    label=\"train set\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5417b92-0470-4c62-8932-507bbbe1bb8e",
   "metadata": {},
   "source": [
    "Splits are usually done randomly to avoid selection bias.\n",
    "\n",
    "Sometimes random sampling can introduce imbalances to both training and test dataset.\n",
    "\n",
    "In this case stratified sampling is a better approach (such as `scikit-learn` [Stratified Shuffle Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn-model-selection-stratifiedshufflesplit))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bf89f-d0f2-40ed-8598-4ad1c9b0aa8d",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43991440-f3ff-4499-862c-4150d270fb71",
   "metadata": {},
   "source": [
    "There are many measures of performance.\n",
    "\n",
    "Accuracy, which is percentage of correct predictions, is commonly used for classification.\n",
    "\n",
    "Other metrics will provide different information on the model's performace.\n",
    "\n",
    "See the list of [classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) available in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e9955-607d-4268-821c-cc19ec7acfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new Nearest Neighbor model\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit with train split\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# predict on the test data\n",
    "test_predictions = model.predict(test_x)\n",
    "\n",
    "# compare to ground truth\n",
    "is_correct = test_predictions == test_y\n",
    "\n",
    "# print first results\n",
    "is_correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96c6fa-c6fd-46ca-ba9b-1ea46e7e27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "n_correct_predictions = is_correct.sum()\n",
    "accuracy = n_correct_predictions / len(test_x)\n",
    "\n",
    "print(f\"Nearest Neighbor model accuracy = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943a722-2438-4102-9fe2-6a5a4a47e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn provides an easy way of evaluating models\n",
    "score = model.score(test_x, test_y)\n",
    "\n",
    "print(f\"Model score = {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5a188-a82e-4887-bbc9-521f8bc1540c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "To evaluate a model:\n",
    "1. Split dataset into train and test\n",
    "2. Fit model with training data\n",
    "3. Select relevant performance metrics\n",
    "4. Evaluate with test data\n",
    "\n",
    "This is done succinctly with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ea229-4365-4618-b034-4317c241838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y_species, test_size=0.3)\n",
    "\n",
    "# create model\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit model with training data\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# evaluate model with test data\n",
    "# scikit learn has preselected accuracy as the relevant metric\n",
    "score = model.score(test_x, test_y)\n",
    "\n",
    "print(f\"Model score = {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c98c8b-3da7-4d3b-a783-3b5115aaa826",
   "metadata": {},
   "source": [
    "### **Exercise**: \n",
    "\n",
    "Compute the accuracy score of all previous models.\n",
    "\n",
    "Which one is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650c6a1-d257-4fb8-ba1c-36c5d95abafa",
   "metadata": {},
   "source": [
    "## 5. What is a regression task?\n",
    "\n",
    "When the target of supervised learning is a numerical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d74bf3-91c2-4bf2-9c80-20627f780131",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "* Trying to predict future $CO_2$ levels for the next decade. Here the feature vector **x** = year and target variable **y** = $CO_2$ levels.\n",
    "\n",
    "![historic atmospheric co2 data](https://research.noaa.gov/Portals/0/EasyGalleryImages/1/864/co2_data_mlo.png)\n",
    "\n",
    "(taken from [NOAA research news](https://research.noaa.gov/article/ArtMID/587/ArticleID/2764/Coronavirus-response-barely-slows-rising-carbon-dioxide), Monday, June 7, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c108b-75db-4549-8f0a-7cd754e79b08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751c4e0-6d12-4fd0-ada7-63eec0fcf137",
   "metadata": {},
   "source": [
    "Let use scikit-learn to generate synthetic data for a regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d0374-e314-4b8b-bcd3-ed10f11b122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import make_regression function from scikit-learn datasets module\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a random dataset for regression with some noise and 200 points\n",
    "features, target = make_regression(n_features=1, noise=10, n_samples=200)\n",
    "\n",
    "# Use seaborn to generate a scatterplot\n",
    "sns.scatterplot(x=features.flatten(), y=target);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862cdcf-b3b2-4e89-adc2-aa15711bc9d5",
   "metadata": {},
   "source": [
    "A linear regression model assumes that there is a **linear** relation between the features and the target variable\n",
    "\n",
    "$$ {\\bf y} = m {\\bf x} + b $$\n",
    "\n",
    "The parameters $m$ (slope) and $b$ (bias) that best \"fit\" the data points can be found algorithmically.\n",
    "\n",
    "How good a model fits the data is determined by minimizing some **loss** or error.\n",
    "\n",
    "In the case of the linear model, the loss is measured by the Mean Squared Error (MSE), but we won't delve into details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c6f3c-b53f-4329-a383-70e048ea1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Linear Regression model from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit a linear model to the example data\n",
    "linear_model = LinearRegression().fit(features, target)\n",
    "\n",
    "# Plot the datapoints. x = features, y = target value\n",
    "ax = sns.scatterplot(x=features.flatten(), y=target)\n",
    "\n",
    "x_min = features.min()\n",
    "x_max = features.max()\n",
    "\n",
    "# Generate a prediction using the linear model on the example data\n",
    "pred = linear_model.predict([[x_min], [x_max]])\n",
    "\n",
    "# Plot the predicted line\n",
    "ax.plot([x_min, x_max], pred, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# Add labels to axis\n",
    "ax.set_xlabel(\"x = Features\")\n",
    "ax.set_ylabel(\"y = Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec5dc9-86cc-464c-ac42-888b63cad733",
   "metadata": {},
   "source": [
    "Once fitted, a prediction for points outside the dataset is computed with the same formula.\n",
    "\n",
    "$$ {\\bf y_{pred}} = m {\\bf x_{test}} + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b6482-f153-4924-9b53-1c82f3f73aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new test point at x = 2\n",
    "test_point = [2]\n",
    "\n",
    "# Use the linear model to predict its target value\n",
    "predicted_value = linear_model.predict([test_point])[0]\n",
    "\n",
    "# Plot the datapoints. x = features, y = target value\n",
    "ax = sns.scatterplot(x=features.flatten(), y=target)\n",
    "\n",
    "# Plot the predicted line\n",
    "ax.plot([x_min, x_max], pred, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# Draw a point at the test point with its predicted value\n",
    "plt.scatter(test_point, [predicted_value], color=\"red\")\n",
    "\n",
    "y_min = pred.min()\n",
    "\n",
    "# Draw a vertical arrow from the x-axis at x = test_point to its predicted value\n",
    "ax.arrow(\n",
    "    2,\n",
    "    y_min,\n",
    "    0,\n",
    "    predicted_value - y_min,\n",
    "    color=\"red\",\n",
    "    head_width=0.1,\n",
    "    head_length=10,\n",
    "    length_includes_head=True,\n",
    ")\n",
    "\n",
    "# Draw a horizontal arrow from the point (x, y) = (test_point, predicted_value) to\n",
    "# the y-axis at y = predicted_value\n",
    "ax.arrow(\n",
    "    test_point[0],\n",
    "    predicted_value,\n",
    "    -test_point[0] + x_min,\n",
    "    0,\n",
    "    color=\"red\",\n",
    "    head_width=10,\n",
    "    head_length=0.1,\n",
    "    length_includes_head=True,\n",
    ")\n",
    "\n",
    "# Add the linear formula to the plot\n",
    "ax.text(-1, 50, \"y = mx + b\")\n",
    "\n",
    "# Add labels to axis\n",
    "ax.set_xlabel(\"x = Features\")\n",
    "ax.set_ylabel(\"y = Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e7bce-352b-4288-bedd-fdcac275be13",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f6d8b-9398-4ad5-ac71-a814f1e621f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another dataset\n",
    "# here target_ideal is a nonlinear function of x\n",
    "features = np.arange(0, 100, 2.0)\n",
    "target_ideal = np.sin(x / 10) + (x / 50) ** 2\n",
    "\n",
    "# add some noise to our target variable target_ideal\n",
    "target = target_ideal + np.random.normal(size=len(y_ideal)) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35311f62-4176-460e-bbb4-cd22eec36c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatter points (x = features, y = target)\n",
    "ax = sns.scatterplot(x=features, y=target)\n",
    "\n",
    "# add title\n",
    "ax.set_title(\"Non linear dataset\")\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"x = Features\")\n",
    "ax.set_ylabel(\"y = Target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a41f23-f310-4b82-a1b0-0e01b83542c8",
   "metadata": {},
   "source": [
    "As with classification, linear models are very rigid and will produce bad predictions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf849e-b04a-472a-a523-8830b94c1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit linear model\n",
    "lin = LinearRegression()\n",
    "lin.fit(features.reshape(-1, 1), target)\n",
    "\n",
    "# Predict on a range of test points\n",
    "test_points = np.linspace(0, 100, 1000)\n",
    "lin_reg_fit = lin.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "# plot the fitted linear model and random forest\n",
    "sns.scatterplot(x=features, y=target)\n",
    "\n",
    "sns.lineplot(x=test_points, y=lin_reg_fit, color=\"red\", label=\"linear regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1efbd0-b842-4206-9b09-fddf3a8cded0",
   "metadata": {},
   "source": [
    "How else can we predict target value of a test point using features?\n",
    "\n",
    "**Simple idea revisited**: Use the nearest neighbor's target value as a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82150fbd-0db8-4b8c-9ea2-cb008df21a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Nearest Neighbor Regression model from scikit-learn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# fit linear model\n",
    "nn_reg = KNeighborsRegressor(n_neighbors=1)\n",
    "nn_reg.fit(features.reshape(-1, 1), target)\n",
    "\n",
    "# Predict on a range of test points\n",
    "test_points = np.linspace(0, 100, 1000)\n",
    "nn_reg_fit = nn_reg.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "# plot the fitted linear model and random forest\n",
    "sns.scatterplot(x=features, y=target)\n",
    "\n",
    "sns.lineplot(x=test_points, y=nn_reg_fit, color=\"red\", label=\"nearest neighbor\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843571b1-7647-4e25-86e8-376784f52bb4",
   "metadata": {},
   "source": [
    "Nearest neighbor regression suffers from the same problems as nearest neighbor classification:\n",
    "    \n",
    "* Sensitive to noise\n",
    "* Heavy on computation and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096a98d-e9ff-4869-a125-b7874c805fda",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa980005-fdef-4c5f-855e-948d253c0c0a",
   "metadata": {},
   "source": [
    "Similarly the Random Forest model can be adapted for regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddd23a-b6dc-4657-a471-1b992cd9b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Nearest Neighbor Regression model from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit linear model\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(features.reshape(-1, 1), target)\n",
    "\n",
    "# predict on a range of test points\n",
    "test_points = np.linspace(0, 100, 1000)\n",
    "rf_reg_fit = rf_reg.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "# plot the fitted linear model and random forest\n",
    "sns.scatterplot(x=features, y=target)\n",
    "\n",
    "sns.lineplot(x=test_points, y=rf_reg_fit, color=\"red\", label=\"random forest\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9c37-f7f2-4aed-a0e0-cbf648d2436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "sns.scatterplot(x=features, y=target, color=\"black\", label=\"train dataset\", zorder=4)\n",
    "\n",
    "# plot the all fitted models\n",
    "sns.lineplot(x=test_points, y=rf_reg_fit, label=\"random forest\")\n",
    "sns.lineplot(x=test_points, y=nn_reg_fit, label=\"nearest neighbors\")\n",
    "sns.lineplot(x=test_points, y=lin_reg_fit, label=\"linear\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0025b02-bdb8-4961-b9c0-f0e8fa148a65",
   "metadata": {},
   "source": [
    "**Which is the best predictive model?**\n",
    "\n",
    "**How to evaluate regression models?**\n",
    "\n",
    "Similar procedure as classification but different metric.\n",
    "\n",
    "How to measure good fit?\n",
    "\n",
    "One option is to use **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i = 1}^{n} (y_{true} - y_{pred})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703ed90-58f7-4ac4-a77e-65b01cf15bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training dataset\n",
    "ax = sns.scatterplot(x=features, y=target)\n",
    "\n",
    "# plot fitted line\n",
    "sns.lineplot(x=features, y=lin_reg_fit, color=\"red\", label=\"linear regression\")\n",
    "\n",
    "# plot errors\n",
    "for x, y_true, y_pred in zip(features, target, lin_reg_fit):\n",
    "    ax.plot([x, x], [y_true, y_pred], alpha=0.5, color=\"black\", linewidth=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf973945-fa1e-4175-baa3-ad63ac4cad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the array of differences in prediction and true value\n",
    "error = target - lin_reg_fit\n",
    "\n",
    "# compute the square of each error\n",
    "squared_error = error ** 2\n",
    "\n",
    "# compute the mean\n",
    "MSE = squared_error.mean()\n",
    "\n",
    "print(f\"MSE of linear model on training dataset: {MSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28d190-f4f4-452a-b849-1dd2c0b183cc",
   "metadata": {},
   "source": [
    "Scikit-learn implements MSE and offers multiple regression metrics.\n",
    "\n",
    "Each metric has its benefits and pitfalls. Choice depends on use case.\n",
    "\n",
    "Visit this [site](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) to see available regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157127b9-2e28-4d05-a1f2-b973db38ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn provides tools for easy computation of MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use scikit-learn function to compute MSE\n",
    "MSE = mean_squared_error(lin_reg_fit, target)\n",
    "\n",
    "print(f\"Score of linear model: {MSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad1c42-86c4-4074-9705-bba59816a054",
   "metadata": {},
   "source": [
    "Split the dataset into train and test to make a fair comparison between different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dbc67-6035-4bb4-aae5-962a89449fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target into train and test. Test is 30% of all data.\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.3)\n",
    "\n",
    "# iterate over model types\n",
    "for model in [\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(n_neighbors=1),\n",
    "    RandomForestRegressor(),\n",
    "]:\n",
    "    # fit the model to training data\n",
    "    model.fit(train_x.reshape(-1, 1), train_y)\n",
    "\n",
    "    # use fitted model to predict in test data\n",
    "    y_predict = model.predict(test_x.reshape(-1, 1))\n",
    "\n",
    "    # compute MSE using the predictions and ground truth\n",
    "    mse = mean_squared_error(y_predict, test_y)\n",
    "\n",
    "    print(f\"{str(model):>34} mse = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce21ae2-68c0-4ad0-be74-946471a73fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test points\n",
    "sns.scatterplot(x=train_x, y=train_y, color=\"black\", marker=\"o\", label=\"train\")\n",
    "sns.scatterplot(x=test_x, y=test_y, color=\"black\", marker=\"x\", label=\"test\")\n",
    "\n",
    "# iterate over model types\n",
    "for model in [\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(n_neighbors=1),\n",
    "    RandomForestRegressor(),\n",
    "]:\n",
    "    # fit the model to training data\n",
    "    model.fit(train_x.reshape(-1, 1), train_y)\n",
    "\n",
    "    # generate predictions in range of data points\n",
    "    test_points = np.linspace(0, 100, 1000)\n",
    "    y_pred = model.predict(test_points.reshape(-1, 1))\n",
    "\n",
    "    # plot predicted line\n",
    "    sns.lineplot(x=test_points, y=y_pred, label=str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b84b2-6676-4ac8-ac34-ca6702cc0607",
   "metadata": {},
   "source": [
    "**What if number of features > 1?**\n",
    "\n",
    "In most cases multiple features are used for prediction.\n",
    "\n",
    "That is the same as saying feature vectors are multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b41456-6d6f-4923-9774-817181eb63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a synthetic dataset for regression with 2 features\n",
    "features_2d, target_2d = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_targets=1,\n",
    ")\n",
    "\n",
    "# visualise with seaborn\n",
    "# plot points at (x = feature 1, y = feature 2)\n",
    "# use the target variable to determine point size and colour\n",
    "grid = sns.relplot(\n",
    "    x=features_2d[:, 0],\n",
    "    y=features_2d[:, 1],\n",
    "    size=target_2d,\n",
    "    sizes=(40, 400),\n",
    "    alpha=0.5,\n",
    "    hue=target_2d,\n",
    ")\n",
    "\n",
    "# add axis labels\n",
    "grid.ax.set_xlabel(\"feature 1\")\n",
    "grid.ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bce6a3-8c72-48b2-8d80-eee6a55fc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    features_2d,\n",
    "    target_2d,\n",
    "    test_size=0.3,\n",
    ")\n",
    "\n",
    "# fit a Nearest Neighbor Regression model to training data\n",
    "model = KNeighborsRegressor(n_neighbors=1).fit(train_x, train_y)\n",
    "\n",
    "# create a mesh of points\n",
    "XX, YY = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
    "\n",
    "# predict on each point in mesh\n",
    "predictions = model.predict(np.c_[XX.flatten(), YY.flatten()])\n",
    "\n",
    "# compute the min and max value of predictions\n",
    "vmin, vmax = predictions.min(), predictions.max()\n",
    "\n",
    "# select colormap\n",
    "# see available colormaps at https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "cmap = \"plasma\"\n",
    "\n",
    "# plot\n",
    "ax = plt.pcolormesh(\n",
    "    XX, YY, predictions.reshape(XX.shape), vmin=vmin, vmax=vmax, cmap=cmap\n",
    ")\n",
    "\n",
    "# create a color bar to indicate mapping between columns and target values\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "# add label to color bar\n",
    "cbar.set_label(\"target\")\n",
    "\n",
    "# plot training data as small round points\n",
    "plt.scatter(\n",
    "    train_x[:, 0],\n",
    "    train_x[:, 1],\n",
    "    c=train_y,\n",
    "    s=20,\n",
    "    edgecolor=\"black\",\n",
    "    label=\"train\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    cmap=cmap,\n",
    ")\n",
    "\n",
    "# compute predictions at test points\n",
    "y_pred = model.predict(test_x)\n",
    "\n",
    "# compute prediction absolute error\n",
    "error = np.abs(test_y - y_pred)\n",
    "\n",
    "# plot test data as large square markers\n",
    "# color squares using true value of target variable\n",
    "# use absolute error to determine square size\n",
    "plt.scatter(\n",
    "    test_x[:, 0],\n",
    "    test_x[:, 1],\n",
    "    s=error,\n",
    "    c=test_y,\n",
    "    marker=\"s\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"test\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    cmap=cmap,\n",
    "    sizes=(20, 100),\n",
    ")\n",
    "\n",
    "# add legend to figure\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c0015-a5fd-4268-86b8-53c69ddc01af",
   "metadata": {},
   "source": [
    "### **Exercise**:\n",
    "\n",
    "Checkout the [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset).\n",
    "\n",
    "Load the features and target variables from `scikit-learn`.\n",
    "\n",
    "Split the data into training and test sets.\n",
    "\n",
    "Select any regression model(s) of your choice and fit to train data.\n",
    "\n",
    "Evaluate model fit with test data and MSE (and other metrics of your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0479f8-cd05-4649-ad94-77b33fd77ee8",
   "metadata": {},
   "source": [
    "## 6. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12eea6-60d6-4f0c-b9f7-12a7ad097a47",
   "metadata": {},
   "source": [
    "**What if we don't have any labels?**\n",
    "\n",
    "Often our data contains some **structure**.\n",
    "\n",
    "* Features from different classes might be separated (**separability**)\n",
    "\n",
    "* Similar objects might have similar features (**smoothness**)\n",
    "\n",
    "Often we wish find groupings or patterns in our data. This is called **clustering**.\n",
    "\n",
    "Datapoints in the same **cluster** are deemed to be similar under some measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2beaf76-29c0-4e1d-8a54-8f7cd6b9ba15",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "There are many algorithms for clustering. Here you will use k-means clustering.\n",
    "\n",
    "Scikit-learn has a [collection of clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html#clustering), including k-means.\n",
    "\n",
    "If interested, checkout an [explanation](https://www.youtube.com/watch?v=4b5d3muPQmA) of the k-means clustering algorithm or an [interactive simulation](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e24dfa-835c-4614-8a8a-f5ac94bf26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import make_blobs function in scikit-learn datasets module\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate synthetic dataset made up of 5 blobs\n",
    "X, y_true = make_blobs(n_features=2, n_samples=4000, centers=5)\n",
    "\n",
    "# plot synthetic dataset\n",
    "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1])\n",
    "\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60386f0b-6adb-4f69-a1fd-9870d428bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means clustering model from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create a new K-means clustering model.\n",
    "# specify 5 wanted clusters\n",
    "kmeans_model = KMeans(n_clusters=5)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_pred = kmeans_model.predict(X)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
    "\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbc006-44ef-4146-8107-3342ec39b24d",
   "metadata": {},
   "source": [
    "Clustering performance will depend on clustering parameters, choice of algorithm and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdef2f-442a-4313-9ca0-10bc3a2aa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with 3 clusters\n",
    "kmeans_model = KMeans(n_clusters=3)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_pred = kmeans_model.predict(X)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bb4f7-4ff7-4891-945e-38b85c897906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# repeat with other dataset\n",
    "X, y_target = make_circles(factor=0.2, n_samples=4000, noise=0.1)\n",
    "\n",
    "# create K means with 2 clusters\n",
    "kmeans_model = KMeans(n_clusters=2)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_pred = kmeans_model.predict(X)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a7a47-e7a9-467b-b0f2-b7db290050e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat with DBscan algorithm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# repeat with other dataset\n",
    "X, y_target = make_circles(factor=0.2, n_samples=4000, noise=0.1)\n",
    "\n",
    "# create DBSCAN model\n",
    "dbscan_model = DBSCAN(eps=0.15)\n",
    "\n",
    "# fit to dataset\n",
    "y_pred = dbscan_model.fit_predict(X)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cb941-547b-4923-9af0-b7a36857a2da",
   "metadata": {},
   "source": [
    "### **Exercise**: (15 min)\n",
    "\n",
    "Use K-means clustering on the iris dataset\n",
    "\n",
    "Can you recover the species separation?\n",
    "\n",
    "Research Affinity Propagation clustering and compare to K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74447f3e-fd3a-4dae-850f-8edbf98b4492",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139803c-3453-4251-a02b-8d2d8589578f",
   "metadata": {},
   "source": [
    "Often the data we collect can be very **high dimensional**. e.g. D > 1000\n",
    "\n",
    "This poses a problem as it is difficult to visualize anything greater than 3 dimensions.\n",
    "\n",
    "We can **project** this data down to a lower dimension. P << D, where P is typically 2 or 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a06b9-42d1-437e-86e4-17df9b7a9084",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "One of the simplest approach is to do a linear projection.\n",
    "\n",
    "**PCA** is a linear projection that aligns with the directions of highest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7f659-9f87-4f90-8206-707ba28dddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full iris dataset has 4 features\n",
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2527b6-326c-4438-9e8a-c97deffc4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# create a 2-dimensional PCA projection\n",
    "pca_model = PCA(n_components=2)\n",
    "\n",
    "# project the 4-dimensional iris dataset into 2-d points\n",
    "projected_iris = pca_model.fit_transform(iris)\n",
    "\n",
    "# plot projected points\n",
    "# use species to color points\n",
    "ax = sns.scatterplot(\n",
    "    x=projected_iris[:, 0], y=projected_iris[:, 1], hue=y_species, style=y_species\n",
    ")\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"PCA component 1\")\n",
    "ax.set_ylabel(\"PCA component 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5fdb7-dc08-469e-b3f4-5c9fc84f56f9",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b1a69-280e-4780-8aac-0d070ab54842",
   "metadata": {},
   "source": [
    "As a final example let explore a dataset of digits.\n",
    "\n",
    "Each data point is an grayscale image of a handwritten digit.\n",
    "\n",
    "The images are 8x8 pixels, so in total each point has 64 features.\n",
    "\n",
    "In this case a feature is the grayscale value of a single pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d318e-e983-4227-993c-7130887d2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_digits function from scikit-learn datasets module\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# load digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# extract data and target values\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# select a single data point\n",
    "# reshape to original 8x8 array\n",
    "digit = X[0].reshape(8, 8)\n",
    "\n",
    "# use matplotlib to show image\n",
    "plt.imshow(digit, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12aa9c-0efd-4edc-8081-d8e2a160289b",
   "metadata": {},
   "source": [
    "**How to visualize the whole dataset?**\n",
    "\n",
    "Use dimensionality reduction\n",
    "\n",
    "Lets try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65752d57-f479-4426-8629-35271684fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to project to 2 dimensions\n",
    "pca_digits = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# do a scatterplot, color points by digit\n",
    "sns.scatterplot(x=pca_digits[:, 0], y=pca_digits[:, 1], hue=y, palette=\"tab20\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e1d2c-0360-4abb-8c8b-edba92a06039",
   "metadata": {},
   "source": [
    "Some digits seem to cluster.\n",
    "\n",
    "Still, there is a lot of overlap.\n",
    "\n",
    "Lets try a different projection method.\n",
    "\n",
    "Now we will use a non-linear projection called **t-SNE**.\n",
    "\n",
    "Checkout the [paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) where t-SNE was introduced, or this amazing [blog](https://distill.pub/2016/misread-tsne/) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47892027-8ed2-4baa-83b3-6644032b5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# use TSNE to project to 2 dimensions\n",
    "tsne_digits = TSNE(\n",
    "    n_components=2,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ").fit_transform(X)\n",
    "\n",
    "# do a scatterplot, color points by digit\n",
    "sns.scatterplot(x=tsne_digits[:, 0], y=tsne_digits[:, 1], hue=y, palette=\"tab20\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d39150-d0ab-4e88-afaa-9c80d06091af",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7e01c-067c-47f5-9cae-5ad1459f9682",
   "metadata": {},
   "source": [
    "**Which algorithm to choose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e80a3-b8ff-4e18-8ce3-a07c94f38ff6",
   "metadata": {},
   "source": [
    "Short answer: It depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143b484-315c-4e02-8159-723891cf5495",
   "metadata": {},
   "source": [
    "No silver bullet, but often for classification it is sensible to first try a Support Vector Machine or Random Forest.\n",
    "\n",
    "This will give you an idea of how separable your data is. The next step is to try different features, and perhaps even collect more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214a328-0379-4c71-aad6-2e1d96e1b07f",
   "metadata": {},
   "source": [
    "**How much data do I need?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86aaa1-6a7f-4e03-a9f0-046d1592afc5",
   "metadata": {},
   "source": [
    "Short answer: It depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c65baf-518c-4a61-a70c-6e2c59c94029",
   "metadata": {},
   "source": [
    "It depends on how easy it is for your classifier to separate your data. Some problems are relatively easy and dont require lots of data, others such as species identification in images can require 10,000s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci",
   "language": "python",
   "name": "sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
