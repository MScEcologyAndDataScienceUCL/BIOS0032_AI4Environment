{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d1c2e1-3a0d-46f1-bb8f-c47b6b23a06b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/05_AI_for_Bioacoustics/05_AI_for_Bioacoustics_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519b66b-7870-4c8e-8ba7-7ceb7814d015",
   "metadata": {},
   "source": [
    "# Week 5: AI for Bioacoustics [Solutions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf4cc-a14c-437d-8c99-d24edfad0d78",
   "metadata": {},
   "source": [
    "In this week's practical we will explore computer audition applications in ecology, such as\n",
    "automated animal detection and species classification from audio sensor data!\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Setup](#2-setup)\n",
    "3. [Detecting Animal Sounds](#3-detecting-animal-sounds)\n",
    "4. [Identifying Sounds](#4-identifying-sounds)\n",
    "5. [Conclusion](#5-conclusion)\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or\n",
    "answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4acde-b379-4a49-93c2-38a8095be264",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a625520-aa73-4a25-894c-d6dc993f9166",
   "metadata": {},
   "source": [
    "### 1.1 Computer Audition\n",
    "\n",
    "Computer audition is the field of research that deals with the automatic analysis of audio signals.\n",
    "It intersects with many other fields, including machine learning, signal processing, and computer\n",
    "vision.\n",
    "\n",
    "**What does a computer hear**?\n",
    "\n",
    "- Audio files are a sequence of numbers representing the amplitude of the sound\n",
    "  wave at a given time.\n",
    "\n",
    "- The number of samples per second is called the **sampling rate**.\n",
    "\n",
    "<img alt=\"audio and sampling rate\" width=\"600\"\n",
    "src=\"https://cdn.shopify.com/s/files/1/1169/2482/files/Sampling_Rate_Cover_image.jpg?v=1654170259\"></img>\n",
    "\n",
    "**What tasks can we do with computer audition?**\n",
    "\n",
    "Computer audition is used in a wide range of applications, including:\n",
    "\n",
    "- Speech recognition: Siri, Alexa, Google Assistant\n",
    "- Music information retrieval: Spotify, Shazam\n",
    "- Audio classification: What is sounding in this audio?\n",
    "- Sound event detection: Transcription of audio into a sequence of events.\n",
    "\n",
    "<img alt=\"Sound event detection\" width=\"400\"\n",
    "src=\"http://d33wubrfki0l68.cloudfront.net/508a62f305652e6d9af853c65ab33ae9900ff38e/17a88/images/tasks/challenge2016/task3_overview.png\"></img>\n",
    "\n",
    "> Taken from the paper: Mesaros, A., Heittola, T., Diment, A., Elizalde, B., Shah, A., Vincent, E.,\n",
    "> ... & Virtanen, T. (2017, November). DCASE 2017 challenge setup: Tasks, datasets and baseline\n",
    "> system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events.\n",
    "\n",
    "Recently, deep learning has taken over the field of computer audition and is being used to solve\n",
    "many of the above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56935d83-a08f-4a2f-9ba6-45a3b3660833",
   "metadata": {},
   "source": [
    "### 1.2 Data Collection\n",
    "\n",
    "**Acoustic sensors** can be used to collect field recordings of animal sounds.\n",
    "\n",
    "Usually, these sensors are deployed statically in the field for a long periods of time and record\n",
    "sounds continuously. This is called **passive acoustic monitoring**.\n",
    "\n",
    "<img alt=\"passive acoustic monitoring\" width=\"400\"\n",
    "src=\"https://wittmann-tours.de/wp-content/uploads/2018/06/AudioMoth.jpg\"></img>\n",
    "\n",
    "Alternatively, recordings are actively directed towards a specific animal species or sound events.\n",
    "\n",
    "<img alt=\"active recording\" width=\"400\"\n",
    "src=\"https://s3.amazonaws.com/cdn.freshdesk.com/data/helpdesk/attachments/production/48032687175/original/xjI7Dy3Q9kaCZinr5vf4ksNxQbjK13Yv3A.jpg?1584552543\"></img>\n",
    "\n",
    "> Taken from the Macaulay Library blog post: [Sound recording\n",
    "> tips](https://support.ebird.org/en/support/solutions/articles/48001064298-sound-recording-tips)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef333a71-9414-4127-b196-76f37f5f4315",
   "metadata": {},
   "source": [
    "### 1.3 Acoustics for Ecology\n",
    "\n",
    "The sound at a site is a reflection of the species present in the area and other\n",
    "environmental factors.\n",
    "\n",
    "<img alt=\"composition of acoustic space\" width=\"500\"\n",
    "src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs12304-017-9288-5/MediaObjects/12304_2017_9288_Fig1_HTML.gif?as=webp\"></img>\n",
    "\n",
    "> Taken from the paper: Mullet, T.C., Farina, A. & Gage, S.H. The Acoustic\n",
    "> Habitat Hypothesis: An Ecoacoustics Perspective on Species Habitat Selection.\n",
    "> Biosemiotics 10, 319‚Äì336 (2017). https://doi.org/10.1007/s12304-017-9288-5\n",
    "\n",
    "If we could link the sounds to the species, we could use this information to\n",
    "study and monitor the biodiversity of an area.\n",
    "\n",
    "Acoustic sensors produce a lot of data, and it is not always easy to analyse.\n",
    "Can we use computer audition to help us?\n",
    "\n",
    "In this practical, we will explore the task of **animal sound detection** and **species\n",
    "classification**, using both manual and automated methods.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386638-3d9b-4e64-8c38-58f86f6946ad",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ec3ff-fd0e-466b-af3a-a5b549227de5",
   "metadata": {},
   "source": [
    "### 2.1 Enable GPU Runtime\n",
    "\n",
    "Go to `Runtime` -> `Change runtime type` and select `GPU` as the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ceeb1-d929-4570-98d5-d42bc7468552",
   "metadata": {},
   "source": [
    "### 2.2 Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c1656-f864-414a-99ad-7e963cb2d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c907e-d1d2-47ae-8786-dbe2ab2fcaa5",
   "metadata": {},
   "source": [
    "Add a shortcut in you drive to this [shared folder](https://drive.google.com/drive/folders/1hbbbsILNBsQghktuj0z_Jq_3iEZQCCbj?usp=share_link).\n",
    "\n",
    "This will allow you to access the data we will use in this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aba172-13cd-4962-8465-40ce5d134346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Extract data into machine\n",
    "!unzip /content/drive/MyDrive/week4_data.zip -d /content/week4_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865862a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/content/week4_data/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b1804-eda7-4ea2-9ba7-72e15404950d",
   "metadata": {},
   "source": [
    "### 2.3 Install and Import Dependencies\n",
    "\n",
    "Run the following cell to install the required dependencies. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cafa5-4655-48f2-b7ef-621fc148425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get install libfftw3-dev libicu-dev libsndfile1-dev libqt5core5a\n",
    "%pip install git+https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment.git batdetect2 umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3a40ea-60a5-4cbb-9f4d-a586c55dd98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import ipywidgets\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "from IPython.display import Audio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "from bios0032utils.bioacoustics import evaluate_detection, plotting\n",
    "from bios0032utils.bioacoustics.classification import (\n",
    "    load_bat_call_audio_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c27070",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0aaa9-819e-4ed3-a6f7-092b445427e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3: Detecting Animal Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33734ab6-584a-4827-bf6c-7976b3168c80",
   "metadata": {},
   "source": [
    "- Most of the time, we are not interested in all the sounds in a recording, but only in the sounds\n",
    "  of a specific animal species.\n",
    "\n",
    "- Acoustic sensors will indiscriminately record all sounds in the environment, including those of\n",
    "  animals, wind, rain, _etc._ Although some recorders can be triggered by a specific sound, this is\n",
    "  not always the case.\n",
    "\n",
    "- Passive acoustic monitoring produces many hours of recordings, and it's hard to identify and\n",
    "  explore the sounds of interest.\n",
    "\n",
    "These are similar problems to the ones we have seen in the previous practicals for camera trap\n",
    "images.\n",
    "\n",
    "While not as developed as in **computer vision**, there are some tools for automatically **detecting\n",
    "animal sounds** in recordings. Here we will explore a few of them.\n",
    "\n",
    "But first we need to understand how to visualise and annotate sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6d8ce-4bbc-4e71-b29c-a6b7100d5544",
   "metadata": {},
   "source": [
    "### 3.1 Animal Sounds Visualisation\n",
    "\n",
    "- While we can listen to the sounds in a recording, it is often easier to visualise them.\n",
    "\n",
    "- This is especially true when we want to compare sounds from different recordings or navigate\n",
    "quickly without the need to listen.\n",
    "\n",
    "- We can use waveplots and spectrograms to visualise the sounds in a recording.\n",
    "\n",
    "Now we will load a dataset of animal recordings provided by\n",
    "[Avisoft](https://www.avisoft.com/animal-sounds/) and visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6f7a88-6c4d-4a4e-82c8-94d14703cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVISOFT_AUDIO_DIR = os.path.join(DATA_DIR, \"avisoft\", \"audio\")\n",
    "AVISOFT_METADATA_FILE = os.path.join(DATA_DIR, \"avisoft\", \"avisoft_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7eac016-30de-49b7-a1bc-3faa987b7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata dataframe\n",
    "avisoft = pd.read_csv(AVISOFT_METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc08b3-6394-493f-9f67-28740719d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few rows\n",
    "avisoft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5db25f-fa7d-4049-a37c-fa34692737b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random file from the dataset\n",
    "random_recording = avisoft.sample(n=1).iloc[0]\n",
    "\n",
    "# read the audio file and import it as a numpy array\n",
    "wav, samplerate = librosa.load(\n",
    "    os.path.join(AVISOFT_AUDIO_DIR, random_recording.wav),\n",
    "    sr=None\n",
    ")\n",
    "\n",
    "# Compute the duration of audio\n",
    "num_samples = len(wav)  # Number of samples taken by the recorder\n",
    "duration = num_samples / samplerate\n",
    "\n",
    "# Get name of animal\n",
    "animal_name = random_recording.english_name\n",
    "\n",
    "print(f\"File selected = {random_recording.wav}\")\n",
    "print(f\"Samplerate = {samplerate} Hz\")\n",
    "print(f\"Duration = {duration:.2f} s\")\n",
    "print(f\"Species = {animal_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a67a0-10bd-4400-bef5-5a260bf978c3",
   "metadata": {},
   "source": [
    "Let us first listen to the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf5b52-f1e8-4085-81e4-5a547752a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=wav, rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633c8a3-6948-4497-a4bf-a4c319bd4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of the waveform\n",
    "times = np.linspace(0, duration, num_samples)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(times, wav)\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.title(f\"Waveform of {animal_name} sound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eead3-d1d3-41ee-8bc3-970ed31fe704",
   "metadata": {},
   "source": [
    "The **waveform** gives us a visual representation of the sound amplitude over time.\n",
    "\n",
    "However, if there are multiple simultaneous sounds in the recording, it can be hard to see each\n",
    "individual sound. \n",
    "\n",
    "We can use a **spectrogram** to decompose the sound into **frequencies** and visualise them as a 2D\n",
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650df605-e01b-48aa-a436-729aaad2f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram with the short time fourier transform (STFT)\n",
    "spectrogram = np.abs(librosa.stft(wav))\n",
    "\n",
    "# Amplitude is best represented in logarithmic scale (decibels)\n",
    "db_spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def0c4b-b15d-4c7a-bf8e-61eb226b91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of spectrogram\n",
    "num_freq_bins, num_time_bins = db_spectrogram.shape\n",
    "times = np.linspace(0, duration, num_time_bins)\n",
    "freqs = np.linspace(0, samplerate / 2, num_freq_bins)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(times, freqs, db_spectrogram, cmap=\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(f\"Spectrogram of {random_recording.english_name} sound\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"freq (Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbde3f-289e-4097-aa94-9abd336bb6e1",
   "metadata": {},
   "source": [
    "### 3.2 Exercise üêò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72092175-0c8c-440d-82b5-a6d5c75bcb86",
   "metadata": {},
   "source": [
    "The sounds produced by animals can be very different from each other. The transformation used to\n",
    "create the spectrogram, called the **short-time Fourier transform** (STFT), will highlight different\n",
    "features of the sound depending on the parameters used.\n",
    "\n",
    "üñåÔ∏è Research what the STFT is and how its parameters affect the spectrogram. In particular, try to\n",
    "  understand the effect of the **window size** and the **hop size** or **overlap**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e48e0-3925-4e04-afe4-8b7c8166de69",
   "metadata": {},
   "source": [
    "_Solution:_\n",
    "\n",
    "The **Fourier transform** is a method for breaking a time series into its continuent frequencies.\n",
    "[Click here](https://www.youtube.com/watch?v=spUNpyF58BY) if you are interested in an intuitive and\n",
    "visual explanation of the Fourier transform.\n",
    "\n",
    "For the **short-time Fourier transform** the audio is broken up into  **windows** (or **chunks** or\n",
    "**frames**), which usually overlap each other. Each **window** is Fourier transformed, and the\n",
    "result is added to a matrix, which records magnitude for each point in time and frequency.\n",
    "\n",
    "![short time fourier\n",
    "transform](https://www.mdpi.com/applsci/applsci-10-07208/article_deploy/html/images/applsci-10-07208-g001-550.jpg)\n",
    "\n",
    "In order to compute a **STFT** of a signal you must select a `window_length` (or `n_fft`) and a\n",
    "`hop_length` (or `overlap`) to determine how to break up the signal into **windows**.\n",
    "\n",
    "* The `hop_size` controls the temporal resolution, or the minimum interval at which you can detect\n",
    "  changes in sound. If `hop_length = 128` then any transient sounds of length less than 128 samples\n",
    "  will be hard to detect. \n",
    "\n",
    "* The `window_length` controls the frequecy resolution. With larger `window_length` it is possible\n",
    "  to distinguish between closer frequencies.\n",
    "\n",
    "* Selecting a high/low value for `window_length` will produce spectrograms with many/few frequency\n",
    "  bins.\n",
    "\n",
    "* Similarly, a high/low value for `hop_length` will produce spectrograms with many/freq time bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dad7c-f01f-4c9c-aad4-4021450b9d93",
   "metadata": {},
   "source": [
    "Here you can visualise sounds from different species and see how the STFT parameters affect the\n",
    "spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ace48-5613-4dd0-a28d-94f2d5e258d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Interactive spectrogram of animal sounds\n",
    "\n",
    "# @markdown Select the file you wish to visualize. Modify the spectrogram parameters to see its effect on the spectrogram. Change the reproduction speed for interesting effects!\n",
    "\n",
    "# Select some varied sounds from avisoft dataset\n",
    "examples = [\n",
    "    (row.english_name, os.path.join(AVISOFT_AUDIO_DIR, row.wav))\n",
    "    # select one random recording per taxonomic group\n",
    "    for row in avisoft.groupby(\"order\").sample(n=1).itertuples()\n",
    "]\n",
    "\n",
    "# Create interactive plot\n",
    "ipywidgets.interact(\n",
    "    plotting.plot_waveform_with_spectrogram,\n",
    "    hop_length=(32, 1024, 32),\n",
    "    n_fft=(32, 2048, 32),\n",
    "    window=plotting.WINDOW_OPTIONS,\n",
    "    file=examples,\n",
    "    cmap=plotting.COLORMAPS,\n",
    "    speed=[\n",
    "        (\"x1\", 1),\n",
    "        (\"x1.5\", 1.5),\n",
    "        (\"x2\", 2),\n",
    "        (\"x0.5\", 0.5),\n",
    "        (\"x0.2\", 0.2),\n",
    "        (\"x0.1\", 0.1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b087ff-f0c2-4fcb-bbb4-250f778aa86b",
   "metadata": {},
   "source": [
    "Try changing the parameters and see how they affect sounds from different species.\n",
    "\n",
    "üñåÔ∏è Can you see that some choice of parameters are good for some species but not for others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9a748-1168-4044-9745-078921081f65",
   "metadata": {},
   "source": [
    "_Solution:_\n",
    "\n",
    "Some species have slowly changing frequency, like a Sheep, hence selecting a high `hop_length` would\n",
    "be able to capture its vocalization accurately without few temporal samples.\n",
    "\n",
    "Other species, such as the Little Grebe, have quickly varying frequencies, and a high `hop_length`\n",
    "would blur the intricacies of its song.\n",
    "\n",
    "A small `window_length` can be chosen in case identification does not rely on accurate frequency\n",
    "information. For example the Hoopoe call consists of a burst of three rapid pulses at low\n",
    "frequencies. This pattern can be cleary distinguished even with low frequency resolution.\n",
    "\n",
    "Ofter species will call at similar frequency bands. In such case it's best to select a\n",
    "`window_length` that will produce enough frequency resolution to distinguish between similar calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7e57d-12a3-41c3-b35a-eeddf933e990",
   "metadata": {},
   "source": [
    "üñåÔ∏è How do the parameters affect the computation time and resulting image size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4650b-0df6-4a37-b03f-42ee46119ad9",
   "metadata": {},
   "source": [
    "_Solution:_\n",
    "\n",
    "* Larger `window_length` will produce taller spectrograms and slow down computation time.\n",
    "* Smaller `hop_length` will produce lengthier spectrograms and slow down computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6eb41-65a3-4d1d-a871-c9dd50328363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a single file\n",
    "species, filepath = examples[0]\n",
    "\n",
    "print(f\"Will generate multiple spectrograms of {species} sounds. Using the file: {filepath}\")\n",
    "\n",
    "# load the audio\n",
    "wav, sr = librosa.load(filepath, sr=None)\n",
    "\n",
    "# select multiple choices of window_length and hop_length\n",
    "window_lengths = np.arange(64, 2048, 64)\n",
    "hop_lengths = np.arange(32, 1024, 32)\n",
    "\n",
    "# create list in which to store the resulting computation times\n",
    "computation_times = []\n",
    "\n",
    "# iterate over all window_length and hop_length options\n",
    "for window_length in window_lengths:\n",
    "    for hop_length in hop_lengths:\n",
    "        # start counter\n",
    "        computation_time = perf_counter()\n",
    "\n",
    "        # compute spectrogram\n",
    "        spectrogram = librosa.amplitude_to_db(\n",
    "            np.abs(\n",
    "                librosa.stft(\n",
    "                    wav,\n",
    "                    hop_length=hop_length,\n",
    "                    n_fft=window_length,\n",
    "                    window=\"hann\"\n",
    "                )\n",
    "            ),\n",
    "            ref=np.max\n",
    "        )\n",
    "\n",
    "        # end counter\n",
    "        computation_time = perf_counter() - computation_time\n",
    "\n",
    "        # store result in computation_times list\n",
    "        computation_times.append(\n",
    "            {\n",
    "                \"window_length\": window_length,\n",
    "                \"hop_length\": hop_length,\n",
    "                \"computation_time\": computation_time,\n",
    "                \"spectrogram_size\": spectrogram.size\n",
    "            }\n",
    "        )\n",
    "\n",
    "# convert computation_times list into a pandas dataframe\n",
    "computation_times = pd.DataFrame(computation_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace2f2-9447-4f67-af0a-39fef526250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.scatterplot(\n",
    "    data=computation_times,\n",
    "    x=\"computation_time\",\n",
    "    y=\"spectrogram_size\",\n",
    "    size=\"window_length\",\n",
    "    hue=\"hop_length\",\n",
    "    sizes=(20, 200)\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051ea3-bbed-4616-b39c-e5c6052c4863",
   "metadata": {},
   "source": [
    "### 3.3 Detecting Sounds\n",
    "\n",
    "As you can imagine, it is not easy to manually annotate all relevant sounds in a recording.\n",
    "\n",
    "Take a look at this recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577bf7b1-f4a7-4545-b0f3-e4455a204184",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_METADATA = os.path.join(DATA_DIR, \"yucatan\", \"yucatan_metadata.csv\")\n",
    "\n",
    "YUCATAN_AUDIO_DIR = os.path.join(DATA_DIR, \"yucatan\", \"audio\")\n",
    "\n",
    "# Load metadata of dataset of bat recordings from the Yucatan peninsula\n",
    "yucatan = pd.read_csv(YUCATAN_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e66c5-401a-4dbe-b544-5fa6b7a8c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowded_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[1017])\n",
    "\n",
    "plotting.plot_spectrogram(\n",
    "    crowded_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2511b0-03fc-4fab-a720-276156e602e9",
   "metadata": {},
   "source": [
    "There are many bat calls in this recording, and it would be very time-consuming to annotate them\n",
    "all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1de031-0c92-47f9-a4f5-dfb66dd01f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[205])\n",
    "plotting.plot_spectrogram(\n",
    "    empty_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dacb91-cf8a-48f4-bbf9-96b89c318638",
   "metadata": {},
   "source": [
    "This other recording has a single bat pulse. You still need to review it thoroughly to make sure\n",
    "there are no other sounds of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04221ff",
   "metadata": {},
   "source": [
    "### 3.4 BatDetect2\n",
    "\n",
    "Similar to **MegaDetector** for camera traps, there are some tools that automatically detect animal\n",
    "sounds in recordings.\n",
    "\n",
    "Various models have been developed and trained for different taxon groups. The following are very\n",
    "popular examples, all using deep learning:\n",
    "\n",
    "#### Birds\n",
    "\n",
    "**BirdNET:** A convolutional neural network from Cornell University for classifying over 5,000 bird\n",
    "calls in audio recordings.\n",
    "> Kahl, S., Wood, C.M., Eibl, M. and Klinck, H., 2021. BirdNET: A deep learning solution for avian\n",
    "> diversity monitoring. Ecological Informatics, 61, p.101236.\n",
    "> [https://www.sciencedirect.com/science/article/pii/S1574954121000273](https://www.sciencedirect.com/science/article/pii/S1574954121000273)\n",
    "\n",
    "[GitHub](https://github.com/kahst/BirdNET-Analyzer)\n",
    "\n",
    "**Perch:** Similar idea; model trained on over 10,000 species.\n",
    "[GitHub](https://github.com/google-research/perch)\n",
    "\n",
    "\n",
    "####¬†Bats\n",
    "\n",
    "**BatDetect2:** A framework in PyTorch for simultaneous detection and classification of UK bats.\n",
    "> Aodha, O.M., Mart√≠nez Balvanera, S., Damstra, E., Cooke, M., Eichinski, P., Browning, E.,\n",
    "> Barataud, M., Boughey, K., Coles, R., Giacomini, G. and Swiney G, M.C.M., 2022. Towards a general\n",
    "> approach for bat echolocation detection and classification. bioRxiv, pp.2022-12.\n",
    "> [https://www.biorxiv.org/content/10.1101/2022.12.14.520490v1.abstract](https://www.biorxiv.org/content/10.1101/2022.12.14.520490v1.abstract)\n",
    "\n",
    "[GitHub](https://github.com/macaodha/batdetect2)\n",
    "\n",
    "\n",
    "Below, we will use BatDetect2 on our data. Although the model was trained on UK bat calls, we can\n",
    "still test its detection performance on the dataset of bats from the Yucat√°n peninsula.\n",
    "\n",
    "üí° Make sure to use a GPU runtime for this. Even so, completion may take a while as we have quite a\n",
    "number of audio recordings to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "batdetect2 detect /content/week4_data/data/yucatan/audio /content/week4_data/data/yucatan/predictions 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc97dd",
   "metadata": {},
   "source": [
    "üí° BatDetect2 internally computes spectrograms to detect and classify bat calls, just like we\n",
    "manually did above ([source\n",
    "code](https://github.com/macaodha/batdetect2/blob/2100a3e483116037c57698c72bbe506c604ebd0b/batdetect2/train/audio_dataloader.py#L521))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e42d04",
   "metadata": {},
   "source": [
    "**BatDetect2** can predict multiple bounding boxes for each recording. Each bounding box has a\n",
    "**score**, a predicted species and a confidence score for the species.\n",
    "\n",
    "Here, we will throw out the predicted species and confidence score, and only use the bounding box\n",
    "score. The **score** is the probability that the bounding box contains a bat call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all prediction files made by BatDetect2 (CSV format)\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"yucatan\", \"predictions\", \"*.csv\"))\n",
    "\n",
    "# Read each prediction file\n",
    "batdetect2_predictions = []\n",
    "for path in files:\n",
    "    df = pd.read_csv(path).drop(columns=[\"id\", \"class\", \"class_prob\"])\n",
    "    df[\"recording_id\"] = os.path.basename(path)[:-4]\n",
    "    batdetect2_predictions.append(df)\n",
    "\n",
    "# And concatenate them into a single dataframe\n",
    "batdetect2_predictions = pd.concat(batdetect2_predictions)\n",
    "\n",
    "# show the first few rows of predictions made by BatDetect2\n",
    "batdetect2_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bbd6c",
   "metadata": {},
   "source": [
    "Are these good predictions? To find out, we need to compare them to ground truth.\n",
    "\n",
    "Let us load some previously made ground truth annotations first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dcaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "yucatan_annotations = pd.read_csv(os.path.join(DATA_DIR, \"yucatan\", \"yucatan_annotations.csv\"))\n",
    "\n",
    "#¬†show the first few rows\n",
    "yucatan_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f9304-ba3d-4628-bb11-7d19d9be82b9",
   "metadata": {},
   "source": [
    "Now, we can compare the detections with the ground truth. We can use the Intersection-over-Union\n",
    "(IoU) to measure the overlap between the detections and the ground truth.\n",
    "\n",
    "<img alt=\"intersection over union\"\n",
    "src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/Intersection_over_Union_-_visual_equation.png\"\n",
    "width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a33941",
   "metadata": {},
   "source": [
    "Intuitively, the relationship between overlap (intersection) and union tells us how geometrically\n",
    "similar two bounding boxes are. An IoU of 1 means that both boxes are congruent (perfectly\n",
    "overlapping), for IoU 0 the two boxes don't even touch each other.\n",
    "\n",
    "We usually cannot expect a machine learning model to always produce perfectly identical bounding\n",
    "boxes as provided in the ground truth. Hence, we need to set a minimum IoU value for which\n",
    "we count a prediction as \"correct\". We will use such an _IoU threshold_ of 0.5 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a14778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the predictions and annotations from the crowded recording\n",
    "file_detections = batdetect2_predictions[\n",
    "    batdetect2_predictions.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "file_annotations = yucatan_annotations[\n",
    "    yucatan_annotations.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "\n",
    "# Match the bounding boxes by computing the IoU. Discard all matches with IoU less than 0.5\n",
    "pred_boxes = evaluate_detection.bboxes_from_annotations(file_detections)\n",
    "true_boxes = evaluate_detection.bboxes_from_annotations(file_annotations)\n",
    "matches = evaluate_detection.match_bboxes(true_boxes, pred_boxes, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbab295",
   "metadata": {},
   "source": [
    "We can now distinguish three different cases (remember lecture from Session 2):\n",
    "* **True Positives (TP)**: _detections_ with IoU >= threshold\n",
    "* **False Positives (FP)**: _detections_ with IoU < threshold\n",
    "* **False Negatives (FN)**: _annotations_ with no matching detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52beeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of annotated sound events\n",
    "positives = len(file_annotations)\n",
    "\n",
    "num_predictions = len(file_detections)\n",
    "\n",
    "# number of matched prediction boxes\n",
    "true_positives = len(matches)\n",
    "\n",
    "# number of predicted boxes that were not matched\n",
    "false_positives = num_predictions - len(matches)\n",
    "\n",
    "# number of annotated sound events that were not matched\n",
    "false_negatives = positives - len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d24a7f",
   "metadata": {},
   "source": [
    "With this information, we can compute the precision and recall of the detections:\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6888f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of predictions that are correct\n",
    "precision = true_positives / num_predictions\n",
    "\n",
    "# Percentage of sound events that were detected\n",
    "recall = true_positives / positives\n",
    "\n",
    "print(f\"BatDetect2 precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b84d2",
   "metadata": {},
   "source": [
    "Now, we can visualise predictions and ground truth annotations in the spectrograms together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb534e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.3\n",
    "\n",
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    batdetect2_predictions[batdetect2_predictions.det_prob > score_threshold],\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f843877",
   "metadata": {},
   "source": [
    "In the two visualisations above, bounding boxes are drawn in different styles:\n",
    "- red = spurious predicted sound event (false positive)\n",
    "- green = correct prediction (true positive)\n",
    "- white = missed sound event (false negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56538f7d",
   "metadata": {},
   "source": [
    "Besides an IoU threshold, you will notice that we also need to set a _confidence threshold_, because\n",
    "as explained above, BatDetect2 gives us probabilities (softmax-activated output logits; _cf._\n",
    "Session 3).\n",
    "\n",
    "We have set this rather arbitrarily to 0.3 above. You can imagine what happens if we increase or\n",
    "decrease this threshold. Watch what happens for different files when you adjust IoU and confidence\n",
    "thresholds in the widget below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33945242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Batdetect2 predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    path=example_files,\n",
    "    iou_threshold=(0, 1, 0.1),\n",
    "    score_threshold=(0, 1, 0.1),\n",
    ")\n",
    "def plot_batdetect2_results_file_results(\n",
    "    path=crowded_recording,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.3,\n",
    "):\n",
    "    confident_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.det_prob > score_threshold\n",
    "    ]\n",
    "\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Batdetect2 precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "        linewidth=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b581d-2ea9-4116-b83d-3c2becd2d077",
   "metadata": {},
   "source": [
    "### 3.6 Exercise üêã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902f1d1-58fe-4e9d-a333-bb26fa898210",
   "metadata": {},
   "source": [
    "üñåÔ∏è With the code blocks above as help, implement the calculation of the following properties below:\n",
    "\n",
    "* The mean precision and recall across all files.\n",
    "* The percentage of files where all bat calls were missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82473263-9fe4-4743-8fdf-f8f5356fb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls = [], []        # lists of per-file precision and recall scores\n",
    "files_missed = 0                    # counter for number of files where all calls were missed\n",
    "\n",
    "\n",
    "for recording_id in batdetect2_predictions.recording_id.unique():\n",
    "    file_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.recording_id == recording_id\n",
    "    ]\n",
    "    file_annotations = yucatan_annotations[\n",
    "        yucatan_annotations.recording_id == recording_id\n",
    "    ]\n",
    "\n",
    "    # Match the bounding boxes by computing the IoU. Discard all matches with IoU less than 0.5\n",
    "    pred_boxes = evaluate_detection.bboxes_from_annotations(file_detections)\n",
    "    true_boxes = evaluate_detection.bboxes_from_annotations(file_annotations)\n",
    "    matches = evaluate_detection.match_bboxes(true_boxes, pred_boxes, iou_threshold=0.5)\n",
    "\n",
    "    # total number of annotated sound events\n",
    "    positives = len(file_annotations)\n",
    "\n",
    "    num_predictions = len(file_detections)\n",
    "\n",
    "    # number of matched prediction boxes\n",
    "    true_positives = len(matches)\n",
    "\n",
    "    # number of predicted boxes that were not matched\n",
    "    false_positives = num_predictions - len(matches)\n",
    "\n",
    "    # number of annotated sound events that were not matched\n",
    "    false_negatives = positives - len(matches)\n",
    "\n",
    "    # Percentage of predictions that are correct\n",
    "    if num_predictions == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_positives / num_predictions\n",
    "\n",
    "    # Percentage of sound events that were detected\n",
    "    if positives == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_positives / positives\n",
    "\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # increment counter if all bat calls were missed\n",
    "    if true_positives == 0 and positives > 0:\n",
    "        files_missed += 1\n",
    "\n",
    "print(f\"Mean precision = {np.mean(precisions):.2%}, Mean recall = {np.mean(recalls):.2%}.\")\n",
    "print(f\"Percent of files where all bat calls were missed = {files_missed/len(precisions):.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe97e9-24aa-44a0-8e5f-9db037f81d9a",
   "metadata": {},
   "source": [
    "üñåÔ∏è Run the full evaluation again but change the `iou_threshold` parameter. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65950947-aad5-48bf-8e70-9b1bea4a02ac",
   "metadata": {},
   "source": [
    "_Solution:_\n",
    "\n",
    "* Increasing the **IoU threshold** makes it harder to find matches (`true_positives`). Recall will\n",
    "  necessarily be lowered, as less bat calls can be detected. Precision will also be lowered as less\n",
    "  detections can be correct.\n",
    "\n",
    "* Conversely, a lower **IoU threshold** increases the number of matches (`true_positives`), making\n",
    "  both recall and precision greater.\n",
    "\n",
    "* Selecting an **IoU threshold** is not about optimizing performance, but about choosing a matching\n",
    "  criterion. Having a low **IoU** threshold might increase the recall and precision but incurr a\n",
    "  cost in precision of the bounding box of each detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d986c2b",
   "metadata": {},
   "source": [
    "We can visualise BatDetect2's predictions versus ground truth labels for a selection of audio files\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3b2f8-b55d-4e71-ac11-9367dd711ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Batdetect2 predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    path=example_files,\n",
    "    iou_threshold=(0, 1, 0.1),\n",
    "    score_threshold=(0, 1, 0.1),\n",
    ")\n",
    "def plot_batdetect2_results_file_results(\n",
    "    path=crowded_recording,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.3,\n",
    "):\n",
    "    confident_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.det_prob > score_threshold\n",
    "    ]\n",
    "\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Batdetect2 precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "        linewidth=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b395a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99346cc8-4bf3-4c24-a409-580b1deb4c80",
   "metadata": {},
   "source": [
    "## 4. Identifying Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f6e1-6464-47a9-9814-4883ff6ae9a5",
   "metadata": {},
   "source": [
    "In the previous section, we saw how to detect sounds in a recording. But we still need to identify\n",
    "the species that produced the sound.\n",
    "\n",
    "Generally, classification is more challenging than detection, as the sounds produced by different\n",
    "species can be very similar (**interspecific overlap**). Also, a single species can have flexible\n",
    "vocalisations, think humans or mimic birds such as starling (**intraspecific variation**).\n",
    "\n",
    "Bioacoustic data presents similar challenges to the camera trap datasets as recordings can be:\n",
    "* **Ocluded** (Simultaneous sounds)\n",
    "* **Appear in varying ambient conditions** (rain/wind/thunder)\n",
    "* **Partial** (Only captured half the sound)\n",
    "* **Noisy** (Saturation and faulty sensor)\n",
    "* **Quiet or very loud** (depending on animal size, distance, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264ce12-0a2a-418d-90b5-36bbd9d0f623",
   "metadata": {},
   "source": [
    "For the rest of this notebook we will focus on **10** bat species present in the Yucat√°n dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f19b242-1165-426b-aac0-d20ca9bab747",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES = [\n",
    "    \"Mormoops megalophylla\",\n",
    "    \"Myotis keaysi\",\n",
    "    \"Saccopteryx bilineata\",\n",
    "    \"Pteronotus davyi\",\n",
    "    \"Pteronotus parnellii\",\n",
    "    \"Lasiurus ega\",\n",
    "    \"Pteropteryx macrotis\",\n",
    "    \"Eumops underwoodi\",\n",
    "    \"Rhogeessa aeneus\",\n",
    "    \"Eptesicus furinalis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614bd285-7130-41f0-8c1e-12ae0f1892ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = yucatan_annotations[yucatan_annotations[\"class\"].isin(SPECIES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f472b-f647-414e-a44b-c88fda2feff0",
   "metadata": {},
   "source": [
    "### 4.1 Bat Call Features\n",
    "\n",
    "Previous research on bat call identification was based on hand-crafted features of the bat calls.\n",
    "Measuring call features used to be a manual process.\n",
    "\n",
    "<img src=\"https://www.elekon.ch/batexplorer2/doc/_images/CallParams.png\" alt=\"call parameters\" width=\"400\"/>\n",
    "\n",
    "> Image taken from the [BatExplorer 2.1 user guide](https://www.elekon.ch/batexplorer2/doc/batcall_params.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4c318-e095-4eb5-866a-10cefd13b476",
   "metadata": {},
   "source": [
    "**Peak frequency [kHz]:**\n",
    "\n",
    ">    The frequency at which the call is loudest (peak in the spectrum display), aka frequency of maximum energy (FME) or main frequency.\n",
    ">    Most important parameter for bat classification because it can easily be measured and is often typical for a certain species or group of species.\n",
    ">    The standard deviation of the peak frequency allows the detection of alternating calling species.\n",
    "    \n",
    "**Max frequency [kHz]**\n",
    "\n",
    ">    The maximum frequency of the call. Often this is equal to the start frequency, for Rhinolophidae typically equal to the peak frequency.\n",
    "    \n",
    "**Min frequency [kHz]**\n",
    "\n",
    ">    The minimum frequency of the call. Often this is equal to the end frequency, for hockey stick calls (e.g. Pipistrelle) it might be lower than the end frequency.\n",
    "    \n",
    "**BW Peak2Min [kHz]**\n",
    "\n",
    ">    Bandwidth Peak2Min = Peak frequency - Min frequency\n",
    ">    Often used to distinguish Myotis and Pipistrelle calls, Myotis mostly have higher bandwidth.\n",
    "    \n",
    "**Call length [ms]**\n",
    "\n",
    ">    Time period of call start to call end in ms. Can be measured most accurately in the oscillogram (wave rise to wave drop).\n",
    ">    Search calls from European bats are usually between one and up to approximately 30 ms (horseshoe bats up to 80 ms).\n",
    "    \n",
    "**Call distance [ms]**\n",
    "\n",
    ">    Time period between two consecutive calls in ms. Can be measured most accurately in the oscillogram (wave rise call A to wave rise call B).\n",
    ">    Often this parameter is not very significant since most bat species have irregular rhythms. But it can be an indicator for behavior.\n",
    ">    Search calls from European bats usually have distances of about 30 to 300 ms, sometimes even longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0215c-b423-4fa8-93f2-969624bfdb03",
   "metadata": {},
   "source": [
    "### 4.3 Classifying Sounds with a Universal Feature Set\n",
    "\n",
    "In Section 3, we used BatDetect2 to localise bat calls within spectrograms. However, we could not\n",
    "use its predicted species, since those are from the UK and do not occur in Yucat√°n (and BatDetect2\n",
    "does not know about the latter's species either).\n",
    "\n",
    "If we want to do both, _i.e._, detect _and_ classify bat calls, we now have two options:\n",
    "1. Transfer learning: we could fine-tune BatDetect2 on the Yucat√°n data by replacing its final,\n",
    "   fully-connected layer with a new one that outputs logits for the Yucat√°n species. We saw this\n",
    "   last week with ResNet and images. However, since deep learning requires lots of labelled data\n",
    "   points, this may not be realistic if our dataset is too small.\n",
    "2. A simpler alternative is to use a more lightweight machine learning model on a different set of\n",
    "   input features.\n",
    "\n",
    "\n",
    "This second approach may give you a direct connection to what we saw just above: the \"features\" we\n",
    "talked about are based on domain-specific knowledge about bats. In a sense, we could measure them\n",
    "for each bat call and use them as an input to a machine learning classifier. However, creating these\n",
    "_hand-crafted_ features is very tedious and time-consuming. If you think back to what we learnt\n",
    "about deep learning in the last two sessions, we could perhaps use _e.g._ a convolutional neural\n",
    "network or another type of (trained) deep learning model to predict features for us instead.\n",
    "\n",
    "\n",
    "Ultimately, if our deep learning model can summarise the spectrogram contents \"well enough\", we can\n",
    "train a more lightweight model like a Random Forest to predict our target species more easily.\n",
    "\n",
    "Below, we will try this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b535bf6-9d97-4553-bcb7-6fee66366974",
   "metadata": {},
   "source": [
    "To do so, we will use a general-purpose acoustic feature extractor called\n",
    "[Yamnet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet).\n",
    "\n",
    "Yamnet was trained on [AudioSet](http://research.google.com/audioset/) a massive dataset of YouTube\n",
    "recordings with more than 5.8 thousand hours of audio.\n",
    "\n",
    "It was trained to classify sounds clips into 527 different classes. The features it learned to\n",
    "extract are thus useful to distinguish and identify a large variety of sounds.\n",
    "\n",
    "<img src=\"http://research.google.com/audioset/resources/histogram.svg\" alt=\"audioset dataset\"\n",
    "width=\"400\"/>\n",
    "\n",
    "Audioset does not contain ultrasonic recordings, and thus is devoid of bat sounds. However, we\n",
    "hope that the learnt features are sufficiently general that it can help identify bat calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68854c3-ef99-4719-9bb4-0774f36226b4",
   "metadata": {},
   "source": [
    "First, we need to download the model. The model is available in [TensorFlow\n",
    "Hub](https://tfhub.dev/), a repository of pre-trained models (implemented in TensorFlow instead of\n",
    "_e.g._ PyTorch, as the name suggests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f12eb3bd-88f6-41b2-8df2-1f0d204ef26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model using tensorflow_hub\n",
    "yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f54f0-d0d4-48d8-abf6-b6d16dddd5be",
   "metadata": {},
   "source": [
    "Next, we will load all the audio bits from the dataset and extract the features using the model.\n",
    "\n",
    "YamNet was originally trained with 0.96 second audio clips sampled at 16kHz. However, all our\n",
    "recordings have a sample rate of 44.1kHz. Hence, we will only feed the model with 34ms of audio at a\n",
    "time. \n",
    "\n",
    "We will center each audio clip on the bat calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81531304-22c4-4d01-b69b-ebe2b5e386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will load the information of where each bat call starts and ends\n",
    "audio_clips = pd.read_csv(os.path.join(DATA_DIR, \"yucatan\", \"yucatan_species_clips.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8f2797d-b5a7-4ef9-a533-c816db9e30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for _, row in audio_clips.iterrows():\n",
    "    wav = load_bat_call_audio_data(\n",
    "        os.path.join(YUCATAN_AUDIO_DIR, str(row.recording_id)),\n",
    "        row.start_time,\n",
    "        row.end_time,\n",
    "    )\n",
    "    audios.append(wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c88f9",
   "metadata": {},
   "source": [
    "Let's calculate feature vectors with YamNet for each of our bat calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871a436-a5cd-442a-a8c4-1284e00b27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_features = []\n",
    "for wav in tqdm(audios):\n",
    "    scores, feats, spectrogram = yamnet(wav)\n",
    "    yamnet_features.append(feats.numpy().squeeze())\n",
    "yamnet_features = np.array(yamnet_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe5993",
   "metadata": {},
   "source": [
    "üí° **Important:**\n",
    "\n",
    "If the above cell gives you an error like \"Graph Execution Failed\", your Colab runtime might be out\n",
    "of memory. To fix this:\n",
    "1. Select \"Runtime\" > \"Restart session\".\n",
    "2. Re-run all code cells under Section 2.\n",
    "3. Return to Section 4.3 and continue from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f028ad-35a5-4fae-93e4-95fccf6b7a10",
   "metadata": {},
   "source": [
    "Note that YamNet returns a set of 1,024 features for each audio part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da7085-44fb-4121-a93f-19c6101ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yamnet_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8300d-2d82-4c64-b61d-000d263761dc",
   "metadata": {},
   "source": [
    "We don't know yet whether those features are actually useful for separating our Yucat√°n bat species.\n",
    "Thus, we can first visualise the features using one of the dimensionality reduction techniques\n",
    "explored in Session 2, such as [UMAP](https://umap-learn.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320ce59-1422-4bdf-bc72-374661bb81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_yamnet = UMAP().fit_transform(StandardScaler().fit_transform(yamnet_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09809281-e4f1-40ee-b145-cc6c1a1b1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = umap_yamnet.T\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X, y=Y, hue=audio_clips[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d98d8c-167d-4fbc-b805-fcf0358d0bd1",
   "metadata": {},
   "source": [
    "It seems difficult to disentangle bat species based on the feature sets and this UMAP plot. Well,\n",
    "let us nonetheless try and use those features as input for a classifier to really see how well it\n",
    "will work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ae5a-a6ff-4b9a-bbaa-596b2244e662",
   "metadata": {},
   "source": [
    "### 4.6 Exercise üêù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9478963-f2ec-4267-a709-34ce901755c8",
   "metadata": {},
   "source": [
    "Here you will build a Random Forest classifier using the features extracted by YamNet.\n",
    "\n",
    "üñåÔ∏è Split the data into train and test. You can copy the code from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adcd845f-4273-455e-8750-a527ea165a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "X_train_yamnet, X_test_yamnet, y_train_yamnet, y_test_yamnet = train_test_split(\n",
    "    yamnet_features,\n",
    "    audio_clips[\"class\"],\n",
    "    test_size=0.3,\n",
    "    stratify=audio_clips[\"class\"]       # notice we are using the stratified argument\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9015661-4741-4cbb-ad75-833aa854c38f",
   "metadata": {},
   "source": [
    "üñåÔ∏è Train a random forest classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772135be-f13b-4648-b30f-dcc9236b6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit to training data\n",
    "rf.fit(X_train_yamnet, y_train_yamnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3013975",
   "metadata": {},
   "source": [
    "This time, we will use cross-validation instead of a dedicated validation set. Scikit-learn has a\n",
    "handy function\n",
    "[cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "available for us.\n",
    "\n",
    "üñåÔ∏è Implement crossvalidation by completing the code cell below. Then, tweak the Random Forest's\n",
    "hyperparameters based on this result until you are happy to move on to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create new Random Forest object\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# calculate crossvalidation (CV) performance scores\n",
    "scores = cross_val_score(rf,\n",
    "                         X_train_yamnet,\n",
    "                         y_train_yamnet, \n",
    "                         cv=5)\n",
    "\n",
    "# report CV score statistics\n",
    "print(f'CV score mean: {scores.mean():.2%}, std: {scores.std():.2%}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726891e-b690-40cd-8768-03e5d2b4c58a",
   "metadata": {},
   "source": [
    "üñåÔ∏è Once you are happy enough with the performance, evaluate the performance using the\n",
    "classification_report function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0cf78-5157-4ee7-acee-c90c01910a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still need to fit the final model\n",
    "rf.fit(X_train_yamnet, y_train_yamnet)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_yamnet = rf.predict(X_test_yamnet)\n",
    "\n",
    "# evaluate using the classification_report\n",
    "print(classification_report(y_true=y_test_yamnet, y_pred=y_pred_yamnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003b3db-2765-45d1-a1fd-3776be5b2251",
   "metadata": {},
   "source": [
    "üñåÔ∏è Plot the confusion matrix. Tip: check the [scikit-learn\n",
    "documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18315b8d-ad24-440f-93d1-402d29769edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix and normalise each row to get percentages\n",
    "cm = confusion_matrix(y_true=y_test_yamnet, y_pred=y_pred_yamnet, normalize=\"true\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# plot the confusion matrix using the ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay(cm, display_labels=rf.classes_).plot(\n",
    "    xticks_rotation=\"vertical\",\n",
    "    values_format=\".0%\",\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5b12a-749e-4b23-8338-39840ad61f93",
   "metadata": {},
   "source": [
    "üñåÔ∏è Which are the worst performing species? Can you see why? You might need to go back and see some\n",
    "more examples of bat calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1e6db",
   "metadata": {},
   "source": [
    "_Solution:_\n",
    "\n",
    "üí° **Caveat**: Answers are likely to be different each run, as dataset splits and random forest\n",
    "training are random.\n",
    "\n",
    "*Mormoops megalophylla* and *Eumops underwoodi* were the worst-performing species.\n",
    "\n",
    "This could be explained by:\n",
    "* Low number of examples (see \"support\" in accuracy report table). Note that this is especially\n",
    "  problematic given YamNet produces a 1024-dimensional feature vector. With insufficient numbers of\n",
    "  examples we run into the **curse of dimensionality** issue (see Session 2).\n",
    "* Related to this is class imbalance, respectively long-tailed class distribution in general.\n",
    "  Although this affects Random Forests less than _e.g._ deep learning models, it can still cause\n",
    "  mayhem.\n",
    "* Little clustering and insufficient separation of YamNet features (see UMAP scatter plot).\n",
    "* Difficult call signature overall (see visualisations of spectrogram).\n",
    "\n",
    "\n",
    "From an ecological perspective:\n",
    "* The training examples might not cover the whole range of call variation. Test calls could contain\n",
    "  call types not seen during training. This could be fixed by collecting more data, or generating\n",
    "  synthetic data.\n",
    "* There also seems to be substantial overlap between echolocation calls of different species. This\n",
    "  could be due to errors in labelling. Errors in the annotation process are common and should be\n",
    "  expected. This could be fixed by conducting a review process in which suspicious annotations are\n",
    "  verified or corrected. An annotation can be flagged as suspicious if its features lie far from the\n",
    "  usual distribution (outlier detection), or if the model is confidently wrong about its class (hard\n",
    "  example mining).\n",
    "* Bat echolocation calls are are used to navigate and detect pray. If habitual flying space and/or\n",
    "  prey are similar for two species they will tend to echolocate in a similar fashion. When this is\n",
    "  the case using a different classification scheme for bats, such as one based on ecological traits,\n",
    "  might be better suited for the task of acoustic identification.\n",
    "* Sound production is also constrained by morphology. Closely related species will tend to have\n",
    "  similar sound production capabilities and thus similar echolocation calls. However, this is not\n",
    "  always the case as echolocation call is heavily constrained by ecology (as mentioned in the\n",
    "  previous point). A potential workaround is to classify to a higher taxonomic level such as genus\n",
    "  or family.\n",
    "* Selected call features cannot discern between species. It could be the case that there are subtle\n",
    "  cues in the audio that aid identification but haven't been measured. In this case either more\n",
    "  features should be designed and measured, or adopting an approach of automated feature extraction\n",
    "  (such as deep learning approaches) could help find a better feature set for acoustic\n",
    "  identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9e3ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb10e6d",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Outlook\n",
    "\n",
    "We saw some quite interesting phenomena today:\n",
    "* It is totally possible to _detect_ bat calls with a general-purpose bat detector, at least to some\n",
    "  degree.\n",
    "* However, _classifying_ bat species was much harder.\n",
    "\n",
    "We also explored different types of bat calls a bit ‚Äì some of them are really hard to identify and\n",
    "occur relatively rarely (a bat may use more echolocation than _e.g._ social calls), which adds to\n",
    "the challenge.\n",
    "\n",
    "Apart from collecting an enormous amount of labelled data and training a huge model, what else could\n",
    "we do to improve performance in such a case?\n",
    "\n",
    "Well, we have to go back as to why our Random Forest struggled a bit: one major reason is that\n",
    "YamNet simply \"had no idea\" about bat calls. Those are unlikely to be present in YouTube audio\n",
    "(training data). There is a mismatch between training and test data, or else, between the\n",
    "pre-training task (for YamNet) and the target task (Yucat√°n bat call classification). In machine\n",
    "learning terminology, this mismatch is known as **domain shift**, and it describes a gap (shift)\n",
    "between _source_ and _target_ datasets. Real-world problems can have many domain shifts. We have\n",
    "seen one here, but there can be others, such as:\n",
    "* Acquiring data in different regions,\n",
    "* different seasons,\n",
    "* different years,\n",
    "* with different sensors (_e.g._, microphones with different sensitivities, cameras with varying\n",
    "  focal lengths, drones with different flying heights, _etc._),\n",
    "* and so on.\n",
    "\n",
    "All of this means that the model won't really know what to do under different domains unless it is\n",
    "trained on the target data.\n",
    "\n",
    "As it turns out, we have quite a number of approaches available to close this gap/shift between\n",
    "source and target data, many of which are unsupervised (_i.e._, require no labels from the target\n",
    "dataset). If you are interested in those, please let us know and we can talk about them in the\n",
    "course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bios0032",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
