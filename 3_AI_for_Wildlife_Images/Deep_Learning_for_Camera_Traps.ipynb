{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19ed325",
   "metadata": {
    "id": "b19ed325"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/Deep_Learning_for_Camera_Traps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62da0d",
   "metadata": {
    "id": "8b62da0d"
   },
   "source": [
    "# Week 3 - AI for Wildlife Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OA3DDnq8Tjxr",
   "metadata": {
    "id": "OA3DDnq8Tjxr"
   },
   "source": [
    "# What we will learn\n",
    "\n",
    "In this weeks practical we will explore computer vision applications in ecology such as automated animal detection and species classification from camera trap data. Among other stuff we will:\n",
    "\n",
    "1. Manually annotate animals in camera trap data\n",
    "2. Apply an automated animal detector on a set of images to acquire labels and compare with manual labeling\n",
    "3. Analyse automated animal detections\n",
    "4. Train a neural network for species classification in a camera trap dataset\n",
    "5. Thorough evaluation of the trained model and analysis of its predictions\n",
    "6. Experiment with various parameters in the training process of neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rOywx8njBUVs",
   "metadata": {
    "id": "rOywx8njBUVs"
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fff834",
   "metadata": {
    "id": "67fff834"
   },
   "source": [
    "## Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3688da4",
   "metadata": {
    "id": "a3688da4"
   },
   "source": [
    "**What is Computer Vision?**\n",
    "\n",
    "Development of automated systems that can extract useful information from images or sequences of images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d4db8",
   "metadata": {
    "id": "a41d4db8"
   },
   "source": [
    "**What do computers see?** <br>\n",
    "* Images are  2-Dimensional numerical matrices representing the pixel intensity. \n",
    "* Colored images become 3-D (RGB). \n",
    "* Videos are sequences of such images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd38d8",
   "metadata": {
    "id": "02dd38d8"
   },
   "source": [
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/pixels_and_matrices.png?raw=1\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18821f5",
   "metadata": {
    "id": "e18821f5"
   },
   "source": [
    "Most of the computer vision tasks nowadays are tackled with deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9c3fd",
   "metadata": {
    "id": "40f9c3fd"
   },
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17518320",
   "metadata": {
    "id": "17518320"
   },
   "source": [
    "* Image Classification: Classifying chest X-Rays of patients as healthy or pneumonia.\n",
    "* Object Detection: Detecting car signs in traffic cameras for penalty charges.\n",
    "* Semantic Segmentation: Map pixels of land into water, grassland, roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kLHlP1vL18pX",
   "metadata": {
    "id": "kLHlP1vL18pX"
   },
   "source": [
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/cv_tasks.png?raw=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e7524",
   "metadata": {
    "id": "a13e7524"
   },
   "source": [
    "In this lab, we are going to tackle two tasks of ecological interest using Computer Vision, specifically:\n",
    "* **Object Detection:** Detecting animals in camera trap photos\n",
    "* **Image Classification:** Classifying species in camera trap data.\n",
    "\n",
    "We will try to solve the above tasks using Deep Learning, either by exploiting existing models or training our own. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S_Pp5VpAAicP",
   "metadata": {
    "id": "S_Pp5VpAAicP"
   },
   "source": [
    "**Convolutional Neural Networks (CNNs)** is a neural network type commonly used for computer vision tasks. Their basic component, the convolutional layers are acting like **filters** and training such networks essentially translates to learning the best parameters for these filters. The **key properties of CNNs** that make the successfull on visual data are **locality**, i.e. the ability to perceive jointly neighborhoods of pixels instead of individual pixels and **translational invariance** i.e. understand the existence of an object within an image independntly of where it appears within it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mseb2xCCQA0",
   "metadata": {
    "id": "9mseb2xCCQA0"
   },
   "source": [
    "A typical CNN architecture <br><img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/cnn.png?raw=1\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IG68loqVAWCS",
   "metadata": {
    "id": "IG68loqVAWCS"
   },
   "source": [
    "The underlying architectures of the models we are going to try out today are based on CNNs\n",
    "<br><br>\n",
    "\n",
    "But first let's take a look on what camera trap sensors look like and how this type of data is collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3ebf8",
   "metadata": {
    "id": "01d3ebf8"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd937e",
   "metadata": {
    "id": "3fbd937e"
   },
   "source": [
    "**Camera traps** are static cameras set up in the wild to monitor animal populations. Typically, a shot from a camera trap is triggered by motion taking place in the camera's field of view.\n",
    "\n",
    "Each camera trap site can represent variables that ecologists want to include in their analysis. For example, the location of a camera can correspond to different gradients of human pressure. Thus, observing the biodiversity across different locations and hence different gradients of anthropogenic pressure can help us quantify the impact and potentially inform future decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a53601",
   "metadata": {
    "id": "63a53601"
   },
   "source": [
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/ct.png?raw=1\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3737d9",
   "metadata": {
    "id": "9c3737d9"
   },
   "source": [
    "Below you see consecutive frames captured by a camera trap in Kenya. It shows a hyena entering the scene and checking out a buffalo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd3023",
   "metadata": {
    "id": "7bbd3023"
   },
   "source": [
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/hyena.gif?raw=1\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "(image frames from a camera trap set up in Kenya for the [Biome Health Project](https://www.biomehealthproject.com/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac99PmCrWv",
   "metadata": {
    "id": "f5ac99PmCrWv"
   },
   "source": [
    "## The utility of GPUs for Computer Vision\n",
    "* As you know by know, more data can train better algorithms\n",
    "* Graphics Processing Units (GPUs) enable parallelization in the way we process data and thus allow for inclusion of more without compromising the time needed to train a model with them \n",
    "* Especially useful for image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g8kh5cDUSqkZ",
   "metadata": {
    "id": "g8kh5cDUSqkZ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CFKQTkKBEZy2",
   "metadata": {
    "id": "CFKQTkKBEZy2"
   },
   "source": [
    "# Setup Steps\n",
    "The following steps are required to set the notebook for the tasks of this lab. Follow the instructions but do not pay attention to the code and potential output messages as they are irrelevant for main content of the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8Z4LGQQBZxH",
   "metadata": {
    "id": "b8Z4LGQQBZxH"
   },
   "source": [
    "## A) Before starting make sure you change the hardware for this notebook to GPU.\n",
    "* To do this go to *'Runtime'* then *'Change runtime type'* and select GPU as the preferred Hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a87882",
   "metadata": {
    "id": "37a87882"
   },
   "source": [
    "## B) Mount your Google Drive, link to the data we will use today and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1qXSOgKGLT7",
   "metadata": {
    "id": "p1qXSOgKGLT7"
   },
   "source": [
    "* Mount your Google drive to this Colab by running the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1zg2j5Nm3Fq5",
   "metadata": {
    "id": "1zg2j5Nm3Fq5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GFtTDI2XzrFv",
   "metadata": {
    "id": "GFtTDI2XzrFv"
   },
   "source": [
    "* The camera trap data and some functions we are going to use for the lab are stored in the following public GDrive folder:<br>https://drive.google.com/drive/folders/14pRqZAfAq6o-jPU_Zllt20I9Rykwfyxt?usp=sharing\n",
    "* Open the above link and select the *'Add shortcut to Drive'* option as shown in the image below and then select *'MyDrive'*. This should now be available in your 'drive' folder in the 'Files' tab of Colab and ready to use for species classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kfWxPpqf2dAb",
   "metadata": {
    "id": "kfWxPpqf2dAb"
   },
   "source": [
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/add_data_shortcut_to_drive.png?raw=1\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0yP1P1e9t2kY",
   "metadata": {
    "id": "0yP1P1e9t2kY"
   },
   "source": [
    "* Finally, unzip the data in our working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qE5NTMl8tj0m",
   "metadata": {
    "id": "qE5NTMl8tj0m"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip drive/MyDrive/BIOS0032_AI4Environment_Lab3Data_Compressed/BIOS0032_AI4Environment_Lab3Data.zip -d /content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb48de-83a4-4fc8-9040-55869ddcf0d1",
   "metadata": {
    "id": "ZIggiI3HIIfj"
   },
   "source": [
    "## C) Run the setup script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c7040-37b3-4807-8d5b-044ed06b90e4",
   "metadata": {
    "id": "ZIggiI3HIIfj"
   },
   "source": [
    "Run the following command to download and install all the required dependencies for today's lab.\n",
    "\n",
    "These include:\n",
    "\n",
    "* Installing PyTorch, the deep learning library we are going to use for the deep learning experiments. \n",
    "* Setup MegaDetector, the animal detection framework developed by Microsoft AI for Earth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gRjVnaxi-SLJ",
   "metadata": {
    "id": "gRjVnaxi-SLJ"
   },
   "source": [
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/3_AI_for_Wildlife_Images/images/pt_and_ai4earth.png?raw=1\" alt=\"drawing\" width=\"400\"/><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab654d77-a248-4278-b008-d13d075b5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://raw.githubusercontent.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/main/3_AI_for_Wildlife_Images/setup.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b996e6",
   "metadata": {
    "id": "27b996e6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] += \":/content/CameraTraps:/content/ai4eutils:/content/yolov5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jmQxzP0gSjDV",
   "metadata": {
    "id": "jmQxzP0gSjDV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c39a3a",
   "metadata": {
    "id": "c2c39a3a"
   },
   "source": [
    "# **Part 1: Detecting Animals in Camera Trap Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db3a7d",
   "metadata": {
    "id": "76db3a7d"
   },
   "source": [
    "* After camera traps for a survey are set up, photos are captured automatically which means that **huge amounts of data can be accumulated** after some time. Interesting insights can be hiding within large image collections and even though having access to these collection sounds great, it also means that ecologists will need to spend a lot of time to view them, identify and report their underlying content.\n",
    "* As a matter of fact, **many of the images can be empty** as sensors can be triggered by irrelevant movements in the environment. In addition, it is possible to witness presence of humans or vehicles in the scene i.e. categories that are not necessarily interesting for biodiversity monitoring. \n",
    "* On that end, computer vision approaches such as [MegaDetector](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md) can be exploited to **accelerate the labeling process** and optimize for the time ecologists spend on the consevation efforts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd10a5e",
   "metadata": {
    "id": "efd10a5e"
   },
   "source": [
    "But first, let's try to understand how we would tackle the image labeling problem manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NTs3ILXEX5bL",
   "metadata": {
    "id": "NTs3ILXEX5bL"
   },
   "source": [
    "## Image Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47da2d",
   "metadata": {
    "id": "8e47da2d"
   },
   "source": [
    "The annotation of images can be intended for a couple of different scenarios:\n",
    "* Use the acquired information directly for ecological analysis\n",
    "* Use the images along with the associated labels obtained to train a machine learning model in a *supervised learning* setting\n",
    "\n",
    "Given the large amounts of camera trap data produced, ecologistics traditionally take a subset of the images, annotate and directly use the obtained information to reason about the underlying ecosystem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KJT-KFrxz_yI",
   "metadata": {
    "id": "KJT-KFrxz_yI"
   },
   "source": [
    "### Labeling Images for Ecological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70Sllrq8zpM6",
   "metadata": {
    "id": "70Sllrq8zpM6"
   },
   "source": [
    "* Labeling is a time-consuming process\n",
    "* Usually compromises are needed, e.g. annotating a subsample of the data\n",
    "* Plus, the more time a human spends labeling the more likely it is to start making mistakes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VPoi8q71aky9",
   "metadata": {
    "id": "VPoi8q71aky9"
   },
   "source": [
    "Now Let's try to label 10 images on our own to see how the process is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LwPWMDsib1An",
   "metadata": {
    "id": "LwPWMDsib1An"
   },
   "outputs": [],
   "source": [
    "# path for detection data\n",
    "detection_data_path = \"/content/BIOS0032_AI4Environment_Lab3Data/detection_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tBw3I2CkB-Fy",
   "metadata": {
    "id": "tBw3I2CkB-Fy"
   },
   "outputs": [],
   "source": [
    "# import libraries we'll use for the detection task\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from object_detection.utils import colab_utils\n",
    "from PIL import Image\n",
    "\n",
    "from bios0032utils.vision.detection import (\n",
    "    load_image_into_numpy_array,\n",
    "    transform_md_output_to_df,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjskoHmVbNXP",
   "metadata": {
    "id": "jjskoHmVbNXP"
   },
   "source": [
    "Let's load the images to annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CU7tfHkp1SFu",
   "metadata": {
    "id": "CU7tfHkp1SFu"
   },
   "outputs": [],
   "source": [
    "# total number of images to annoate\n",
    "total_images = 10\n",
    "\n",
    "# define path where the images are and get a list of all the images inside it\n",
    "image_directory = os.path.join(detection_data_path, \"images/\")\n",
    "list_of_images = os.listdir(image_directory)\n",
    "selected_list_of_images = [\n",
    "    \"2018_NB01_001794.JPG\",\n",
    "    \"2018_NB40_002921.JPG\",\n",
    "    \"2018_MT22_020230.JPG\",\n",
    "    \"2018_OMC11_009862.JPG\",\n",
    "    \"2018_NB26_025049.JPG\",\n",
    "    \"2018_MN33_009632.JPG\",\n",
    "    \"2018_NB26_000679.JPG\",\n",
    "    \"2018_MT27_005639.JPG\",\n",
    "    \"2018_NB05_002216.JPG\",\n",
    "    \"2018_NB47_006890.JPG\",\n",
    "]\n",
    "\n",
    "# create a list with the numpy arrays that correspond to each of the\n",
    "# images to annotate\n",
    "list_of_image_arrays = []\n",
    "for i, image in enumerate(selected_list_of_images):\n",
    "    image_path = os.path.join(image_directory, image)\n",
    "    image_array = load_image_into_numpy_array(\n",
    "        image_path, output_image_dimensions=(1000, 600)\n",
    "    )\n",
    "    list_of_image_arrays.append(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O_MWcffUftFd",
   "metadata": {
    "id": "O_MWcffUftFd"
   },
   "source": [
    "### **Exercise** 🐘\n",
    "---\n",
    "<br>\n",
    "\n",
    "* The **task is to annotate a set of 10 images**, putting a box around anything you believe is an animal. If an image has more than one animal, you should do this for all of them\n",
    "* Try to place the box directly around the animal, i.e. not including anything else \n",
    "* Some of these images can be empty, so if you can not see any animal, go to next image. \n",
    "* Start the process by running the next 2 cells. When you are done click submit and run the following cell right after to measure your time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MN2mkgQHE3xZ",
   "metadata": {
    "id": "MN2mkgQHE3xZ"
   },
   "outputs": [],
   "source": [
    "annotation_starts = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GibupL_nzjZ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "GibupL_nzjZ6",
    "outputId": "9f711731-b9fa-4e25-b09c-7f44f11ff6cf"
   },
   "outputs": [],
   "source": [
    "# list to store object detection annotations\n",
    "tagged_image_boxes = []\n",
    "\n",
    "# lets start annotatin the list of images generated above\n",
    "colab_utils.annotate(list_of_image_arrays, box_storage_pointer=tagged_image_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1jPaE4G1Ewyn",
   "metadata": {
    "id": "1jPaE4G1Ewyn"
   },
   "outputs": [],
   "source": [
    "# run this cell straight after your annotation\n",
    "annotation_ends = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuiR0sQgJye8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuiR0sQgJye8",
    "outputId": "571949d2-c8bf-4c77-9346-5e72033acc1e"
   },
   "outputs": [],
   "source": [
    "# convert time to minutes\n",
    "minutes = (annotation_ends - annotation_starts) // 60\n",
    "seconds = (annotation_ends - annotation_starts) % 60\n",
    "print(\n",
    "    \"The annotation of the {} images lasted {} minutes and {} seconds\".format(\n",
    "        total_images, int(minutes), np.round(seconds, 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bzL1tp8Reqoo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzL1tp8Reqoo",
    "outputId": "f7195e80-8772-4222-845e-031eb44ebdd1"
   },
   "outputs": [],
   "source": [
    "# lets see how the produced animal annotations look like\n",
    "print(tagged_image_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dLPxGlkAfGrx",
   "metadata": {
    "id": "dLPxGlkAfGrx"
   },
   "source": [
    "Each image that contains an animal has one or more boxes that correspond define the position of the 4 edges of the box within the image. The empty images have a None value instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dvON2uIzGqf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvON2uIzGqf3",
    "outputId": "432e4dcd-ac8f-4f99-cc5e-b852460f08bf"
   },
   "outputs": [],
   "source": [
    "# given produced annotation list, we calculate total animals and the number\n",
    "# of images with animals\n",
    "number_of_animal_tags = sum(x.shape[0] for x in tagged_image_boxes if x is not None)\n",
    "number_images_with_animal_tags = sum(x is not None for x in tagged_image_boxes)\n",
    "print(\n",
    "    \"In total, you found {} animals across {} images while {} out of the {} images were tagged as empty.\".format(\n",
    "        number_of_animal_tags,\n",
    "        number_images_with_animal_tags,\n",
    "        total_images - number_images_with_animal_tags,\n",
    "        total_images,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5537447",
   "metadata": {
    "id": "a5537447"
   },
   "source": [
    "## MegaDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58287744",
   "metadata": {
    "id": "58287744"
   },
   "source": [
    "* As mentioned above, to detect animals automatically we will utilize [**MegaDetector**](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md), a model developed and trained by Microsoft AI for Earth on lots of annotated images captured in a variety of ecosystems. This model is able to **detect animals, people, and vehicles within camera trap images**. \n",
    "* An already trained or else  **pretrained** model is essentially corresponding to a model with parameters learnt when training with a dataset different from the one in our task. Here, we use the model pretrained to detect animals to reason about our images.\n",
    "* MegaDetector detections comes with a confidence threshold between 0 and 1 where 0 corresponds to a completely certain prediction and 1 to a fully certain one.\n",
    "* For the particular tasks, images with low animal detection confidence are considered **empty**.\n",
    "* Note: Depending on our task we might want to relax this threhold, i.e. if we do not want to miss a single animal and we can afford viewing false detections we can lower it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZTbwGVXVoDdg",
   "metadata": {
    "id": "ZTbwGVXVoDdg"
   },
   "source": [
    "### Run MegaDetector on a set of camera trap images collected in Kenya\n",
    "These include the 10 images you annotated in the task above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WPKXwuxElmtX",
   "metadata": {
    "id": "WPKXwuxElmtX"
   },
   "outputs": [],
   "source": [
    "# define confidence threshold for MegaDetector. Images with detections below\n",
    "# this confidence will be considered empty\n",
    "confidence_threshold = \"0.2\"\n",
    "\n",
    "# define path where MegaDetector's detections will be saved\n",
    "output_file_path = \"/content/results/md_detections.json\"\n",
    "\n",
    "# name of pretrained model\n",
    "pretrained_model = \"md_v5a.0.0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wv3VixvMkZNa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wv3VixvMkZNa",
    "outputId": "1df80138-ccc8-4745-964e-2cd3f93d7185"
   },
   "outputs": [],
   "source": [
    "# run megadetector with the defined parameters on the images under the image directory\n",
    "!python /content/CameraTraps/detection/run_detector_batch.py \"$pretrained_model\" \"$image_directory\" \"$output_file_path\" --threshold \"$confidence_threshold\" --recursive --output_relative_filenames --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8qEhxiwch1Er",
   "metadata": {
    "id": "8qEhxiwch1Er"
   },
   "source": [
    "MegaDetector produced a dictionary with the detection for each image. For the same set of 10 images we calculate again the total animals and the number of images that included at least one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fvWZRY6VMT7y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvWZRY6VMT7y",
    "outputId": "02a00b66-018d-43f7-a019-844a5398fd35"
   },
   "outputs": [],
   "source": [
    "# given the generated file with megadetector's detection , we calculate total\n",
    "# animals and the number of images with animals\n",
    "with open(output_file_path) as f:\n",
    "    md_detections = json.load(f)\n",
    "\n",
    "animals_per_img_MD = []\n",
    "for md_tagged_img in md_detections[\"images\"]:\n",
    "    if md_tagged_img[\"file\"] in selected_list_of_images:\n",
    "        animals_per_img_MD.append(len(md_tagged_img[\"detections\"]))\n",
    "\n",
    "number_of_animal_tags_MD = np.sum(animals_per_img_MD)\n",
    "number_images_with_animal_tags_MD = np.count_nonzero(animals_per_img_MD)\n",
    "np.sum(np.array(animals_per_img_MD) > 0)\n",
    "print(\n",
    "    \"In total, MegaDetector found {} animals across {} images while {} out of the {} images were tagged as empty.\".format(\n",
    "        number_of_animal_tags_MD,\n",
    "        number_images_with_animal_tags_MD,\n",
    "        total_images - number_images_with_animal_tags_MD,\n",
    "        total_images,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aRCvjcnoFmXe",
   "metadata": {
    "id": "aRCvjcnoFmXe"
   },
   "source": [
    "### Visualize detections of MegaDetector\n",
    "Let's take a visual look on the annotations produced by MegaDetector. These are the same images you annotated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4PUKGh8smNms",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PUKGh8smNms",
    "outputId": "e6ff5fba-ede8-4f3a-a2a7-0a3b71bfd8d4"
   },
   "outputs": [],
   "source": [
    "# first we will use the following utility to store detections on top of\n",
    "# images in the following directory\n",
    "visualization_dir = \"/content/results/visualized_images\"\n",
    "!python /content/CameraTraps/visualization/visualize_detector_output.py \"$output_file_path\" \"$visualization_dir\" --confidence \"$confidence_threshold\" --images_dir \"$image_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ia3YMHDTm1MB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ia3YMHDTm1MB",
    "outputId": "91d9b2e2-ffaa-4dcb-dbad-a78c5d9e9032"
   },
   "outputs": [],
   "source": [
    "# show the images with bounding boxes in Colab\n",
    "for viz_file_name in [\"anno_\" + img for img in selected_list_of_images]:\n",
    "    # viewing tags from images seen before\n",
    "    print(viz_file_name)\n",
    "    im = Image.open(os.path.join(visualization_dir, viz_file_name))\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bjl_z7hASjFN",
   "metadata": {
    "id": "Bjl_z7hASjFN"
   },
   "source": [
    "**How many animals did you manage to find?!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vpBK-K0uotEj",
   "metadata": {
    "id": "vpBK-K0uotEj"
   },
   "source": [
    "### Make detections analysis-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495dda5",
   "metadata": {
    "id": "3495dda5"
   },
   "source": [
    "* We will now take a look into MegaDetector results across all of the 50 images stored under our image path.\n",
    "* Below we save the results in a pandas dataframe format to make it easier to manipulate. Run the next cell without drawing focusing too much on the underlying code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K4SMLROiTj9Z",
   "metadata": {
    "id": "K4SMLROiTj9Z"
   },
   "outputs": [],
   "source": [
    "# what do output categories of megadetector mean. we define 0 as the empty\n",
    "# category, i.e. where there is no detection within an image with more\n",
    "# than the treshold defined above\n",
    "category_dict = {\"0\": \"empty\", \"1\": \"animal\", \"2\": \"person\", \"3\": \"vehicle\"}\n",
    "\n",
    "# given detection output of megadetector, get the results in a pandas dataframe\n",
    "md_df = transform_md_output_to_df(md_detections, category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Et-aUWtucUh-",
   "metadata": {
    "id": "Et-aUWtucUh-"
   },
   "source": [
    "### **Exercise** 🦌\n",
    "---\n",
    "<br>\n",
    "\n",
    "Using the dataframe with the MegaDetector detections produced above (`md_df`), calculate:\n",
    "\n",
    "* The percentage of empty images.\n",
    "* The total number of animals detected with MegaDetector\n",
    "* The maximum number of animals detected in a single image. Can you visualize this image?\n",
    "* Number of animals per conservancy\n",
    "* What are some obvious benefits of using megadetector?\n",
    "* (Optional) If time allows change the confidence threshold of megadetector and see how the above conclusions change\n",
    "\n",
    "Note: The rows in the dataframe are more than 50, why do you think that is the case?<br>\n",
    "\n",
    "Pandas Tip: use the `.groupby()` function to make quick aggregate calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gSfpR371aDm-",
   "metadata": {
    "id": "gSfpR371aDm-"
   },
   "outputs": [],
   "source": [
    "# calculate  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdecf0",
   "metadata": {
    "id": "e2bdecf0"
   },
   "source": [
    "To conclude, MegaDetector is a great general model that can be used to identify the camera trap images that include one or more animals and draw a box around them. The above can save a lot of human processing time given that a large percentage of the photos can be empty. However, MegaDetector does not identify animals to the species level, which is not enough for accurate biodiversity monitoring that depends on more detailed information for the data captured. \n",
    "\n",
    "For this reason, we can train a model to perform species classification given the boxes of animals. That is going to be the subject of the following task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e5484",
   "metadata": {
    "id": "1a8e5484"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c86ab6",
   "metadata": {
    "id": "f8c86ab6"
   },
   "source": [
    "# **Part 2: Species Classification in the Wild**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34b313",
   "metadata": {
    "id": "de34b313"
   },
   "source": [
    "* We are going to tackle the problem of Image Classification on a challenging real-world task. In particular, we will try to **recognize species in photos captured from camera traps** i.e. static cameras set up in the wild using Deep Learning \n",
    "* The dataset used here is a subset from the [**Caltech Camera Traps**](https://lila.science/datasets/caltech-camera-traps) collection (Beery et al., 2018) captured across 20 locations (CCT20)\n",
    "* The training set consists of 6334 images distributed in an imbalanced manner across 15 species. Naturally, **some species appear during day, some during night** and some in both cases making their appearance more diverse.  \n",
    "* The 15 categories are: badger, bird, bobcat, car, cat, coyote, deer, dog, fox, opossum, rabbit, raccoon, rodent, skunk, squirrel\n",
    "* Moreover, the data come from **diverse locations** and for some of them there is **no overlap between the defined training and test sets**\n",
    "* Finally, there is an extra difficulty associated with the fact that the images are collected in the wild. For example, we can have **blurry, noisy, occluded, partial, dark or distant views** of animals to recognize\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WpB39j1gcIOX",
   "metadata": {
    "id": "WpB39j1gcIOX"
   },
   "outputs": [],
   "source": [
    "# path for classification data\n",
    "classification_data_path = (\n",
    "    \"/content/BIOS0032_AI4Environment_Lab3Data/classification_data/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec85ba",
   "metadata": {
    "id": "19ec85ba"
   },
   "outputs": [],
   "source": [
    "# import libraries we'll use for the species classification task\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bios0032utils.vision.classification import (\n",
    "    accuracy,\n",
    "    plot_confusion_matrix,\n",
    "    train,\n",
    "    validate,\n",
    ")\n",
    "from bios0032utils.vision.datasets import CTDataset\n",
    "\n",
    "# set a constant instead of random seed for repeatability\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46400795",
   "metadata": {
    "id": "46400795"
   },
   "outputs": [],
   "source": [
    "# if you have activated the GPU feature in the colab device you will get\n",
    "# better performance, otherwise a (slower) CPU will be used\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0f06a",
   "metadata": {
    "id": "58f0f06a"
   },
   "source": [
    "## Data Preparation\n",
    "The images used in this task can be imagined as the boxes extracted by a tool such as MegaDetector accompanied with an expert-provided species label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c431d0",
   "metadata": {
    "id": "c0c431d0"
   },
   "source": [
    "### Load annotation and path file\n",
    "\n",
    "* We load a python dictionary with rows that correspond to images of the CCT20 dataset \n",
    "* Each row mainly contains **path to the image**, the **target id** and the name of the **species** that relates with this target id and the **split** it belongs to. In addition it has information about the encoded **location** the photo was taken and the **datetime** which could be exploited for ecological analysis\n",
    "* The dataset is already **split into train, validation and test**\n",
    "* Notice the existence of the validation split here. This is typically used to select the best model or its hyperparameters during training as it remains independent to the training set <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd459a",
   "metadata": {
    "id": "c6dd459a"
   },
   "outputs": [],
   "source": [
    "# available dataset versions\n",
    "# cct20_labels.json: the default dataset.\n",
    "# cct20_labels_large.json: more training images\n",
    "# cct20_labels_random.json: like the default datset but the train, validation,\n",
    "#     test splits are selected in random\n",
    "\n",
    "# dictionary with train,val and test data and every row in format\n",
    "# [img_path, target_id, species]\n",
    "with open(os.path.join(classification_data_path, \"cct20_labels.json\")) as f:\n",
    "    cct20_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VgjwGWSCsxDR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgjwGWSCsxDR",
    "outputId": "ad4420dd-a0c7-48a9-8561-3b8d1e814339"
   },
   "outputs": [],
   "source": [
    "# let's see how data look like\n",
    "random.sample(cct20_dict[\"train\"], k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KRGkWB2C9rQA",
   "metadata": {
    "id": "KRGkWB2C9rQA"
   },
   "outputs": [],
   "source": [
    "# lets also store the metadata in a pandas dataframe for better\n",
    "# accessibility for analysis\n",
    "column_names = [\n",
    "    \"img_path\",\n",
    "    \"target_id\",\n",
    "    \"species\",\n",
    "    \"location\",\n",
    "    \"datetime\",\n",
    "    \"split\",\n",
    "    \"rights_holder\",\n",
    "]\n",
    "cct20_df = pd.DataFrame()\n",
    "\n",
    "# traverse dictionary entries and add rows to dataframe\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    split_df = pd.DataFrame(cct20_dict[split], columns=column_names)\n",
    "    cct20_df = pd.concat((cct20_df, split_df))\n",
    "\n",
    "cct20_df = cct20_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6sAH9nlKflRB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sAH9nlKflRB",
    "outputId": "504d6018-ab3a-4ed4-dc4b-a1cf141586c6"
   },
   "outputs": [],
   "source": [
    "# number of classes\n",
    "num_classes = cct20_df.target_id.nunique()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YhFV1qpoep-c",
   "metadata": {
    "id": "YhFV1qpoep-c"
   },
   "source": [
    "Let's take a look into how each class can look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DcrP6BXLcCm6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "DcrP6BXLcCm6",
    "outputId": "188ae119-7cb1-46bb-f0b5-868cc0bf58e9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 12))\n",
    "\n",
    "for i, sp in enumerate(set(cct20_df.species), 1):\n",
    "    sample_row = cct20_df.query(\"species==@sp\").sample(1).iloc[0]\n",
    "    fig.add_subplot(1, 15, i)\n",
    "    img_path = os.path.join(classification_data_path, sample_row.img_path)\n",
    "    img = np.array(Image.open(img_path).resize((64, 64)))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Species:{}\".format(sample_row.species))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BCygOS-Oa5Z0",
   "metadata": {
    "id": "BCygOS-Oa5Z0"
   },
   "source": [
    "Also, notice that the test set has more locations than the training set. This means that the model will be evaluated in locations it has not been trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7NLSqn0YaRLo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NLSqn0YaRLo",
    "outputId": "16281f2d-c8f4-44f3-9a67-b1d0ce3d8965"
   },
   "outputs": [],
   "source": [
    "cct20_df.groupby([\"split\"]).location.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FNUEvgEJb9N0",
   "metadata": {
    "id": "FNUEvgEJb9N0"
   },
   "source": [
    "Below we see how **imbalanced** are the classes of our dataset; some species are going to be better represented than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXRU-e4zAvXC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "xXRU-e4zAvXC",
    "outputId": "0c1bbafa-6ec2-4d76-93e0-eacb54ccfec5"
   },
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    kind=\"bar\",\n",
    "    x=\"species\",\n",
    "    y=\"count\",\n",
    "    data=cct20_df.species.value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"species\": \"count\", \"index\": \"species\"}),\n",
    "    aspect=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c10df1",
   "metadata": {
    "id": "36c10df1"
   },
   "source": [
    "### Image Augmentations\n",
    "* During deep learning training, image augmentations can be used to provide a variety in the visual appearance of the input data given that the learning stage comprises of multiple epochs/steps. Below we select a set of such augmentations to \"enhance\" our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0dc89",
   "metadata": {
    "id": "3ae0dc89"
   },
   "outputs": [],
   "source": [
    "# select final image resolution for image transformation.\n",
    "img_res = 32\n",
    "\n",
    "# image transformation to augment our dataset for better training.\n",
    "# the outcome of these transformation will be the input of the\n",
    "# deep learning model\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(size=img_res),\n",
    "        T.RandomCrop(size=img_res),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomApply(\n",
    "            [\n",
    "                T.ColorJitter(\n",
    "                    brightness=0.5,\n",
    "                    contrast=0.5,\n",
    "                    saturation=0.5,\n",
    "                    hue=0.1,\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        T.RandomGrayscale(),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(size=img_res),\n",
    "        T.CenterCrop(size=img_res),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5b7bb",
   "metadata": {
    "id": "1bc5b7bb"
   },
   "source": [
    "### Loading the camera trap dataset\n",
    "* The following PyTorch utilities let you define how the training and test data are initialized and what is returned when we load data every iteration. \n",
    "Note: This functionality is implemented by us and you do not need to take any action\n",
    "* The train, val and test splits are predefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e52dd",
   "metadata": {
    "id": "857e52dd"
   },
   "outputs": [],
   "source": [
    "train_set = CTDataset(\n",
    "    root_dir=classification_data_path,\n",
    "    annotation_dict=cct20_dict[\"train\"],\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_set = CTDataset(\n",
    "    root_dir=classification_data_path,\n",
    "    annotation_dict=cct20_dict[\"validation\"],\n",
    "    transform=val_test_transform,\n",
    ")\n",
    "test_set = CTDataset(\n",
    "    root_dir=classification_data_path,\n",
    "    annotation_dict=cct20_dict[\"test\"],\n",
    "    transform=val_test_transform,\n",
    ")\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ef825",
   "metadata": {
    "id": "d16ef825"
   },
   "source": [
    "### Show how transformed images look like\n",
    "* We preview what kind of transformations come out of the constructed dataloaders. \n",
    "* Below, we see differently augmented versions (some augmentations are stochastic) for each of the sample images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05d549",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "2c05d549",
    "outputId": "b25f4b38-d7ca-45a2-c0a1-8c68ef428020"
   },
   "outputs": [],
   "source": [
    "# plotting 3 differently augmented versions of the same image.\n",
    "total_images = 7\n",
    "total_iter = 3\n",
    "np.random.seed(42)\n",
    "random_image_idx = np.random.choice(list(range(0, len(train_set))), total_images)\n",
    "train_set_subset = torch.utils.data.Subset(train_set, random_image_idx)\n",
    "\n",
    "random_images = np.random.choice(range(0, batch_size), total_images)\n",
    "fig, ax = plt.subplots(total_iter, total_images, figsize=(12, 6))\n",
    "for i in range(total_iter):\n",
    "    train_loader_example = DataLoader(\n",
    "        train_set_subset, batch_size=total_images, shuffle=False, drop_last=False\n",
    "    )\n",
    "    loaded_sample = next(iter(train_loader_example))\n",
    "\n",
    "    for j in range(total_images):\n",
    "        ax[i, j].imshow(np.array(loaded_sample[\"img\"][j].permute([1, 2, 0]).cpu()))\n",
    "        ax[i, j].axis(\"off\")\n",
    "        ax[i, j].axis(\"off\")\n",
    "        ax[i, j].set_title(loaded_sample[\"species\"][j])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88e104",
   "metadata": {
    "id": "5f88e104"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d898c",
   "metadata": {
    "id": "915d898c"
   },
   "source": [
    "* The dataloaders defined above load the data for each batch either during the training or the prediction stage. The **train** and **validate** functions correspond to the actions taken during an iteration of the training or validation phase respectively. \n",
    "* The main difference is that **optimization takes place only during training** based on a defined loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea56b80",
   "metadata": {
    "id": "7ea56b80"
   },
   "source": [
    "### Training Parameters\n",
    "* Some **parameters** for the image classification training play an important role on the procedure.\n",
    "* For example, **number of epochs** correspond to how many iterations we are going to operate through the dataset, the selected **model** is the type of neural network we are going to train, the **loss function** that orchestreates learning and the **optimization algorithm** thar defines how parameter updates take place\n",
    "* Note: You can play with some of these later and observe how performance is affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33ff4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "61e11a7d20ee46759d3af2d6da067336",
      "0c0eab6b90fe4b10b0ffc32eddf248f0",
      "5e2bc07b72fc4a4cb4468bc798e6b226",
      "98ce5307cffa4a8ba0aacaa45e98f6fe",
      "f11ca9e9e4c647efae40672ba1177ec9",
      "a382b9a685f840db977ef12976a4edba",
      "3b2c47520d1f43df9eac510c408922a3",
      "eda1eb96d8284e488b71c8123d5adaf3",
      "d6ba35e0ecb0429fbe47c4d6fa215576",
      "0c78a0aa8bbf4a7394c6c040379c3eae",
      "2408323fd2a5433c94b6a530e97e3bb8"
     ]
    },
    "id": "ed33ff4a",
    "outputId": "9d811d06-12e3-406e-f42c-8118cf16e83c"
   },
   "outputs": [],
   "source": [
    "# number of training epochs. Each epoch corresponds to a full iteration\n",
    "# over our training images\n",
    "num_epochs = 5\n",
    "\n",
    "# loading a cnn model implemented in PyTorch. ResNet18 is picked\n",
    "# arbitrarily here, we can select other architectures supported.\n",
    "model = models.resnet18()\n",
    "\n",
    "# loading a cnn model implemented in PyTorch PRETRAINED on ImageNet\n",
    "# model = models.resnet18(pretrained=True) #uncomment this line to\n",
    "# try transfer learning i.e. using a pretrained network as our starting point\n",
    "\n",
    "# input Features before final layer of the model\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# we define the output size of the network as the number of classes,\n",
    "# e.g. to produce a probability distribution over the 15 classes of CCT20\n",
    "model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# we are using the cross entropy loss as our loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# and stochastic gradient descent (sgd) as the optimization algorithm\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbab176",
   "metadata": {
    "id": "9dbab176"
   },
   "source": [
    "### Training a model from Scratch\n",
    "* We are going to use ResNet18, one of the CNN architectures already implemented in torchvision and train it from scratch on species recognition on Caltech Camera Traps (CCT20).\n",
    "* If you wanna learn more about the network architectures follow the torchvision documentation or read the respective paper that suggested it. For example, information about the ResNet18 can be found [here](https://pytorch.org/vision/stable/models/resnet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad1153",
   "metadata": {
    "id": "acad1153"
   },
   "source": [
    "Training of our model... \n",
    "* We train the model for multiple iterations. Observe how training and validation accuracy evolve through training\n",
    "* As the final model we keep the one that achieved the highest validation accuracy during training. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb218d8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb218d8a",
    "outputId": "1f681a4b-2602-40f2-a3ed-6a4836930f20"
   },
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# main train loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    print(\"Train epoch {} ... \\n\".format(epoch))\n",
    "    # training step: train routine passes images through the model,\n",
    "    # get predictions and based on the available labels reward or\n",
    "    # punish these predictions and optimize network\n",
    "    train_loss, train_acc = train(\n",
    "        train_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # validation step: given the current model get predictions\n",
    "    # on the validation set of images\n",
    "    val_loss, val_acc, _ = validate(\n",
    "        val_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        device,\n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "    # store training and validation accuracies through training stage.\n",
    "    # these should be getting better as we train\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    # if new validation accuracy is the best so far, keep the model\n",
    "    # that produced it as the best version\n",
    "    if val_acc > best_val_acc:\n",
    "        print(\"New best validation accuracy on epoch {} \\n \\n\".format(epoch))\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(f\"Best validation Accuracy: {best_val_acc:4f} achieved on epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OpSZarSzUj43",
   "metadata": {
    "id": "OpSZarSzUj43"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361b4f8",
   "metadata": {
    "id": "7361b4f8"
   },
   "source": [
    "### Training Accuracy vs Validation Accuracy\n",
    "* We plot the progress of the training and the validation accuracy given predictions by the model during its training stages. Are we overfitting? \n",
    "* This plot reveals the progress during training. As you can see the model kept learning after the first 5 epochs so it is highly likely that we would get better results if we let it train for more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80764957",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "80764957",
    "outputId": "0ab62834-93ac-408c-cfab-920be2cb2b1b"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Training vs Validation Accuracy during training\")\n",
    "plt.plot(train_acc_list, color=\"blue\", label=\"Training Set\")\n",
    "plt.plot(val_acc_list, color=\"orange\", label=\"Validation Set\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Top-1 Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bf39d",
   "metadata": {
    "id": "6c6bf39d"
   },
   "source": [
    "### Test set Evaluation\n",
    "* Now we apply the trained model on the unseen test set and get its predictions and the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2ded6",
   "metadata": {
    "id": "f7d2ded6"
   },
   "outputs": [],
   "source": [
    "# load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# evaluate on unseen test set\n",
    "_, test_acc, preds = validate(test_loader, model, criterion, device, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ArFxhwWkNED3",
   "metadata": {
    "id": "ArFxhwWkNED3"
   },
   "source": [
    "### Make predictions analysis ready\n",
    "Get predictions across images in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vIyg5C_9MtzQ",
   "metadata": {
    "id": "vIyg5C_9MtzQ"
   },
   "outputs": [],
   "source": [
    "# add test image predictions in a dataframe\n",
    "cct20_pred_df = cct20_df.query('split==\"test\"').copy()\n",
    "cct20_pred_df[\"predicted_target_id\"] = np.array(preds)\n",
    "\n",
    "# create a dictionary that maps target_id / predicted_target_id to species\n",
    "target_id_to_species = dict(\n",
    "    cct20_pred_df.drop_duplicates(\"target_id\")[[\"target_id\", \"species\"]]\n",
    "    .sort_values(\"target_id\")\n",
    "    .values\n",
    ")\n",
    "cct20_pred_df[\"predicted_species\"] = cct20_pred_df[\"predicted_target_id\"].apply(\n",
    "    lambda x: target_id_to_species[x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dj9IGd8OsTY",
   "metadata": {
    "id": "1dj9IGd8OsTY"
   },
   "source": [
    "### **Exercise** 🦓\n",
    "---\n",
    "<br>\n",
    "\n",
    "Using the dataframe with the model predictions (`cct20_pred_df`) do the following:\n",
    "* Calculate and ideally plot the test **accuracy per species**. Above we have calculated overall accuracy but breaking down will inform us where our model struggles the most.\n",
    "* Calculate the accuracy of the model across test **locations** comparing performance between **seen** and **unseen** locations. Unseen locations, essentialy correspond to camera trap sites producing images that are not included during training. Below we provide the two sets of locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aRDMLFydR-vL",
   "metadata": {
    "id": "aRDMLFydR-vL"
   },
   "outputs": [],
   "source": [
    "# calculate  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "__87V5N2QQZc",
   "metadata": {
    "id": "__87V5N2QQZc"
   },
   "outputs": [],
   "source": [
    "unseen_locations = set(cct20_df.query('split==\"test\"').location) - set(\n",
    "    cct20_df.query('split==\"train\"').location\n",
    ")\n",
    "seen_locations = set(cct20_df.query('split==\"test\"').location).intersection(\n",
    "    set(cct20_df.query('split==\"train\"').location)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ssspV3IlEw8",
   "metadata": {
    "id": "5ssspV3IlEw8"
   },
   "source": [
    "### Species Confusion Matrix\n",
    "* As we mentioned before, accuracy might not always be the right metric\n",
    "* A confusion matrix also reveal the types of mispredictions that take place and is a useful way to perform error analysis and imrove our model in an informed way\n",
    "* A healthy confusion matrix has a green (high accuracy) diagonal and red (low accuracy) elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p_XBRdaEXUbk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "p_XBRdaEXUbk",
    "outputId": "569ae27a-5c60-453f-f713-fa587864f0c8"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cct20_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-TaU1Fp60Oo",
   "metadata": {
    "id": "G-TaU1Fp60Oo"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faKUt6_M61wd",
   "metadata": {
    "id": "faKUt6_M61wd"
   },
   "source": [
    "## Extra Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ea779",
   "metadata": {
    "id": "d15ea779"
   },
   "source": [
    "### Exercise: Transfer Learning 🦆\n",
    "* Now let's use the same architecture used above (i.e. ResNet18) but **start from a pretrained version** (ImageNet weights) instead of training from scratch.\n",
    "* Transfer-learning from models pretrained on larger datasets is quite common practice in computer vision, i.e. models can be pretrained on ImageNet which instead totals around 1 million images that cover 1000 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-7zaWE9suwy6",
   "metadata": {
    "id": "-7zaWE9suwy6"
   },
   "source": [
    "All you need to do go above in *Training Parameters* and replace `models.resnet18()` with ` models.resnet18(pretrained=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cRSo5ZiZPuK",
   "metadata": {
    "id": "8cRSo5ZiZPuK"
   },
   "source": [
    "**If time allows**, you can proceed with the following exercises to get more insights about decisions in the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-57zu_63lWib",
   "metadata": {
    "id": "-57zu_63lWib"
   },
   "source": [
    "### Exercise: Using more training data 🦍\n",
    "\n",
    "\n",
    "Now let's use a larger portion of data in our training set.\n",
    "* Go to the *Load annotation and path file* section and replace *cct20_labels.json* with *cct20_labels_large.json*\n",
    "* Re-run the above chunk of code. You can run either training from random initialization OR transfer learning for simplicity\n",
    "* What are your findings on the results after increasing the training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6d0e6",
   "metadata": {
    "id": "e0d6d0e6"
   },
   "source": [
    "### Exercise: Split train, validation and test sets differently 🦒\n",
    "\n",
    "* Similarly to above, we replace dataset with an alternative version. Here we use the same images as in *cct20_labels.json* but the train/validation/test splits are selected randomly. \n",
    "* Go to the *Load annotation and path file* section and replace *cct20_labels.json* with *cct20_labels_random.json*\n",
    "* Re-run the above chunk of code. You can run either training from random initialization OR transfer learning for simplicity\n",
    "* What are your findings on the results after training with random? What's the relationship between the training and the validation/test loss now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f20a",
   "metadata": {
    "id": "bd03f20a"
   },
   "source": [
    "### Additional Tasks (Optional)\n",
    "* You can **run for more epochs** to actually reach a point where the model converges. It just takes a lot of time for this lab session\n",
    "* Experiment **by changing the utilized loss function or the gradient optimizer and** its parameters such as the learning rate. \n",
    "* **Increase image size** from 32 by 32. Actually given image size was quite small. Classes with fine-grained differences might suffer from low resolution. A resolution around 128 would make more sense but hard to process without a GPU.\n",
    "* **Use larger model**. Likewise, it's fair to assume that using a better backbone, e.g. by replacing the pre-implemented ResNet18 with a ResNet50 which has double the size of parameters will lead to better performance. Again, to use a larger backbone model you'll probably need a large machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VRI-krW8qabV",
   "metadata": {
    "id": "VRI-krW8qabV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "You are encouraged to explore the rich documentation of PyTorch and learn more about this deep learning library\n",
    "You may find practical info for tools such as the ones used on this lab under the [PyTorch](https://pytorch.org/docs/stable/index.html) or [Torchvision](https://pytorch.org/vision/stable/) documentations. Also, here is a nice computer vision tutorial if you want to dig more into computer vision with PyTorch\n",
    "[Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "sci",
   "language": "python",
   "name": "sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c0eab6b90fe4b10b0ffc32eddf248f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a382b9a685f840db977ef12976a4edba",
      "placeholder": "​",
      "style": "IPY_MODEL_3b2c47520d1f43df9eac510c408922a3",
      "value": "100%"
     }
    },
    "0c78a0aa8bbf4a7394c6c040379c3eae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2408323fd2a5433c94b6a530e97e3bb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b2c47520d1f43df9eac510c408922a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e2bc07b72fc4a4cb4468bc798e6b226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eda1eb96d8284e488b71c8123d5adaf3",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6ba35e0ecb0429fbe47c4d6fa215576",
      "value": 46830571
     }
    },
    "61e11a7d20ee46759d3af2d6da067336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c0eab6b90fe4b10b0ffc32eddf248f0",
       "IPY_MODEL_5e2bc07b72fc4a4cb4468bc798e6b226",
       "IPY_MODEL_98ce5307cffa4a8ba0aacaa45e98f6fe"
      ],
      "layout": "IPY_MODEL_f11ca9e9e4c647efae40672ba1177ec9"
     }
    },
    "98ce5307cffa4a8ba0aacaa45e98f6fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c78a0aa8bbf4a7394c6c040379c3eae",
      "placeholder": "​",
      "style": "IPY_MODEL_2408323fd2a5433c94b6a530e97e3bb8",
      "value": " 44.7M/44.7M [00:01&lt;00:00, 26.9MB/s]"
     }
    },
    "a382b9a685f840db977ef12976a4edba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6ba35e0ecb0429fbe47c4d6fa215576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eda1eb96d8284e488b71c8123d5adaf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f11ca9e9e4c647efae40672ba1177ec9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
