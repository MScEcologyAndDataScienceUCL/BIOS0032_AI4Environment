{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d1c2e1-3a0d-46f1-bb8f-c47b6b23a06b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/4_AI_for_Bioacoustics/AI_for_Bioacoustics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519b66b-7870-4c8e-8ba7-7ceb7814d015",
   "metadata": {},
   "source": [
    "# Week 4 - AI for Bioacoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1c4ab-0475-4ecd-98fe-457c3e4a30fa",
   "metadata": {},
   "source": [
    "## What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf4cc-a14c-437d-8c99-d24edfad0d78",
   "metadata": {},
   "source": [
    "In this weeks practical we will explore computer audition applications in ecology such as automated animal detection and species classification from audio sensor data. Among other stuff we will:\n",
    "\n",
    "1. Learn how to visualize audio data.\n",
    "2. Apply an automated animal detector on a set of audio files.\n",
    "3. Learn to evaluate the performance of a sound detector.\n",
    "4. Use a trained neural network to detect animals.\n",
    "5. Manually extract relevant sound features.\n",
    "6. Use features to create an automated animal sound identifier.\n",
    "7. Compare between hand-crafted feature sets with learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea53d95-0644-42b5-9c7b-93f0ce6bc001",
   "metadata": {},
   "source": [
    "If time permits, we will have some time exploring how to train a neural network for audio classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4acde-b379-4a49-93c2-38a8095be264",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a625520-aa73-4a25-894c-d6dc993f9166",
   "metadata": {},
   "source": [
    "### Computer audition\n",
    "\n",
    "**What is Computer audition?**\n",
    "\n",
    "- Computer audition is the field of research that deals with the automatic\n",
    "  analysis of audio signals.\n",
    "\n",
    "- It intersects with many other fields, including machine learning, signal\n",
    "  processing, and computer vision.\n",
    "\n",
    "**What does a computer hear**?\n",
    "\n",
    "- Audio files are a sequence of numbers representing the amplitude of the sound\n",
    "  wave at a given time.\n",
    "\n",
    "- The number of samples per second is called the **sampling rate**.\n",
    "\n",
    "<img alt=\"audio and sampling rate\" width=\"600\" src=\"https://cdn.shopify.com/s/files/1/1169/2482/files/Sampling_Rate_Cover_image.jpg?v=1654170259\"></img>\n",
    "\n",
    "**What tasks can we do with computer audition?**\n",
    "\n",
    "Computer audition is used in a wide range of applications, including:\n",
    "\n",
    "- Speech recognition: Siri, Alexa, Google Assistant\n",
    "- Music information retrieval: Spotify, Shazam\n",
    "- Audio classification: What is sounding in this audio?\n",
    "- Sound event detection: Transcription of audio into a sequence of events.\n",
    "\n",
    "<img alt=\"Sound event detection\" width=\"400\" src=\"http://d33wubrfki0l68.cloudfront.net/508a62f305652e6d9af853c65ab33ae9900ff38e/17a88/images/tasks/challenge2016/task3_overview.png\"></img>\n",
    "\n",
    "> Taken from the paper: Mesaros, A., Heittola, T., Diment, A., Elizalde, B., Shah, A., Vincent, E., ... & Virtanen, T. (2017, November). DCASE 2017 challenge setup: Tasks, datasets and baseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events.\n",
    "\n",
    "Recently neural network models have taken over the field of computer audition and are being used to solve many of the above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56935d83-a08f-4a2f-9ba6-45a3b3660833",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "\n",
    "**Acoustic sensors** can be used to collect field recordings of animal sounds.\n",
    "\n",
    "Usually, these sensors are deployed statically in the field for a long periods of time and record sounds continuously. This is called **passive acoustic monitoring**.\n",
    "\n",
    "<img alt=\"passive acoustic monitoring\" width=\"400\" src=\"https://wittmann-tours.de/wp-content/uploads/2018/06/AudioMoth.jpg\"></img>\n",
    "\n",
    "Alternatively, recordings are actively directed towards a specific animal species or sound events.\n",
    "\n",
    "<img alt=\"active recording\" width=\"400\" src=\"https://s3.amazonaws.com/cdn.freshdesk.com/data/helpdesk/attachments/production/48032687175/original/xjI7Dy3Q9kaCZinr5vf4ksNxQbjK13Yv3A.jpg?1584552543\"></img>\n",
    "\n",
    "> Taken from the Macaulay Library blog post:\n",
    "> [Sound recording tips](https://support.ebird.org/en/support/solutions/articles/48001064298-sound-recording-tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef333a71-9414-4127-b196-76f37f5f4315",
   "metadata": {},
   "source": [
    "### Acoustics for ecology\n",
    "\n",
    "The sound at a site is a reflection of the species present in the area and other\n",
    "environmental factors.\n",
    "\n",
    "<img alt=\"composition of acoustic space\" width=\"400\" src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs12304-017-9288-5/MediaObjects/12304_2017_9288_Fig1_HTML.gif?as=webp\"></img>\n",
    "\n",
    "> Taken from the paper: Mullet, T.C., Farina, A. & Gage, S.H. The Acoustic\n",
    "> Habitat Hypothesis: An Ecoacoustics Perspective on Species Habitat Selection.\n",
    "> Biosemiotics 10, 319‚Äì336 (2017). https://doi.org/10.1007/s12304-017-9288-5\n",
    "\n",
    "If we could link the sounds to the species, we could use this information to\n",
    "study and monitor the biodiversity of an area.\n",
    "\n",
    "Acoustic sensors produce a lot of data, and it is not always easy to analyse.\n",
    "Can we use computer audition to help us?\n",
    "\n",
    "In this practical we will explore the task of **animal sound detection** and\n",
    "**species classification**, using both manual and automated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386638-3d9b-4e64-8c38-58f86f6946ad",
   "metadata": {},
   "source": [
    "## Setup Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0c85-f597-4e48-ba23-dc19fe24996a",
   "metadata": {},
   "source": [
    "Here we will go through the steps to setup the environment for this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ec3ff-fd0e-466b-af3a-a5b549227de5",
   "metadata": {},
   "source": [
    "### Make sure to use GPU runtime in Colab. \n",
    "\n",
    "Go to `Runtime` -> `Change runtime type` and select `GPU` as the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ceeb1-d929-4570-98d5-d42bc7468552",
   "metadata": {},
   "source": [
    "### Mount your Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c1656-f864-414a-99ad-7e963cb2d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c907e-d1d2-47ae-8786-dbe2ab2fcaa5",
   "metadata": {},
   "source": [
    "Add a shortcut in you drive to this [shared folder](https://drive.google.com/drive/folders/1hbbbsILNBsQghktuj0z_Jq_3iEZQCCbj?usp=share_link).\n",
    "\n",
    "This will allow you to access the data we will use in this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aba172-13cd-4962-8465-40ce5d134346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Extract data into machine\n",
    "!unzip /content/drive/MyDrive/BIO0032_AI4Environment/week4_data.zip -d /content/week4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b1804-eda7-4ea2-9ba7-72e15404950d",
   "metadata": {},
   "source": [
    "### Install and import dependencies. \n",
    "\n",
    "Run the following cell to install the required dependencies. This will take a few minutes. You can omit the outputs of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cafa5-4655-48f2-b7ef-621fc148425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get install libfftw3-dev libicu-dev libsndfile1-dev libqt5core5a\n",
    "!pip install pytadarida git+https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment.git git+https://github.com/mbsantiago/batdetect2 umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a40ea-60a5-4cbb-9f4d-a586c55dd98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import ipywidgets\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import xarray as xa\n",
    "from IPython.display import Audio\n",
    "from librosa import display\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "from bios0032utils.bioacoustics import detection, evaluate_detection, plotting\n",
    "from bios0032utils.bioacoustics.classification import (\n",
    "    TADARIDA_FEATURES,\n",
    "    load_bat_call_audio_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0aaa9-819e-4ed3-a6f7-092b445427e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Detecting Animal Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33734ab6-584a-4827-bf6c-7976b3168c80",
   "metadata": {},
   "source": [
    "- Most of the time, we are not interested in all the sounds in a recording, but only in the sounds of a specific animal species.\n",
    "\n",
    "- Acoustic sensors will indiscriminately record all sounds in the environment, including those of animals, wind, rain, etc. Although some recorders can be triggered by a specific sound, this is not always the case.\n",
    "\n",
    "- Passive acoustic monitoring produces many hours of recordings, and it's hard to identify and explore the sounds of interest.\n",
    "\n",
    "These are similar problems to the ones we have seen in the previous practicals for camera trap images.\n",
    "\n",
    "While not as developed as in **computer vision**, there are some tools for automatically **detecting animal sounds** in recordings. Here we will explore a few of them.\n",
    "\n",
    "But first we need to understand how to visualise and annotate sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6d8ce-4bbc-4e71-b29c-a6b7100d5544",
   "metadata": {},
   "source": [
    "### Animal sounds visualisation\n",
    "\n",
    "- While we can listen to the sounds in a recording, it is often easier to\n",
    "visualise them.\n",
    "\n",
    "- This is especially true when we want to compare sounds from\n",
    "different recordings or navigate quickly without the need to listen.\n",
    "\n",
    "- We can use waveplots and spectrograms to visualise the sounds in a recording.\n",
    "\n",
    "Now we will load a dataset of animal recordings provided by [Avisoft](https://www.avisoft.com/animal-sounds/) and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a1eef-d8a1-4efe-94d1-56a6f610d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f7a88-6c4d-4a4e-82c8-94d14703cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVISOFT_AUDIO_DIR = DATA_DIR / \"avisoft\" / \"audio\"\n",
    "AVISOFT_METADATA_FILE = DATA_DIR / \"avisoft\" / \"avisoft_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eac016-30de-49b7-a1bc-3faa987b7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata dataframe\n",
    "avisoft = pd.read_csv(AVISOFT_METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc08b3-6394-493f-9f67-28740719d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few rows\n",
    "avisoft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5db25f-fa7d-4049-a37c-fa34692737b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random file from the dataset\n",
    "random_recording = avisoft.sample(n=1).iloc[0]\n",
    "\n",
    "# read the audio file and import it as a numpy array\n",
    "wav, samplerate = librosa.load(\n",
    "    os.path.join(AVISOFT_AUDIO_DIR, random_recording.wav),\n",
    "    sr=None,\n",
    ")\n",
    "\n",
    "# Compute the duration of audio\n",
    "num_samples = len(wav)  # Number of samples taken by the recorder\n",
    "duration = num_samples / samplerate\n",
    "\n",
    "# Get name of animal\n",
    "animal_name = random_recording.english_name\n",
    "\n",
    "print(f\"File selected = {random_recording.wav}\")\n",
    "print(f\"Samplerate = {samplerate} Hz\")\n",
    "print(f\"Duration = {duration:.2f} s\")\n",
    "print(f\"Species = {animal_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a67a0-10bd-4400-bef5-5a260bf978c3",
   "metadata": {},
   "source": [
    "Lets first listen to the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf5b52-f1e8-4085-81e4-5a547752a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=wav, rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633c8a3-6948-4497-a4bf-a4c319bd4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of the waveform\n",
    "times = np.linspace(0, duration, num_samples)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(times, wav)\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.title(f\"Waveform of {animal_name} sound\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eead3-d1d3-41ee-8bc3-970ed31fe704",
   "metadata": {},
   "source": [
    "The **waveform** gives us a visual representation of the sound amplitude over time.\n",
    "\n",
    "However, ff there are multiple simultaneous sounds in the recording, it can be hard to see each individual sound. \n",
    "\n",
    "We can use a **spectrogram** to decompose the sound into **frequencies** and visualise them as a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650df605-e01b-48aa-a436-729aaad2f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram with the short time fourier transform (STFT)\n",
    "spectrogram = np.abs(librosa.stft(wav))\n",
    "\n",
    "# Amplitude is best represented in logarithmic scale (decibels)\n",
    "db_spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def0c4b-b15d-4c7a-bf8e-61eb226b91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of spectrogram\n",
    "num_freq_bins, num_time_bins = db_spectrogram.shape\n",
    "times = np.linspace(0, duration, num_time_bins)\n",
    "freqs = np.linspace(0, samplerate / 2, num_freq_bins)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(times, freqs, db_spectrogram, cmap=\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(f\"Spectrogram of {random_recording.english_name} sound\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"freq (Hz)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbde3f-289e-4097-aa94-9abd336bb6e1",
   "metadata": {},
   "source": [
    "### Exercise üêò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72092175-0c8c-440d-82b5-a6d5c75bcb86",
   "metadata": {},
   "source": [
    "The sounds produced by animals can be very different from each other. The\n",
    "transformation used to create the spectrogram, called the **short-time Fourier\n",
    "transform** (STFT), will highlight different features of the sound depending on\n",
    "the parameters used.\n",
    "\n",
    "- Research what the STFT is and how its parameters affect the spectrogram. In\n",
    "  particular, try to understand the effect of the **window size** and the **hop\n",
    "  size** or **overlap**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e48e0-3925-4e04-afe4-8b7c8166de69",
   "metadata": {},
   "source": [
    "The **Fourier transform** is a method for breaking a time series into its continuent frequencies. [Click here](https://www.youtube.com/watch?v=spUNpyF58BY) if you are interested in an intuitive and visual explanation of the Fourier transform.\n",
    "\n",
    "For the **short-time Fourier transform** the audio is broken up into  **windows** (or **chunks** or **frames**), which usually overlap each other. Each **window** is Fourier transformed, and the result is added to a matrix, which records magnitude for each point in time and frequency.\n",
    "\n",
    "![short time fourier transform](https://www.mdpi.com/applsci/applsci-10-07208/article_deploy/html/images/applsci-10-07208-g001-550.jpg)\n",
    "\n",
    "In order to compute a **STFT** of a signal you must select a `window_length` (or `n_fft`) and a `hop_length` (or `overlap`) to determine how to break up the signal into **windows**.\n",
    "\n",
    "* The `hop_size` controls the temporal resolution, or the minimum interval at which you can detect changes in sound. If `hop_length = 128` then any transient sounds of length less than 128 samples will be hard to detect. \n",
    "\n",
    "* The `window_length` controls the frequecy resolution. With larger `window_length` it is possible to distinguish between closer frequencies.\n",
    "\n",
    "* Selecting a high/low value for `window_length` will produce spectrograms with many/few frequency bins.\n",
    "\n",
    "* Similarly, a high/low value for `hop_length` will produce spectrograms with many/freq time bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dad7c-f01f-4c9c-aad4-4021450b9d93",
   "metadata": {},
   "source": [
    "Here you can visualise sounds from different species and see how the STFT\n",
    "parameters affect the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ace48-5613-4dd0-a28d-94f2d5e258d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Interactive spectrogram of animal sounds\n",
    "\n",
    "# @markdown Select the file you wish to visualize. Modify the spectrogram parameters to see its effect on the spectrogram. Change the reproduction speed for interesting effects!\n",
    "\n",
    "# Select some varied sounds from avisoft dataset\n",
    "examples = [\n",
    "    (row.english_name, os.path.join(AVISOFT_AUDIO_DIR, row.wav))\n",
    "    # select one random recording per taxonomic group\n",
    "    for row in avisoft.groupby(\"order\").sample(n=1).itertuples()\n",
    "]\n",
    "\n",
    "# Create interactive plot\n",
    "ipywidgets.interact(\n",
    "    plotting.plot_waveform_with_spectrogram,\n",
    "    hop_length=(32, 1024, 32),\n",
    "    n_fft=(32, 2048, 32),\n",
    "    window=plotting.WINDOW_OPTIONS,\n",
    "    file=examples,\n",
    "    cmap=plotting.COLORMAPS,\n",
    "    speed=[\n",
    "        (\"x1\", 1),\n",
    "        (\"x1.5\", 1.5),\n",
    "        (\"x2\", 2),\n",
    "        (\"x0.5\", 0.5),\n",
    "        (\"x0.2\", 0.2),\n",
    "        (\"x0.1\", 0.1),\n",
    "    ],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b087ff-f0c2-4fcb-bbb4-250f778aa86b",
   "metadata": {},
   "source": [
    "- Try changing the parameters and see how they affect sounds from different\n",
    "  species.\n",
    "\n",
    "- Can you see that some choice of parameters are good for some species but\n",
    "  not for others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9a748-1168-4044-9745-078921081f65",
   "metadata": {},
   "source": [
    "Some species have slowly changing frequency, like a Sheep, hence selecting a high `hop_length` would be able to capture its vocalization accurately without few temporal samples.\n",
    "\n",
    "Other species, such as the Little Grebe, have quickly varying frequencies, and a high `hop_length` would blur the intricacies of its song.\n",
    "\n",
    "A small `window_length` can be chosen in case identification does not rely on accurate frequency information. For example the Hoopoe call consists of a burst of three rapid pulses at low frequencies. This pattern can be cleary distinguished even with low frequency resolution.\n",
    "\n",
    "Ofter species will call at similar frequency bands. In such case it's best to select a `window_length` that will produce enough frequency resolution to distinguish between similar calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7e57d-12a3-41c3-b35a-eeddf933e990",
   "metadata": {},
   "source": [
    "- How do the parameters affect the computation time and resulting image size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4650b-0df6-4a37-b03f-42ee46119ad9",
   "metadata": {},
   "source": [
    "* Larger `window_length` will produce taller spectrograms and slow down computation time.\n",
    "* Smaller `hop_length` will produce lengthier spectrograms and slow down computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6eb41-65a3-4d1d-a871-c9dd50328363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a single file\n",
    "species, filepath = examples[0]\n",
    "\n",
    "print(\n",
    "    f\"Will generate multiple spectrograms of {species} sounds. Using the file: {filepath}\"\n",
    ")\n",
    "\n",
    "# load the audio\n",
    "wav, sr = librosa.load(filepath, sr=None)\n",
    "\n",
    "# select multiple choices of window_length and hop_length\n",
    "window_lengths = np.arange(64, 2048, 64)\n",
    "hop_lengths = np.arange(32, 1024, 32)\n",
    "\n",
    "# create list in which to store the resulting computation times\n",
    "computation_times = []\n",
    "\n",
    "# iterate over all window_length and hop_length options\n",
    "for window_length in window_lengths:\n",
    "    for hop_length in hop_lengths:\n",
    "        # start counter\n",
    "        computation_time = perf_counter()\n",
    "\n",
    "        # compute spectrogram\n",
    "        spectrogram = librosa.amplitude_to_db(\n",
    "            np.abs(\n",
    "                librosa.stft(\n",
    "                    wav,\n",
    "                    hop_length=hop_length,\n",
    "                    n_fft=window_length,\n",
    "                    window=\"hann\",\n",
    "                )\n",
    "            ),\n",
    "            ref=np.max,\n",
    "        )\n",
    "\n",
    "        # end counter\n",
    "        computation_time = perf_counter() - computation_time\n",
    "\n",
    "        # store result in computation_times list\n",
    "        computation_times.append(\n",
    "            {\n",
    "                \"window_length\": window_length,\n",
    "                \"hop_length\": hop_length,\n",
    "                \"computation_time\": computation_time,\n",
    "                \"spectrogram_size\": spectrogram.size,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# convert computation_times list into a pandas dataframe\n",
    "computation_times = pd.DataFrame(computation_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace2f2-9447-4f67-af0a-39fef526250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.scatterplot(\n",
    "    data=computation_times,\n",
    "    x=\"computation_time\",\n",
    "    y=\"spectrogram_size\",\n",
    "    size=\"window_length\",\n",
    "    hue=\"hop_length\",\n",
    "    sizes=(20, 200),\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051ea3-bbed-4616-b39c-e5c6052c4863",
   "metadata": {},
   "source": [
    "### Detecting sounds\n",
    "\n",
    "As you can imagine, it is not easy to manually annotate all relevant sounds in a recording.\n",
    "\n",
    "Look at this recording from a bat detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bf7b1-f4a7-4545-b0f3-e4455a204184",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_METADATA = DATA_DIR / \"yucatan\" / \"yucatan_metadata.csv\"\n",
    "\n",
    "YUCATAN_AUDIO_DIR = DATA_DIR / \"yucatan\" / \"audio\"\n",
    "\n",
    "# Load metadata of dataset of bat recordings from the yucatan peninsula\n",
    "yucatan = pd.read_csv(YUCATAN_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e66c5-401a-4dbe-b544-5fa6b7a8c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowded_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[1017])\n",
    "\n",
    "plotting.plot_spectrogram(\n",
    "    crowded_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2511b0-03fc-4fab-a720-276156e602e9",
   "metadata": {},
   "source": [
    "There are many bat calls in this recording, it would be very time consuming to annotate them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1de031-0c92-47f9-a4f5-dfb66dd01f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[205])\n",
    "plotting.plot_spectrogram(\n",
    "    empty_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dacb91-cf8a-48f4-bbf9-96b89c318638",
   "metadata": {},
   "source": [
    "This other recording has a single bat pulse. You still need to review it thoroughly to make sure there are no other sounds of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58cfa0-2b33-476a-95b1-65844dd94a34",
   "metadata": {},
   "source": [
    "### Tadarida\n",
    "\n",
    "Similar to **MegaDetector** for camera traps, there are some tools that automatically detect animal sounds in recordings.\n",
    "\n",
    "Here we will explore the tool **Tadarida**. Tadarida is a non-ML generic detector that uses a set of hand-crafted features to detect sounds. It is based on the work of:\n",
    "\n",
    "> Bas, Y., Bas, D. and Julien, J.-F., 2017. Tadarida: A Toolbox for Animal\n",
    "> Detection on Acoustic Recordings. Journal of Open Research Software, 5(1),\n",
    "> p.6. DOI: http://doi.org/10.5334/jors.154"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08608fac-89f4-4e20-ac23-2b67bed2a601",
   "metadata": {},
   "source": [
    "We will use tadarida to detect bat calls in a recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb899b1-46c1-4e66-95dc-7d68bc022921",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detection.run_tadarida_detection([empty_recording, crowded_recording])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4b7fc-d43c-430a-8f0b-537ac3312da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(crowded_recording, detections);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe222ac1-5b12-495e-96b8-44fcfbeb1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(empty_recording, detections);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a40821-856b-4646-b288-8b89382240df",
   "metadata": {},
   "source": [
    "### Evaluate Detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc807f-30b6-4210-b6e8-ecf3c16d94e7",
   "metadata": {},
   "source": [
    "The calls of this dataset were manually annotated, so we can compare the detections with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f009c8-d64a-48a0-b6e3-5bd62fae3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_ANNOTATIONS = DATA_DIR / \"yucatan\" / \"yucatan_annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305747f-6f97-4b8b-b615-43ea078a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "yucatan_annotations = pd.read_csv(YUCATAN_ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37062a1-5f2a-4b89-be9e-abba8e181382",
   "metadata": {},
   "source": [
    "Lets first visualize the ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d8c4e-79d9-4ac2-b5a2-dade6aa88d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(empty_recording, yucatan_annotations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd1dc3-1662-43fc-969a-f3b157b48e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(crowded_recording, yucatan_annotations);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f9304-ba3d-4628-bb11-7d19d9be82b9",
   "metadata": {},
   "source": [
    "Now we can compare the detections with the ground truth. We can use the\n",
    "Intersection over Union (IoU) to measure the overlap between the detections and\n",
    "the ground truth.\n",
    "\n",
    "<img alt=\"intersection over union\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/Intersection_over_Union_-_visual_equation.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c82c7b-7b3e-4539-a5d1-1c2b5fd43a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the predictions and annotations from the crowded recording\n",
    "file_detections = detections[\n",
    "    detections.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "file_annotations = yucatan_annotations[\n",
    "    yucatan_annotations.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "\n",
    "# Match the bounding boxes by computing the IoU. Discard all matches with IoU less than 0.5\n",
    "pred_boxes = evaluate_detection.bboxes_from_annotations(file_detections)\n",
    "true_boxes = evaluate_detection.bboxes_from_annotations(file_annotations)\n",
    "matches = evaluate_detection.match_bboxes(true_boxes, pred_boxes, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6384941-a2bf-4b63-a71d-3f4c86d50cbc",
   "metadata": {},
   "source": [
    "We select all the detections that have an IoU greater than 0.5 and count them as true positives. All the other detections are false positives. Sound events that are not detected are false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d095903-dcc4-443b-8c6b-1a08ea1cd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of annotated sound events\n",
    "positives = len(file_annotations)\n",
    "\n",
    "num_predictions = len(file_detections)\n",
    "\n",
    "# number of matched prediction boxes\n",
    "true_positives = len(matches)\n",
    "\n",
    "# number of predicted boxes that were not matched\n",
    "false_positives = num_predictions - len(matches)\n",
    "\n",
    "# number of annotated sound events that were not matched\n",
    "false_negatives = positives - len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ca26a-8588-469e-9b8a-7d640b07c923",
   "metadata": {},
   "source": [
    "With this information we can compute the precision and recall of the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d09b3b-7135-46a2-89ab-2a92d9e26ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of predictions that are correct\n",
    "precision = true_positives / num_predictions\n",
    "\n",
    "# Percentage of sound events that were detected\n",
    "recall = true_positives / positives\n",
    "\n",
    "print(\n",
    "    f\"Tadarida precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e1423-6f8c-4c6a-b2c3-88a3de2c2949",
   "metadata": {},
   "source": [
    "Lets plot predictions and annotations at the same time.\n",
    "\n",
    "- red = spurious predicted sound event (false positive)\n",
    "- green = correct prediction (true positive)\n",
    "- white = missed sound event (false negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b48e0-4a7b-48ab-a852-af37516fb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    detections,\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891dec79-2156-4a10-9162-847cd77c0ad5",
   "metadata": {},
   "source": [
    "We can also compute the precision/recall on each file in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f05dd3-e7f9-48bc-a178-d3d759c2f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed tadarida detections to save some time\n",
    "full_tadarida_detections = pd.read_csv(\n",
    "    DATA_DIR / \"yucatan\" / \"yucatan_tadarida_detections.csv\"\n",
    ")\n",
    "\n",
    "# compute the precision and recall for each file\n",
    "td_evaluation = []\n",
    "for filename in yucatan_annotations.recording_id.unique():\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        filename,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=0.5,\n",
    "    )\n",
    "    td_evaluation.append({\"wav\": filename, \"precision\": precision, \"recall\": recall})\n",
    "\n",
    "# store the results in a pandas dataframe\n",
    "td_evaluation = pd.DataFrame(td_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b581d-2ea9-4116-b83d-3c2becd2d077",
   "metadata": {},
   "source": [
    "### Exercise üêã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902f1d1-58fe-4e9d-a333-bb26fa898210",
   "metadata": {},
   "source": [
    "Using the dataframe with precision and recall of tadarida on each file (`td_evaluation`), calculate: \n",
    "\n",
    "- The mean precision and recall across all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67d89d-b407-4e27-8871-8b7ccfcb0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first rows of the td_evaluation dataframe\n",
    "td_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42dc591-83e6-453f-a3c0-1e2472148a63",
   "metadata": {},
   "source": [
    "Notice that some precision and recall values are **NaN**.\n",
    "\n",
    "Precision is **NaN** when there were no detections made by tadarida.\n",
    "\n",
    "Recall is **NaN** when there are no true bat sounds in the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82473263-9fe4-4743-8fdf-f8f5356fb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean precision and recall.\n",
    "# The pandas mean method will ignore NaNs for mean computation.\n",
    "mean_precision = td_evaluation.precision.mean()\n",
    "mean_recall = td_evaluation.recall.mean()\n",
    "\n",
    "print(f\"Mean precision = {mean_precision:.2%}, Mean recall = {mean_recall:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1c0e7-b486-43ad-a98e-e8dd9db19ccd",
   "metadata": {},
   "source": [
    "- The percentage of files where all bat calls were missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5636c-0f69-49c0-b092-e4d2b97f5ec7",
   "metadata": {},
   "source": [
    "As recall is **NaN** when no true bat sounds are in the recording we can ignore these files.\n",
    "\n",
    "Saying `recall = 0` is equivalent to saying that all bat call were missed. This is because\n",
    "\n",
    "    recall = true_positives / positives\n",
    "    \n",
    "And so for `recall = 0` this needs `true_positives = 0`, equivalently no detection was correct and all bat calls were missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9c108-70d8-4ba9-9636-36821d346ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of the td_evaluation dataframe gives the number of evaluated files\n",
    "total_files = len(td_evaluation)\n",
    "\n",
    "# remove NaNs from the recall column\n",
    "non_na_recalls = td_evaluation.recall.dropna()\n",
    "\n",
    "# check which recalls are 0\n",
    "recall_is_cero = non_na_recalls == 0\n",
    "\n",
    "# count the number of True\n",
    "num_recall_is_cero = recall_is_cero.sum()\n",
    "\n",
    "# compute the percentage\n",
    "perc_all_bats_missed = num_recall_is_cero / total_files\n",
    "\n",
    "print(\n",
    "    f\"Percentage of files where all bat calls were missed: {perc_all_bats_missed:.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80706f1d-27c9-4392-ad7e-5fae61d45411",
   "metadata": {},
   "source": [
    "- The percentage of files where at least half of the predictions were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8efa9-5d9b-490a-a3f4-a5f1faed79cf",
   "metadata": {},
   "source": [
    "We can ignore files were precision is **NaN**, since this occurs when no predictions were made.\n",
    "\n",
    "Saying `precision >= 0.5` is equivalent to saying that at least half of the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d75b5-c077-4711-afbf-af6bec6ce5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of the td_evaluation dataframe gives the number of evaluated files\n",
    "total_files = len(td_evaluation)\n",
    "\n",
    "# remove NaNs from the precision column\n",
    "non_na_precisions = td_evaluation.precision.dropna()\n",
    "\n",
    "# check which recalls are 0\n",
    "precision_is_gt_half = non_na_precisions >= 0.5\n",
    "\n",
    "# count the number of True\n",
    "num_precision_is_gt_half = precision_is_gt_half.sum()\n",
    "\n",
    "# compute the percentage\n",
    "perc_at_least_half_preds_correct = num_precision_is_gt_half / total_files\n",
    "\n",
    "print(\n",
    "    f\"Percentage of files where at least half of the predictions are correct: {perc_at_least_half_preds_correct:.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe97e9-24aa-44a0-8e5f-9db037f81d9a",
   "metadata": {},
   "source": [
    "Run the full evaluation again but change the `iou_threshold` parameter. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65950947-aad5-48bf-8e70-9b1bea4a02ac",
   "metadata": {},
   "source": [
    "* Increasing the **IoU threshold** makes it harder to find matches (`true_positives`). Recall will necessarily be lowered, as less bat calls can be detected. Precision will also be lowered as less detections can be correct.\n",
    "\n",
    "* Conversely, a lower **IoU threshold** increases the number of matches (`true_positives`), making both recall and precision greater.\n",
    "\n",
    "* Selecting an **IoU threshold** is not about optimizing performance, but about choosing a matching criterion. Having a low **IoU** threshold might increase the recall and precision but incurr a cost in precision of the bounding box of each detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6f39a-d2d5-4d66-bae6-2eba3f1a7369",
   "metadata": {},
   "source": [
    "Lets create a function to do the whole evaluation with `iou_threshold` as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7918e-5ad4-4a7c-a000-f52a56bed578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_whole_evaluation(iou_threshold):\n",
    "    # compute the precision and recall for each file\n",
    "    td_evaluation = []\n",
    "    for filename in yucatan_annotations.recording_id.unique():\n",
    "        precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "            filename,\n",
    "            full_tadarida_detections,\n",
    "            yucatan_annotations,\n",
    "            iou_threshold=iou_threshold,\n",
    "        )\n",
    "        td_evaluation.append(\n",
    "            {\"wav\": filename, \"precision\": precision, \"recall\": recall}\n",
    "        )\n",
    "\n",
    "    # store the results in a pandas dataframe\n",
    "    td_evaluation = pd.DataFrame(td_evaluation)\n",
    "    return td_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4dfcd4-0089-4df9-bd98-479c0b0477aa",
   "metadata": {},
   "source": [
    "We create another function to compute evaluation statistics of the previous points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40504a-231a-40bf-83f4-ba25c57d2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_statistics(td_evaluation):\n",
    "    # compute mean precision and recall.\n",
    "    # The pandas mean method will ignore NaNs for mean computation.\n",
    "    mean_precision = td_evaluation.precision.mean()\n",
    "    mean_recall = td_evaluation.recall.mean()\n",
    "\n",
    "    # the length of the td_evaluation dataframe gives the number of evaluated files\n",
    "    total_files = len(td_evaluation)\n",
    "\n",
    "    # remove NaNs from the recall column\n",
    "    non_na_recalls = td_evaluation.recall.dropna()\n",
    "\n",
    "    # check which recalls are 0\n",
    "    recall_is_cero = non_na_recalls == 0\n",
    "\n",
    "    # count the number of True\n",
    "    num_recall_is_cero = recall_is_cero.sum()\n",
    "\n",
    "    # compute the percentage\n",
    "    perc_all_bats_missed = num_recall_is_cero / total_files\n",
    "\n",
    "    # remove NaNs from the precision column\n",
    "    non_na_precisions = td_evaluation.precision.dropna()\n",
    "\n",
    "    # check which recalls are 0\n",
    "    precision_is_gt_half = non_na_precisions >= 0.5\n",
    "\n",
    "    # count the number of True\n",
    "    num_precision_is_gt_half = precision_is_gt_half.sum()\n",
    "\n",
    "    # compute the percentage\n",
    "    perc_at_least_half_preds_correct = num_precision_is_gt_half / total_files\n",
    "\n",
    "    return {\n",
    "        \"mean_precision\": mean_precision,\n",
    "        \"mean_recall\": mean_recall,\n",
    "        \"perc_all_bats_missed\": perc_all_bats_missed,\n",
    "        \"perc_at_least_half_preds_correct\": perc_at_least_half_preds_correct,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9b61e-afb4-4482-8def-ffecaab252bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# run evaluation with multiple iou_thresholds\n",
    "for iou_threshold in tqdm([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    # evaluate each file with IoU threshold and gather results in dataframe\n",
    "    evaluation_df = run_whole_evaluation(iou_threshold)\n",
    "\n",
    "    # compute some evaluation statistics\n",
    "    stats = get_evaluation_statistics(evaluation_df)\n",
    "\n",
    "    # specify which threshold was used\n",
    "    stats[\"iou_threshold\"] = iou_threshold\n",
    "\n",
    "    # store in results list\n",
    "    results.append(stats)\n",
    "\n",
    "# convert results into dataframe\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e4cf3-ac6e-41b6-b060-21daa10ce8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe\n",
    "(\n",
    "    results.set_index(\"iou_threshold\")\n",
    "    .style.format(formatter=\"{:.1%}\")\n",
    "    .format_index(\"{:.1f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b142091-7b82-498f-bf24-0a2aa64fff5e",
   "metadata": {},
   "source": [
    "You can use the following interactive widget to get a better grasp on tadarida's behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d1d19-b15f-4d3e-b09e-13588d5c5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tadarida predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(path=example_files, iou_threshold=(0, 1, 0.1))\n",
    "def plot_results_file_results(path=crowded_recording, iou_threshold=0.5):\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Tadarida precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f2bc0-e583-416c-91ce-6a6a61e3790c",
   "metadata": {},
   "source": [
    "### BatDetect2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f767d-7a3e-486f-84ac-128fbc76b7ce",
   "metadata": {},
   "source": [
    "As we have seen the performance of Tadarida has room for improvement. We can improve performance by using a specialised machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83711ad2-c991-47af-af15-034764405700",
   "metadata": {},
   "source": [
    "Now we will use **BatDetect2**, a deep learning model for simultaneous\n",
    "detection and classification of bat calls. Although the model was trained on a\n",
    "bat calls of UK bats, we can test its detection performance on the dataset of\n",
    "bats from the Yucat√°n peninsula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a405e-1603-4ec0-8c46-2bd4d43ed67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "batdetect2 /content/week4_data/data/yucatan/audio /content/week4_data/data/yucatan/predictions 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e7cdb-e1c3-4ff4-ac21-40a6044feaf3",
   "metadata": {},
   "source": [
    "The **BatDetect2** model can predict multiple bounding boxes for each recording.\n",
    "Unlike Tadarida, each bounding box has a **score**, a predicted species and a\n",
    "confidence score for the species.\n",
    "\n",
    "We will throw out the predicted species and confidence score, and only use the\n",
    "bounding box score. The **score** is the probability that the bounding box contains\n",
    "a bat call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c2e63-14f9-404c-8d0e-068f7ac34752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all prediction files\n",
    "files = glob.glob(str(DATA_DIR / \"yucatan\" / \"predictions\" / \"*.csv\"))\n",
    "\n",
    "# Read each prediction file\n",
    "batdetect2_predictions = []\n",
    "for path in files:\n",
    "    df = pd.read_csv(path).drop(columns=[\"id\", \"class\", \"class_prob\"])\n",
    "    df[\"recording_id\"] = os.path.basename(path)[:-4]\n",
    "    batdetect2_predictions.append(df)\n",
    "\n",
    "# And concatenate them into a single dataframe\n",
    "batdetect2_predictions = pd.concat(batdetect2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43a94a-ffb0-4086-9cf0-4b5fe1936996",
   "metadata": {},
   "source": [
    "We can then use the same evaluation procedure as before to compute the\n",
    "precision and recall, except now we can select detections with a score greater\n",
    "than some customizable threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57886490-4c53-4d58-aaec-c01309e92f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "batdetect2_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea57ec7-431b-486f-a5f0-061db014a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.3\n",
    "\n",
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    batdetect2_predictions[batdetect2_predictions.det_prob > score_threshold],\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3b2f8-b55d-4e71-ac11-9367dd711ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Batdetect2 predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    path=example_files,\n",
    "    iou_threshold=(0, 1, 0.1),\n",
    "    score_threshold=(0, 1, 0.1),\n",
    ")\n",
    "def plot_batdetect2_results_file_results(\n",
    "    path=crowded_recording,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.3,\n",
    "):\n",
    "    confident_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.det_prob > score_threshold\n",
    "    ]\n",
    "\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Batdetect2 precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "        linewidth=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31c0c1-1567-4249-aa56-75dddcaf8aec",
   "metadata": {},
   "source": [
    "### Exercise üê∏\n",
    "\n",
    "Here you will compare the performance of the two detectors.\n",
    "\n",
    "1. Use the function `evaluate_detection.compute_detection_metrics` to calculate the number\n",
    "of positive (P), true positives (TP), false positives (FP) and false negatives (FN) for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d8307-c427-4889-8a81-ad68144b64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example, here is how you use the function\n",
    "example_file = example_files[0]\n",
    "\n",
    "confident_detections = batdetect2_predictions[batdetect2_predictions.det_prob > 0.3]\n",
    "\n",
    "(\n",
    "    positives,\n",
    "    true_positives,\n",
    "    false_positives,\n",
    "    false_negatives,\n",
    ") = evaluate_detection.compute_detection_metrics(\n",
    "    example_file,\n",
    "    confident_detections,\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.5,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{positives = }, {true_positives = }, {false_positives = }, {false_negatives = }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e144a-f6af-4c58-acb5-f5b1cfe856b8",
   "metadata": {},
   "source": [
    "2. Compute the overall precision and recall for each detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede78abe-d68d-44bc-91a8-79cfc2cce347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list in which to store results\n",
    "batdetect2_eval = []\n",
    "\n",
    "# select predictions with confidence score greater that 0.3\n",
    "confident_detections = batdetect2_predictions[batdetect2_predictions.det_prob > 0.3]\n",
    "\n",
    "# iterate over each file in the dataset\n",
    "for filename in yucatan[\"id\"]:\n",
    "    # compute positives, true_positives, false_positives and false_negatives for file\n",
    "    (\n",
    "        positives,\n",
    "        true_positives,\n",
    "        false_positives,\n",
    "        false_negatives,\n",
    "    ) = evaluate_detection.compute_detection_metrics(\n",
    "        filename,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=0.5,\n",
    "    )\n",
    "\n",
    "    batdetect2_eval.append(\n",
    "        {\n",
    "            \"filename\": filename,\n",
    "            \"positives\": positives,\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# convert batdetect2_eval into dataframe\n",
    "batdetect2_eval = pd.DataFrame(batdetect2_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab77aeb-3cde-46bf-aff9-09d02d7dfcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first rows to check results\n",
    "batdetect2_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71010d-7e91-4feb-9481-27b4af552ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will do the same with the tadarida detections\n",
    "# create list in which to store results\n",
    "tadarida_eval = []\n",
    "\n",
    "# iterate over each file in the dataset\n",
    "for filename in yucatan[\"id\"]:\n",
    "    # compute positives, true_positives, false_positives and false_negatives for file\n",
    "    (\n",
    "        positives,\n",
    "        true_positives,\n",
    "        false_positives,\n",
    "        false_negatives,\n",
    "    ) = evaluate_detection.compute_detection_metrics(\n",
    "        filename,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=0.5,\n",
    "    )\n",
    "\n",
    "    tadarida_eval.append(\n",
    "        {\n",
    "            \"filename\": filename,\n",
    "            \"positives\": positives,\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# convert batdetect2_eval into dataframe\n",
    "tadarida_eval = pd.DataFrame(tadarida_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba35f87-facc-4b92-8a60-6ec8437b935e",
   "metadata": {},
   "source": [
    "2. Compute the overall precision and recall for each detector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11903fe-bb4b-4d8a-94a3-bce711d9e680",
   "metadata": {},
   "source": [
    "Precision is defined as\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    \n",
    "and recall as\n",
    "\n",
    "    recall = true_positives / positives\n",
    "    \n",
    "We can compute overall precision and recall by obtaining the total number of `true_positives`, `false_positives` and `positives`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb6265-9ddc-4817-af02-862756f09b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute batdetec2 precision\n",
    "batdetect2_precision = batdetect2_eval.true_positives.sum() / (\n",
    "    batdetect2_eval.true_positives.sum() + batdetect2_eval.false_positives.sum()\n",
    ")\n",
    "\n",
    "# compute batdetect2 recall\n",
    "batdetect2_recall = (\n",
    "    batdetect2_eval.true_positives.sum() / batdetect2_eval.positives.sum()\n",
    ")\n",
    "\n",
    "# computetadarida precision\n",
    "tadarida_precision = tadarida_eval.true_positives.sum() / (\n",
    "    tadarida_eval.true_positives.sum() + tadarida_eval.false_positives.sum()\n",
    ")\n",
    "\n",
    "# compute tadarida recall\n",
    "tadarida_recall = tadarida_eval.true_positives.sum() / tadarida_eval.positives.sum()\n",
    "\n",
    "print(\n",
    "    f\"BatDetect2 -- Precision = {batdetect2_precision:.1%}  Recall = {batdetect2_recall:.1%}\"\n",
    ")\n",
    "print(\n",
    "    f\"Tadarida   -- Precision = {tadarida_precision:.1%}  Recall = {tadarida_recall:.1%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f63d50-e9a6-4a15-b8b5-63f2a5dcdc02",
   "metadata": {},
   "source": [
    "3. Which detector performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97006cb-e403-4dc0-965d-b1dc92dc56c5",
   "metadata": {},
   "source": [
    "BatDetect2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9699b-1e28-40ca-a5cf-701ff52a2fd2",
   "metadata": {},
   "source": [
    "4. How does this change if you change the score threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d40028-0b40-4b77-8754-bcbbef2bd1fe",
   "metadata": {},
   "source": [
    "Generally,\n",
    "\n",
    "* A higher score threshold will result in higher precision and lower recall.\n",
    "* A lower score threshold will result in lower precision and higher recall.\n",
    "\n",
    "Lets try out a range of score threshold values and plot the precision-recall for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ef8cf-7e3c-4da3-8929-99d9c7856914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to compute precision recall of batdetect2\n",
    "def compute_batdetect2_precision_recall(score):\n",
    "    # create list in which to store results\n",
    "    results = []\n",
    "\n",
    "    # select predictions with confidence score greater that score\n",
    "    confident_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.det_prob > score\n",
    "    ]\n",
    "\n",
    "    # iterate over each file in the dataset\n",
    "    for filename in yucatan[\"id\"]:\n",
    "        # compute positives, true_positives, false_positives and false_negatives for file\n",
    "        (\n",
    "            positives,\n",
    "            true_positives,\n",
    "            false_positives,\n",
    "            false_negatives,\n",
    "        ) = evaluate_detection.compute_detection_metrics(\n",
    "            filename,\n",
    "            confident_detections,\n",
    "            yucatan_annotations,\n",
    "            iou_threshold=0.5,  # note we are using a 50% IoU threshold\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"positives\": positives,\n",
    "                \"true_positives\": true_positives,\n",
    "                \"false_positives\": false_positives,\n",
    "                \"false_negatives\": false_negatives,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # convert results into dataframe\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    total_true_positives = results.true_positives.sum()\n",
    "    total_false_negatives = results.false_positives.sum()\n",
    "    total_positives = results.positives.sum()\n",
    "    total_detections = total_true_positives + total_false_negatives\n",
    "\n",
    "    # compute precision\n",
    "    precision = (\n",
    "        np.nan if total_detections == 0 else total_true_positives / total_detections\n",
    "    )\n",
    "\n",
    "    # compute recall\n",
    "    recall = np.nan if total_positives == 0 else total_true_positives / total_positives\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf7bf6-1798-46a9-aa2b-9e4b890d4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recalls = []\n",
    "\n",
    "# iterate over several score threshold values\n",
    "for score in tqdm(np.linspace(0.1, 0.9, 9)):\n",
    "    # compute precision recall for this score threshold value\n",
    "    precision, recall = compute_batdetect2_precision_recall(score)\n",
    "\n",
    "    # add to result list\n",
    "    precision_recalls.append({\"score\": score, \"precision\": precision, \"recall\": recall})\n",
    "\n",
    "# convert to dataframe\n",
    "precision_recalls = pd.DataFrame(precision_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a0a95-1f5a-4150-a1df-6c401df5abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=precision_recalls,\n",
    "    x=\"precision\",\n",
    "    y=\"recall\",\n",
    "    color=\"black\",\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=precision_recalls,\n",
    "    x=\"precision\",\n",
    "    y=\"recall\",\n",
    "    hue=\"score\",\n",
    "    s=200,\n",
    ")\n",
    "# set x and y axes limits to 0-1\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99346cc8-4bf3-4c24-a409-580b1deb4c80",
   "metadata": {},
   "source": [
    "## Part 2: Identifying Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f6e1-6464-47a9-9814-4883ff6ae9a5",
   "metadata": {},
   "source": [
    "- In the previous section we saw how to detect sounds in a recording. But we still need to identify the species that produced the sound.\n",
    "\n",
    "- Generally, classification is more challenging than detection, as the sounds produced by different species can be very similar (**interspecific overlap**).\n",
    "\n",
    "- A single species can have flexible vocalisations, think humans or mimic birds such as starling (**intraspecific variation**).\n",
    "\n",
    "- Bioacoustic data presents similar challenges to the camera trap datasets as recordings can be:\n",
    "    - **Ocluded** (Simultaneous sounds)\n",
    "    - **Appear in varying ambient condition**s (rain/wind/thunder)\n",
    "    - **Partial** (Only captured half the sound)\n",
    "    - **Noisy** (Saturation and faulty sensor)\n",
    "    - **Quiet or very loud** (depending on animal size, distance, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264ce12-0a2a-418d-90b5-36bbd9d0f623",
   "metadata": {},
   "source": [
    "For the rest of this notebook we will focus on **10** bat species present in the Yucat√°n dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19b242-1165-426b-aac0-d20ca9bab747",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES = [\n",
    "    \"Mormoops megalophylla\",\n",
    "    \"Myotis keaysi\",\n",
    "    \"Saccopteryx bilineata\",\n",
    "    \"Pteronotus davyi\",\n",
    "    \"Pteronotus parnellii\",\n",
    "    \"Lasiurus ega\",\n",
    "    \"Pteropteryx macrotis\",\n",
    "    \"Eumops underwoodi\",\n",
    "    \"Rhogeessa aeneus\",\n",
    "    \"Eptesicus furinalis\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bd285-7130-41f0-8c1e-12ae0f1892ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = yucatan_annotations[yucatan_annotations[\"class\"].isin(SPECIES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f472b-f647-414e-a44b-c88fda2feff0",
   "metadata": {},
   "source": [
    "### Bat call features\n",
    "\n",
    "- Previous research on bat call identification was based on hand-crafted features of the bat calls.\n",
    "\n",
    "- Measuring call features used to be a manual process.\n",
    "\n",
    "<img src=\"https://www.elekon.ch/batexplorer2/doc/_images/CallParams.png\" alt=\"call parameters\" width=\"400\"/>\n",
    "\n",
    "> Image taken from the [BatExplorer 2.1 user guide](https://www.elekon.ch/batexplorer2/doc/batcall_params.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4c318-e095-4eb5-866a-10cefd13b476",
   "metadata": {},
   "source": [
    "**Peak frequency [kHz]:**\n",
    "\n",
    ">    The frequency at which the call is loudest (peak in the spectrum display), aka frequency of maximum energy (FME) or main frequency.\n",
    ">    Most important parameter for bat classification because it can easily be measured and is often typical for a certain species or group of species.\n",
    ">    The standard deviation of the peak frequency allows the detection of alternating calling species.\n",
    "    \n",
    "**Max frequency [kHz]**\n",
    "\n",
    ">    The maximum frequency of the call. Often this is equal to the start frequency, for Rhinolophidae typically equal to the peak frequency.\n",
    "    \n",
    "**Min frequency [kHz]**\n",
    "\n",
    ">    The minimum frequency of the call. Often this is equal to the end frequency, for hockey stick calls (e.g. Pipistrelle) it might be lower than the end frequency.\n",
    "    \n",
    "**BW Peak2Min [kHz]**\n",
    "\n",
    ">    Bandwidth Peak2Min = Peak frequency - Min frequency\n",
    ">    Often used to distinguish Myotis and Pipistrelle calls, Myotis mostly have higher bandwidth.\n",
    "    \n",
    "**Call length [ms]**\n",
    "\n",
    ">    Time period of call start to call end in ms. Can be measured most accurately in the oscillogram (wave rise to wave drop).\n",
    ">    Search calls from European bats are usually between one and up to approximately 30 ms (horseshoe bats up to 80 ms).\n",
    "    \n",
    "**Call distance [ms]**\n",
    "\n",
    ">    Time period between two consecutive calls in ms. Can be measured most accurately in the oscillogram (wave rise call A to wave rise call B).\n",
    ">    Often this parameter is not very significant since most bat species have irregular rhythms. But it can be an indicator for behavior.\n",
    ">    Search calls from European bats usually have distances of about 30 to 300 ms, sometimes even longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952d398-e237-4ad2-854c-a2927a5e762f",
   "metadata": {},
   "source": [
    "### Exercise üê¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c2358-71af-436e-8ed0-6d3fcd0ec5e0",
   "metadata": {},
   "source": [
    "* Explore 1 call per species.\n",
    "* Measure peak frequency, max frequency, min frequency and call length.\n",
    "* Store the measurements in a pandas dataframe.\n",
    "* Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3378d03-31bb-4579-8653-241cecf60b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_annotate = classification_df.groupby(\"class\").sample(n=1)\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    index=[\n",
    "        (f\"{r['class']}_{index}\", i)\n",
    "        for i, (index, r) in enumerate(to_annotate.iterrows())\n",
    "    ]\n",
    ")\n",
    "def manually_extract_features_from_spectrogram(index=0):\n",
    "    row = to_annotate.iloc[index]\n",
    "    return plotting.plot_spectrogram_with_plotly(\n",
    "        path=os.path.join(YUCATAN_AUDIO_DIR, row[\"recording_id\"]),\n",
    "        start_time=row[\"start_time\"],\n",
    "        end_time=row[\"end_time\"],\n",
    "        low_freq=row[\"low_freq\"],\n",
    "        high_freq=row[\"high_freq\"],\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbd3e1-5d8f-45bb-a744-a198e97425a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of measurements of bat call features.\n",
    "# probably will be different to yours as random calls are selected.\n",
    "measurements = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"species\": \"Saccopteryx bilineata\",\n",
    "            \"start_time\": 0.0438,\n",
    "            \"end_time\": 0.0517,\n",
    "            \"peak_freq\": 46511,\n",
    "            \"low_freq\": 43066,\n",
    "            \"high_freq\": 48234,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Rhogeessa aeneus\",\n",
    "            \"start_time\": 0.0447,\n",
    "            \"end_time\": 0.0508,\n",
    "            \"peak_freq\": 46511,\n",
    "            \"low_freq\": 43066,\n",
    "            \"high_freq\": 65460,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Pteropteryx macrotis\",\n",
    "            \"start_time\": 0.04505,\n",
    "            \"end_time\": 0.0502,\n",
    "            \"peak_freq\": 41343,\n",
    "            \"low_freq\": 39621,\n",
    "            \"high_freq\": 42205,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Pteronotus parnellii\",\n",
    "            \"start_time\": 0.0401,\n",
    "            \"end_time\": 0.05523,\n",
    "            \"peak_freq\": 63738,\n",
    "            \"low_freq\": 62000,\n",
    "            \"high_freq\": 64599,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Pteronotus davyi\",\n",
    "            \"start_time\": 0.0459,\n",
    "            \"end_time\": 0.0523,\n",
    "            \"peak_freq\": 65460,\n",
    "            \"low_freq\": 55125,\n",
    "            \"high_freq\": 69767,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Myotis keaysi\",\n",
    "            \"start_time\": 0.0468,\n",
    "            \"end_time\": 0.0497,\n",
    "            \"peak_freq\": 61154,\n",
    "            \"low_freq\": 60292,\n",
    "            \"high_freq\": 85271,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Mormoops megalophylla\",\n",
    "            \"start_time\": 0.043,\n",
    "            \"end_time\": 0.05,\n",
    "            \"peak_freq\": 52541,\n",
    "            \"low_freq\": 48234,\n",
    "            \"high_freq\": 53402,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Eptesicus furinalis\",\n",
    "            \"start_time\": 0.04476,\n",
    "            \"end_time\": 0.05,\n",
    "            \"peak_freq\": 34453,\n",
    "            \"low_freq\": 33591,\n",
    "            \"high_freq\": 39621,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Eumops underwoodi\",\n",
    "            \"start_time\": 0.0427,\n",
    "            \"end_time\": 0.0537,\n",
    "            \"peak_freq\": 27562,\n",
    "            \"low_freq\": 24978,\n",
    "            \"high_freq\": 31007,\n",
    "        },\n",
    "        {\n",
    "            \"species\": \"Lasiurus ega\",\n",
    "            \"start_time\": 0.0412,\n",
    "            \"end_time\": 0.0566,\n",
    "            \"peak_freq\": 27562,\n",
    "            \"low_freq\": 24978,\n",
    "            \"high_freq\": 31869,\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fd737-0f70-4674-8f9c-04d40b6323d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create call length column\n",
    "measurements[\"call_length\"] = measurements[\"end_time\"] - measurements[\"start_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b09b1b-97b2-48eb-b528-b968df17f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatterplot of call_length vs peak frequency\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=measurements,\n",
    "    x=\"call_length\",\n",
    "    y=\"peak_freq\",\n",
    "    hue=\"species\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09917246-4e51-4d27-b5cb-099c39170a40",
   "metadata": {},
   "source": [
    "### Tadarida automated features\n",
    "\n",
    "- Tadarida extracts a large set of features from each detected sound event.\n",
    "\n",
    "- It is possible to build a pipeline for automated species identification using of automatic feature extraction process.\n",
    "\n",
    "- First we detect the sounds in the recording, then we extract the features, and\n",
    "  finally we classify the sounds.\n",
    "\n",
    "- The classification is done using the extracted features a classifier\n",
    "  algorithm, like Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d0492-0505-48c0-879d-e9df6b6d20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load tadarida extracted features\n",
    "tadarida_features = pd.read_csv(DATA_DIR / \"yucatan\" / \"yucatan_tadarida_features.csv\")\n",
    "\n",
    "# the dataframe contains the class of each detected pulse and\n",
    "# a suite of automatically extracted parameters\n",
    "species = tadarida_features[\"Class\"]\n",
    "\n",
    "print(f\"Number of features = {len(TADARIDA_FEATURES)}\")\n",
    "features = tadarida_features[TADARIDA_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d51a5-e25a-49d5-99d6-ef7367f19c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will split the data into train a test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    species,\n",
    "    test_size=0.3,\n",
    "    stratify=species,  # notice we are using the stratified argument\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181dd8d-961f-4ce5-b9d8-79dc858ca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the number of examples per species in each dataset\n",
    "# Train dataset\n",
    "y_train.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876d4a5-b3e5-4441-95a7-7c1e0c0c45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "y_test.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0dc8c-f141-4df3-86fe-a45052dc2834",
   "metadata": {},
   "source": [
    "Now we are ready to train a classifier to identify the species of the bat calls.\n",
    "\n",
    "We will use a **Random Forest Classifier** from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd5da9-dd8f-4127-81c4-a1d02527dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# fit to training data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71016c0-4a23-4912-80dd-b3f94f953ce2",
   "metadata": {},
   "source": [
    "We can evaluate the performance of the classifier in a similar fashion to previous practicals.\n",
    "\n",
    "There is a useful `classification_report` function from `sklearn.metrics` to compute the precision, recall and F1 score for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7525c-5668-46e7-b371-52a9fef64818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with rf model on evaluation data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917af10-d7b1-4057-b6f1-7f7569a0107d",
   "metadata": {},
   "source": [
    "### Exercise ü¶ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e4a9a-0493-45ad-82ff-580a2efe5fc4",
   "metadata": {},
   "source": [
    "In this exercise you will analyze the model's performance in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbe60c-afff-46ee-871c-7f70f9efdd03",
   "metadata": {},
   "source": [
    "1. Plot the confusion matrix using the `sklearn.metrics.ConfusionMatrixDisplay` from scikit learn.\n",
    "\n",
    "If you are confused it is always helpful to look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) and provided examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21318c-89f7-45ee-8151-1a2313feae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "# plot the confusion matrix using the ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay(cm, display_labels=rf_model.classes_).plot(\n",
    "    xticks_rotation=\"vertical\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0c81d-b4ba-4203-8f90-58ea8c481513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will compute the confusion matrix again, but normalize each row\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize=\"true\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# plot the confusion matrix using the ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay(cm, display_labels=rf_model.classes_).plot(\n",
    "    xticks_rotation=\"vertical\",\n",
    "    values_format=\".0%\",\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad54dc-ec28-4b10-bde1-2afad7899b5b",
   "metadata": {},
   "source": [
    "2. Which are the worst performing species? Can you see why? You might need to go back and see some more examples of bat calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d490a-a7b6-4d8c-a6f6-08f4a9676e19",
   "metadata": {},
   "source": [
    "**Caveat**: Answers are likely to be different each run, as dataset splits and random forest training are random.\n",
    "\n",
    "* *Lasiurus intermedius* and *Eptesicus furinalis* were the worst performing species.\n",
    "\n",
    "This could be explained by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07663476-5a11-4dc0-9129-3fa8dc2336e5",
   "metadata": {},
   "source": [
    "1. Both species have few training examples:\n",
    "\n",
    "    a. The training examples might not cover the whole range of call variation. Test calls could contain call types not seen during training. This could be fixed by collecting more data, or generating synthetic data.\n",
    "    \n",
    "    b. In general in unbalanced datasets the classes with least amount of examples will suffer, as the training algorithms bias model parameters to benefit the most frequent classes. This could be fixed by subsampling overrepresented classes or oversampling underrepresented classes.\n",
    "    \n",
    "    c. Each species has around 70-80 calls in the training dataset. However, we are using a set of 143 features, meaning that any model with sufficient flexibility can fully overfit to the training data. This could be fixed by constraining model flexiblity (modify RF params, or select another model type) or using less features.\n",
    "    \n",
    "2. There is substantial overlap between echolocation calls of different species:\n",
    "\n",
    "    a. This could be due to errors in labelling. Errors in the annotation process are common and should be expected. This could be fixed by conducting a review process in which suspicious annotations are verified or corrected. An annotation can be flagged as suspicious if its features lies far from the usual distribution (outlier detection), or if the model is confidently wrong about its class (hard example mining).\n",
    "    \n",
    "    b. Bat echolocation calls are are used to navigate and detect pray. If habitual flying space and/or prey are similar for two species they will tend to echolocate in a similar fashion. When this is the case using a different classification scheme for bats, such as one based on ecological traits, might be better suited for the task of acoustic identification.\n",
    "    \n",
    "    c. Sound production is also constrained by morphology. Closely related species will tend to have similar sound production capabilities and thus similar echolocation calls. However, this is not always the case as echolocation call is heavily constrained by ecology (as mentioned in the previous point). A potential workaround is to classify to a higher taxonomic level such as genus or family.\n",
    "    \n",
    "    d. Selected call features cannot discern between species. It could be the case that there are subtle cues in the audio that aid identification but haven't been measured. In this case either more features should be designed and measured, or adopting an approach of automated feature extraction (such as deep learning approaches) could help find a better feature set for acoustic identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104b73-b72a-414e-b6cc-a3536251b103",
   "metadata": {},
   "source": [
    "Here we will illustrate both points empirically with some plots: \n",
    "\n",
    "1. Plot the relation between the precision and recall of a species and the number of training examples.\n",
    "\n",
    "2. Plot measured features per species to study overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ba431-3b18-4e50-849c-565ca8c93b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classification report and convert it to a pandas dataframe\n",
    "report = (\n",
    "    pd.DataFrame(\n",
    "        classification_report(\n",
    "            y_true=y_test,\n",
    "            y_pred=y_pred,\n",
    "            output_dict=True,\n",
    "        )\n",
    "    )\n",
    "    .drop(  # remove unwanted columns\n",
    "        columns=[\n",
    "            \"accuracy\",\n",
    "            \"macro avg\",\n",
    "            \"weighted avg\",\n",
    "        ]\n",
    "    )\n",
    "    .T.rename(columns={\"support\": \"test_samples\"})\n",
    "    .astype({\"test_samples\": int})\n",
    ")\n",
    "\n",
    "# compute the number of samples per species\n",
    "train_samples = y_train.value_counts()\n",
    "\n",
    "# add column with number of train samples\n",
    "report[\"train_samples\"] = train_samples\n",
    "\n",
    "report = report.reset_index(names=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ebca8-edf8-43ee-aa6b-d88f3f3c6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the report table\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b50048-7b74-4358-a7f2-ce8d725ba7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision vs recall, use number of train samples to set marker size\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    data=report,\n",
    "    x=\"precision\",\n",
    "    y=\"recall\",\n",
    "    style=\"species\",\n",
    "    hue=\"train_samples\",\n",
    "    size=\"train_samples\",\n",
    "    sizes=(20, 400),\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "# set x and y limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527e725-3bbd-4bda-b542-8a9b950a4901",
   "metadata": {},
   "source": [
    "To plot overlap between call features we will:\n",
    "\n",
    "1. Project the call features into 3-dimension using UMAP. We will only use the test data.\n",
    "2. Create an interactive 3D-scatterplot with the projected features. Each point will be colored and styled depending on the species.\n",
    "\n",
    "By clicking on a species in the plot legend you can toggle its visibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3461907-7b56-4d37-b0b5-436c75921b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize each column in the dataset. Standardized is mean=0 variance=1.\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Project into three dimensional space with UMAP\n",
    "X, Y, Z = UMAP(n_components=3).fit_transform(X_test_scaled).T\n",
    "\n",
    "# Use plotly express to create interactive 3D scatterplot\n",
    "px.scatter_3d(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    z=Z,\n",
    "    color=y_test,\n",
    "    symbol=y_test,\n",
    "    height=600,\n",
    "    opacity=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0215c-b423-4fa8-93f2-969624bfdb03",
   "metadata": {},
   "source": [
    "### Universal feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6be64-1e56-440e-939f-ad470e249807",
   "metadata": {},
   "source": [
    "- Is Tadarida's feature set optimal for bat call identification?\n",
    "\n",
    "- There is a huge amount of information in the recorded audio that we are throwing away by using hand-crafted features.\n",
    "\n",
    "- Can we learn a better feature set from the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b535bf6-9d97-4553-bcb7-6fee66366974",
   "metadata": {},
   "source": [
    "Here instead of relying on hand-crafted features we will use acoustic feature extractor called [Yamnet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet).\n",
    "\n",
    "Yamnet was trained on [AudioSet](http://research.google.com/audioset/) a massive dataset of YouTube recordings with more than 5.8 thousand hours of audio.\n",
    "\n",
    "It was trained to classify sounds clips into 527 different classes. The features it learned to extract are thus useful to distinguish and identify a large variety of sounds.\n",
    "\n",
    "<img src=\"http://research.google.com/audioset/resources/histogram.svg\" alt=\"audioset dataset\" width=\"400\"/>\n",
    "\n",
    "Audioset does not contain ultrasonic recordings, and thus is devoid of bat sounds. However, we expect the learnt features to be sufficiently general that it can help identify bat calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68854c3-ef99-4719-9bb4-0774f36226b4",
   "metadata": {},
   "source": [
    "First we need to download the model. Thankfully the model is available in [Tensorflow Hub](https://tfhub.dev/), a repository of pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12eb3bd-88f6-41b2-8df2-1f0d204ef26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will load the model using tensorflow_hub\n",
    "yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f54f0-d0d4-48d8-abf6-b6d16dddd5be",
   "metadata": {},
   "source": [
    "Next we will load all the audio bits from the dataset and extract the features using the model.\n",
    "\n",
    "The yamnet was originally trained with 0.96 second audio clips sampled at 16kHz. \n",
    "\n",
    "All our recordings have a samplerate of 441kHz so we will only feed the model with 34ms of audio at a time. \n",
    "\n",
    "We will center each audio clip on the bat calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81531304-22c4-4d01-b69b-ebe2b5e386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will load the information of where each bat call starts and ends\n",
    "audio_clips = pd.read_csv(DATA_DIR / \"yucatan\" / \"yucatan_species_clips.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2797d-b5a7-4ef9-a533-c816db9e30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for _, row in audio_clips.iterrows():\n",
    "    wav = load_bat_call_audio_data(\n",
    "        str(YUCATAN_AUDIO_DIR / row.recording_id),\n",
    "        row.start_time,\n",
    "        row.end_time,\n",
    "    )\n",
    "    audios.append(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871a436-a5cd-442a-a8c4-1284e00b27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_features = []\n",
    "for wav in tqdm(audios):\n",
    "    scores, feats, spectrogram = yamnet(wav)\n",
    "    yamnet_features.append(feats.numpy().squeeze())\n",
    "yamnet_features = np.array(yamnet_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f028ad-35a5-4fae-93e4-95fccf6b7a10",
   "metadata": {},
   "source": [
    "Notice the yamnet model returns a set of 1024 features for each audio bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da7085-44fb-4121-a93f-19c6101ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yamnet_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8300d-2d82-4c64-b61d-000d263761dc",
   "metadata": {},
   "source": [
    "We can visualize the features using the dimensionality reduction technique\n",
    "explore in the previous practicals, [UMAP](https://umap-learn.readthedocs.io/en/latest/).\n",
    "\n",
    "We will compare tadarida's features with the features extracted by yamnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320ce59-1422-4bdf-bc72-374661bb81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_yamnet = UMAP().fit_transform(StandardScaler().fit_transform(yamnet_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09809281-e4f1-40ee-b145-cc6c1a1b1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = umap_yamnet.T\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X, y=Y, hue=audio_clips[\"class\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc29ee-3327-4726-9c0e-abff268f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tadarida = tadarida_features[TADARIDA_FEATURES]\n",
    "y_tadarida = tadarida_features[\"Class\"]\n",
    "\n",
    "umap_tadarida = UMAP().fit_transform(StandardScaler().fit_transform(X_tadarida))\n",
    "X, Y = umap_tadarida.T\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X, y=Y, hue=y_tadarida);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d98d8c-167d-4fbc-b805-fcf0358d0bd1",
   "metadata": {},
   "source": [
    "Its hard to evaluate between the two feature sets by just these plots. In the next exercise we will train a classifier using the features extracted by Yamnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ae5a-a6ff-4b9a-bbaa-596b2244e662",
   "metadata": {},
   "source": [
    "### Exercise üêù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9478963-f2ec-4267-a709-34ce901755c8",
   "metadata": {},
   "source": [
    "Here you will build a random forest classifier using the features extracted by Yamnet.\n",
    "\n",
    "1. Split the data into train and test. You can copy the code from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd845f-4273-455e-8750-a527ea165a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "X_train_yamnet, X_test_yamnet, y_train_yamnet, y_test_yamnet = train_test_split(\n",
    "    yamnet_features,\n",
    "    audio_clips[\"class\"],\n",
    "    test_size=0.3,\n",
    "    stratify=audio_clips[\"class\"],  # notice we are using the stratified argument\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9015661-4741-4cbb-ad75-833aa854c38f",
   "metadata": {},
   "source": [
    "2. Train a random forest classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772135be-f13b-4648-b30f-dcc9236b6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random forest model\n",
    "rf_model_2 = RandomForestClassifier()\n",
    "\n",
    "# fit to training data\n",
    "rf_model_2.fit(X_train_yamnet, y_train_yamnet);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726891e-b690-40cd-8768-03e5d2b4c58a",
   "metadata": {},
   "source": [
    "3. Evaluate the performance using the classification_report function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0cf78-5157-4ee7-acee-c90c01910a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred_yamnet = rf_model_2.predict(X_test_yamnet)\n",
    "\n",
    "# evaluate using the classification_report\n",
    "print(classification_report(y_true=y_test_yamnet, y_pred=y_pred_yamnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003b3db-2765-45d1-a1fd-3776be5b2251",
   "metadata": {},
   "source": [
    "4. Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18315b8d-ad24-440f-93d1-402d29769edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will compute the confusion matrix again, but normalize each row\n",
    "cm = confusion_matrix(y_true=y_test_yamnet, y_pred=y_pred_yamnet, normalize=\"true\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# plot the confusion matrix using the ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay(cm, display_labels=rf_model_2.classes_).plot(\n",
    "    xticks_rotation=\"vertical\",\n",
    "    values_format=\".0%\",\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5b12a-749e-4b23-8338-39840ad61f93",
   "metadata": {},
   "source": [
    "5. Was this better? What are the advantages and disadvantages of using a pre-trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56147968-fa0f-4b39-bbed-1c3f8bd0df76",
   "metadata": {},
   "source": [
    "**No**, model performance dropped for every species.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. No feature selection stage.\n",
    "2. If the dataset on which it was trained closely resembles the target dataset then it will provide a substantial performance boost (as seen in the previous Lab).\n",
    "3. Can be finetuned to current dataset.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Not necessarily better than a specialised approach. Tadarida features were extracted from several decades of acoustic research on bat echolocation. \n",
    "2. Features are **very** hard to interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci",
   "language": "python",
   "name": "sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
