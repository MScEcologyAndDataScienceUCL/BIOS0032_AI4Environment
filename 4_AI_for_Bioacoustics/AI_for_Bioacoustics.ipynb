{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d1c2e1-3a0d-46f1-bb8f-c47b6b23a06b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/4_AI_for_Bioacoustics/AI_for_Bioacoustics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519b66b-7870-4c8e-8ba7-7ceb7814d015",
   "metadata": {},
   "source": [
    "# Week 4 - AI for Bioacoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1c4ab-0475-4ecd-98fe-457c3e4a30fa",
   "metadata": {},
   "source": [
    "## What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf4cc-a14c-437d-8c99-d24edfad0d78",
   "metadata": {},
   "source": [
    "In this weeks practical we will explore computer audition applications in ecology such as automated animal detection and species classification from audio sensor data. Among other stuff we will:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4acde-b379-4a49-93c2-38a8095be264",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a625520-aa73-4a25-894c-d6dc993f9166",
   "metadata": {},
   "source": [
    "### Computer audition\n",
    "\n",
    "**What is Computer audition?**\n",
    "\n",
    "- Computer audition is the field of research that deals with the automatic\n",
    "  analysis of audio signals.\n",
    "\n",
    "- It intersects with many other fields, including machine learning, signal\n",
    "  processing, and computer vision.\n",
    "\n",
    "**What does a computer hear**?\n",
    "\n",
    "- Audio files are a sequence of numbers representing the amplitude of the sound\n",
    "  wave at a given time.\n",
    "\n",
    "- The number of samples per second is called the **sampling rate**.\n",
    "\n",
    "<img alt=\"audio and sampling rate\" width=\"600\" src=\"https://cdn.shopify.com/s/files/1/1169/2482/files/Sampling_Rate_Cover_image.jpg?v=1654170259\"></img>\n",
    "\n",
    "**What tasks can we do with computer audition?**\n",
    "\n",
    "Computer audition is used in a wide range of applications, including:\n",
    "\n",
    "- Speech recognition: Siri, Alexa, Google Assistant\n",
    "- Music information retrieval: Spotify, Shazam\n",
    "- Audio classification: What is sounding in this audio?\n",
    "- Sound event detection: Transcription of audio into a sequence of events.\n",
    "\n",
    "<img alt=\"Sound event detection\" width=\"400\" src=\"http://d33wubrfki0l68.cloudfront.net/508a62f305652e6d9af853c65ab33ae9900ff38e/17a88/images/tasks/challenge2016/task3_overview.png\"></img>\n",
    "\n",
    "> Taken from the paper: Mesaros, A., Heittola, T., Diment, A., Elizalde, B., Shah, A., Vincent, E., ... & Virtanen, T. (2017, November). DCASE 2017 challenge setup: Tasks, datasets and baseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events.\n",
    "\n",
    "Recently neural network models have taken over the field of computer audition and are being used to solve many of the above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56935d83-a08f-4a2f-9ba6-45a3b3660833",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "\n",
    "**Acoustic sensors** can be used to collect field recordings of animal sounds.\n",
    "\n",
    "Usually, these sensors are deployed statically in the field for a long periods of time and record sounds continuously. This is called **passive acoustic monitoring**.\n",
    "\n",
    "<img alt=\"passive acoustic monitoring\" width=\"400\" src=\"https://wittmann-tours.de/wp-content/uploads/2018/06/AudioMoth.jpg\"></img>\n",
    "\n",
    "Alternatively, recordings are actively directed towards a specific animal species or sound events.\n",
    "\n",
    "<img alt=\"active recording\" width=\"400\" src=\"https://s3.amazonaws.com/cdn.freshdesk.com/data/helpdesk/attachments/production/48032687175/original/xjI7Dy3Q9kaCZinr5vf4ksNxQbjK13Yv3A.jpg?1584552543\"></img>\n",
    "\n",
    "> Taken from the Macaulay Library blog post:\n",
    "> [Sound recording tips](https://support.ebird.org/en/support/solutions/articles/48001064298-sound-recording-tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef333a71-9414-4127-b196-76f37f5f4315",
   "metadata": {},
   "source": [
    "### Acoustics for ecology\n",
    "\n",
    "The sound at a site is a reflection of the species present in the area and other\n",
    "environmental factors.\n",
    "\n",
    "<img alt=\"composition of acoustic space\" width=\"400\" src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs12304-017-9288-5/MediaObjects/12304_2017_9288_Fig1_HTML.gif?as=webp\"></img>\n",
    "\n",
    "> Taken from the paper: Mullet, T.C., Farina, A. & Gage, S.H. The Acoustic\n",
    "> Habitat Hypothesis: An Ecoacoustics Perspective on Species Habitat Selection.\n",
    "> Biosemiotics 10, 319‚Äì336 (2017). https://doi.org/10.1007/s12304-017-9288-5\n",
    "\n",
    "If we could link the sounds to the species, we could use this information to\n",
    "study and monitor the biodiversity of an area.\n",
    "\n",
    "Acoustic sensors produce a lot of data, and it is not always easy to analyse.\n",
    "Can we use computer audition to help us?\n",
    "\n",
    "In this practical we will explore the task of **animal sound detection** and\n",
    "**species classification**, using both manual and automated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386638-3d9b-4e64-8c38-58f86f6946ad",
   "metadata": {},
   "source": [
    "## Setup Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0c85-f597-4e48-ba23-dc19fe24996a",
   "metadata": {},
   "source": [
    "Here we will go through the steps to setup the environment for this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ec3ff-fd0e-466b-af3a-a5b549227de5",
   "metadata": {},
   "source": [
    "1. Make sure to use GPU runtime in Colab. Go to `Runtime` -> `Change runtime\n",
    "   type` and select `GPU` as the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ceeb1-d929-4570-98d5-d42bc7468552",
   "metadata": {},
   "source": [
    "2. Mount your Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c1656-f864-414a-99ad-7e963cb2d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c907e-d1d2-47ae-8786-dbe2ab2fcaa5",
   "metadata": {},
   "source": [
    "Add a shortcut in you drive to this [shared folder](https://drive.google.com/drive/folders/1hbbbsILNBsQghktuj0z_Jq_3iEZQCCbj?usp=share_link).\n",
    "\n",
    "This will allow you to access the data we will use in this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aba172-13cd-4962-8465-40ce5d134346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Extract data into machine\n",
    "!unzip /content/drive/MyDrive/BIO0032_AI4Environment/week4_data.zip -d /content/week4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b1804-eda7-4ea2-9ba7-72e15404950d",
   "metadata": {},
   "source": [
    "3. Install and import dependencies. Run the following cell to install the required\n",
    "   dependencies. This will take a few minutes. You can omit\n",
    "   the outputs of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cafa5-4655-48f2-b7ef-621fc148425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get install libfftw3-dev libicu-dev libsndfile1-dev libqt5core5a\n",
    "!pip install pytadarida git+https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment.git git+https://github.com/mbsantiago/batdetect2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a40ea-60a5-4cbb-9f4d-a586c55dd98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import perf_counter\n",
    "\n",
    "import ipywidgets\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import plotly.express as px\n",
    "import xarray as xa\n",
    "from IPython.display import Audio\n",
    "from librosa import display\n",
    "\n",
    "from bios0032utils.bioacoustics import detection, evaluate_detection, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0aaa9-819e-4ed3-a6f7-092b445427e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Detecting Animal Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33734ab6-584a-4827-bf6c-7976b3168c80",
   "metadata": {},
   "source": [
    "- Most of the time, we are not interested in all the sounds in a recording, but only in the sounds of a specific animal species.\n",
    "\n",
    "- Acoustic sensors will indiscriminately record all sounds in the environment, including those of animals, wind, rain, etc. Although some recorders can be triggered by a specific sound, this is not always the case.\n",
    "\n",
    "- Passive acoustic monitoring produces many hours of recordings, and it's hard to identify and explore the sounds of interest.\n",
    "\n",
    "These are similar problems to the ones we have seen in the previous practicals for camera trap images.\n",
    "\n",
    "While not as developed as in **computer vision**, there are some tools for automatically **detecting animal sounds** in recordings. Here we will explore a few of them.\n",
    "\n",
    "But first we need to understand how to visualise and annotate sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6d8ce-4bbc-4e71-b29c-a6b7100d5544",
   "metadata": {},
   "source": [
    "### Animal sounds visualisation\n",
    "\n",
    "- While we can listen to the sounds in a recording, it is often easier to\n",
    "visualise them.\n",
    "\n",
    "- This is especially true when we want to compare sounds from\n",
    "different recordings or navigate quickly without the need to listen.\n",
    "\n",
    "- We can use waveplots and spectrograms to visualise the sounds in a recording.\n",
    "\n",
    "Now we will load a dataset of animal recordings provided by [Avisoft](https://www.avisoft.com/animal-sounds/) and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f7a88-6c4d-4a4e-82c8-94d14703cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVISOFT_AUDIO_DIR = \"/content/week4_data/data/avisoft/audio\"\n",
    "AVISOFT_METADATA_FILE = \"/content/week4_data/data/avisoft/avisoft_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eac016-30de-49b7-a1bc-3faa987b7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata dataframe\n",
    "avisoft = pd.read_csv(AVISOFT_METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc08b3-6394-493f-9f67-28740719d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few rows\n",
    "avisoft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5db25f-fa7d-4049-a37c-fa34692737b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random file from the dataset\n",
    "random_recording = avisoft.sample(n=1).iloc[0]\n",
    "\n",
    "# read the audio file and import it as a numpy array\n",
    "wav, samplerate = librosa.load(\n",
    "    os.path.join(AVISOFT_AUDIO_DIR, random_recording.wav),\n",
    "    sr=None,\n",
    ")\n",
    "\n",
    "# Compute the duration of audio\n",
    "num_samples = len(wav)  # Number of samples taken by the recorder\n",
    "duration = num_samples / samplerate\n",
    "\n",
    "# Get name of animal\n",
    "animal_name = random_recording.english_name\n",
    "\n",
    "print(f\"File selected = {random_recording.wav}\")\n",
    "print(f\"Samplerate = {samplerate} Hz\")\n",
    "print(f\"Duration = {duration:.2f} s\")\n",
    "print(f\"Species = {animal_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a67a0-10bd-4400-bef5-5a260bf978c3",
   "metadata": {},
   "source": [
    "Lets first listen to the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf5b52-f1e8-4085-81e4-5a547752a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=wav, rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633c8a3-6948-4497-a4bf-a4c319bd4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of the waveform\n",
    "times = np.linspace(0, duration, num_samples)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(times, wav)\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.title(f\"Waveform of {animal_name} sound\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eead3-d1d3-41ee-8bc3-970ed31fe704",
   "metadata": {},
   "source": [
    "The **waveform** gives us a visual representation of the sound amplitude over time.\n",
    "\n",
    "However, ff there are multiple simultaneous sounds in the recording, it can be hard to see each individual sound. \n",
    "\n",
    "We can use a **spectrogram** to decompose the sound into **frequencies** and visualise them as a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650df605-e01b-48aa-a436-729aaad2f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram with the short time fourier transform (STFT)\n",
    "spectrogram = np.abs(librosa.stft(wav))\n",
    "\n",
    "# Amplitude is best represented in logarithmic scale (decibels)\n",
    "db_spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def0c4b-b15d-4c7a-bf8e-61eb226b91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of spectrogram\n",
    "num_freq_bins, num_time_bins = db_spectrogram.shape\n",
    "times = np.linspace(0, duration, num_time_bins)\n",
    "freqs = np.linspace(0, samplerate / 2, num_freq_bins)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(times, freqs, db_spectrogram, cmap=\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(f\"Spectrogram of {random_recording.english_name} sound\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"freq (Hz)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbde3f-289e-4097-aa94-9abd336bb6e1",
   "metadata": {},
   "source": [
    "### Exercise üîä üëÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72092175-0c8c-440d-82b5-a6d5c75bcb86",
   "metadata": {},
   "source": [
    "The sounds produced by animals can be very different from each other. The\n",
    "transformation used to create the spectrogram, called the **short-time Fourier\n",
    "transform** (STFT), will highlight different features of the sound depending on\n",
    "the parameters used.\n",
    "\n",
    "- Research what the STFT is and how its parameters affect the spectrogram. In\n",
    "  particular, try to understand the effect of the **window size** and the **hop\n",
    "  size** or **overlap**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dad7c-f01f-4c9c-aad4-4021450b9d93",
   "metadata": {},
   "source": [
    "Here you can visualise sounds from different species and see how the STFT\n",
    "parameters affect the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ace48-5613-4dd0-a28d-94f2d5e258d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Interactive spectrogram of animal sounds\n",
    "\n",
    "# @markdown Select the file you wish to visualize. Modify the spectrogram parameters to see its effect on the spectrogram. Change the reproduction speed for interesting effects!\n",
    "\n",
    "# Select some varied sounds from avisoft dataset\n",
    "examples = [\n",
    "    (row.english_name, os.path.join(AVISOFT_AUDIO_DIR, row.wav))\n",
    "    # select one random recording per taxonomic group\n",
    "    for row in avisoft.groupby(\"order\").sample(n=1).itertuples()\n",
    "]\n",
    "\n",
    "# Create interactive plot\n",
    "ipywidgets.interact(\n",
    "    plotting.plot_waveform_with_spectrogram,\n",
    "    hop_length=(32, 1024, 32),\n",
    "    n_fft=(32, 2048, 32),\n",
    "    window=plotting.WINDOW_OPTIONS,\n",
    "    file=examples,\n",
    "    cmap=plotting.COLORMAPS,\n",
    "    speed=[\n",
    "        (\"x1\", 1),\n",
    "        (\"x1.5\", 1.5),\n",
    "        (\"x2\", 2),\n",
    "        (\"x0.5\", 0.5),\n",
    "        (\"x0.2\", 0.2),\n",
    "        (\"x0.1\", 0.1),\n",
    "    ],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228393ff-2f15-4a52-a068-f9b01f20dcd3",
   "metadata": {},
   "source": [
    "- Try changing the parameters and see how they affect sounds from different\n",
    "  species.\n",
    "\n",
    "- Can you see that some choice of parameters are good for some species but\n",
    "  not for others?\n",
    "\n",
    "- How do the parameters affect the computation time and resulting image size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051ea3-bbed-4616-b39c-e5c6052c4863",
   "metadata": {},
   "source": [
    "### Detecting sounds\n",
    "\n",
    "As you can imagine, it is not easy to manually annotate all relevant sounds in a recording.\n",
    "\n",
    "Look at this recording from a bat detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bf7b1-f4a7-4545-b0f3-e4455a204184",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_METADATA = \"/content/week4_data/data/yucatan/yucatan_metadata.csv\"\n",
    "\n",
    "YUCATAN_AUDIO_DIR = \"/content/week4_data/data/yucatan/audio\"\n",
    "\n",
    "# Load metadata of dataset of bat recordings from the yucatan peninsula\n",
    "yucatan = pd.read_csv(YUCATAN_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e66c5-401a-4dbe-b544-5fa6b7a8c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowded_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[1017])\n",
    "\n",
    "plotting.plot_spectrogram(\n",
    "    crowded_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2511b0-03fc-4fab-a720-276156e602e9",
   "metadata": {},
   "source": [
    "There are many bat calls in this recording, it would be very time consuming to annotate them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1de031-0c92-47f9-a4f5-dfb66dd01f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[205])\n",
    "plotting.plot_spectrogram(\n",
    "    empty_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dacb91-cf8a-48f4-bbf9-96b89c318638",
   "metadata": {},
   "source": [
    "This other recording has a single bat pulse. You still need to review it thoroughly to make sure there are no other sounds of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58cfa0-2b33-476a-95b1-65844dd94a34",
   "metadata": {},
   "source": [
    "### Tadarida\n",
    "\n",
    "Similar to **MegaDetector** for camera traps, there are some tools that automatically detect animal sounds in recordings.\n",
    "\n",
    "Here we will explore the tool **Tadarida**. Tadarida is a non-ML generic detector that uses a set of hand-crafted features to detect sounds. It is based on the work of:\n",
    "\n",
    "> Bas, Y., Bas, D. and Julien, J.-F., 2017. Tadarida: A Toolbox for Animal\n",
    "> Detection on Acoustic Recordings. Journal of Open Research Software, 5(1),\n",
    "> p.6. DOI: http://doi.org/10.5334/jors.154"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08608fac-89f4-4e20-ac23-2b67bed2a601",
   "metadata": {},
   "source": [
    "We will use tadarida to detect bat calls in a recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb899b1-46c1-4e66-95dc-7d68bc022921",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detection.run_tadarida_detection([empty_recording, crowded_recording])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4b7fc-d43c-430a-8f0b-537ac3312da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(crowded_recording, detections);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe222ac1-5b12-495e-96b8-44fcfbeb1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(empty_recording, detections);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a40821-856b-4646-b288-8b89382240df",
   "metadata": {},
   "source": [
    "### Evaluate Detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc807f-30b6-4210-b6e8-ecf3c16d94e7",
   "metadata": {},
   "source": [
    "The calls of this dataset were manually annotated, so we can compare the detections with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f009c8-d64a-48a0-b6e3-5bd62fae3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_ANNOTATIONS = \"/content/week4_data/data/yucatan/yucatan_annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305747f-6f97-4b8b-b615-43ea078a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "yucatan_annotations = pd.read_csv(YUCATAN_ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37062a1-5f2a-4b89-be9e-abba8e181382",
   "metadata": {},
   "source": [
    "Lets first visualize the ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d8c4e-79d9-4ac2-b5a2-dade6aa88d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(empty_recording, yucatan_annotations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd1dc3-1662-43fc-969a-f3b157b48e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(crowded_recording, yucatan_annotations);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f9304-ba3d-4628-bb11-7d19d9be82b9",
   "metadata": {},
   "source": [
    "Now we can compare the detections with the ground truth. We can use the\n",
    "Intersection over Union (IoU) to measure the overlap between the detections and\n",
    "the ground truth.\n",
    "\n",
    "<img alt=\"intersection over union\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/Intersection_over_Union_-_visual_equation.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c82c7b-7b3e-4539-a5d1-1c2b5fd43a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the predictions and annotations from the crowded recording\n",
    "file_detections = detections[\n",
    "    detections.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "file_annotations = yucatan_annotations[\n",
    "    yucatan_annotations.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "\n",
    "# Match the bounding boxes by computing the IoU. Discard all matches with IoU less than 0.5\n",
    "pred_boxes = evaluate_detection.bboxes_from_annotations(file_detections)\n",
    "true_boxes = evaluate_detection.bboxes_from_annotations(file_annotations)\n",
    "matches = evaluate_detection.match_bboxes(true_boxes, pred_boxes, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6384941-a2bf-4b63-a71d-3f4c86d50cbc",
   "metadata": {},
   "source": [
    "We select all the detections that have an IoU greater than 0.5 and count them as true positives. All the other detections are false positives. Sound events that are not detected are false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d095903-dcc4-443b-8c6b-1a08ea1cd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of annotated sound events\n",
    "positives = len(file_annotations)\n",
    "\n",
    "num_predictions = len(file_detections)\n",
    "\n",
    "# number of matched prediction boxes\n",
    "true_positives = len(matches)\n",
    "\n",
    "# number of predicted boxes that were not matched\n",
    "false_positives = num_predictions - len(matches)\n",
    "\n",
    "# number of annotated sound events that were not matched\n",
    "false_negatives = positives - len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ca26a-8588-469e-9b8a-7d640b07c923",
   "metadata": {},
   "source": [
    "With this information we can compute the precision and recall of the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d09b3b-7135-46a2-89ab-2a92d9e26ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of predictions that are correct\n",
    "precision = true_positives / num_predictions\n",
    "\n",
    "# Percentage of sound events that were detected\n",
    "recall = true_positives / positives\n",
    "\n",
    "print(\n",
    "    f\"Tadarida precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e1423-6f8c-4c6a-b2c3-88a3de2c2949",
   "metadata": {},
   "source": [
    "Lets plot predictions and annotations at the same time.\n",
    "\n",
    "- red = spurious predicted sound event (false positive)\n",
    "- green = correct prediction (true positive)\n",
    "- white = missed sound event (false negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b48e0-4a7b-48ab-a852-af37516fb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    detections,\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891dec79-2156-4a10-9162-847cd77c0ad5",
   "metadata": {},
   "source": [
    "We can also compute the precision/recall on each file in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f05dd3-e7f9-48bc-a178-d3d759c2f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed tadarida detections to save some time\n",
    "full_tadarida_detections = pd.read_csv(\"/content/week4_data/data/yucatan/yucatan_tadarida_detections.csv\")\n",
    "\n",
    "# compute the precision and recall for each file\n",
    "td_evaluation = []\n",
    "for filename in yucatan_annotations.recording_id.unique():\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        filename,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=0.5,\n",
    "    )\n",
    "    td_evaluation.append({\"wav\": filename, \"precision\": precision, \"recall\": recall})\n",
    "\n",
    "# store the results in a pandas dataframe\n",
    "td_evaluation = pd.DataFrame(td_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b581d-2ea9-4116-b83d-3c2becd2d077",
   "metadata": {},
   "source": [
    "### Exercise: Evaluate detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2df686-90e8-40b3-9a0a-3225a3fd1728",
   "metadata": {},
   "source": [
    "Using the dataframe with precision and recall of tadarida on each file (`td_evaluation`), calculate: \n",
    "\n",
    "- The mean precision and recall across all files.\n",
    "- The percentage of files where all bat calls were missed.\n",
    "- The percentage of files where at least half of the predictions were correct.\n",
    "\n",
    "Run the full evaluation again but change the IoU `threshold` parameter. What do you observe?\n",
    "\n",
    "You can use the following interactive widget to get a better grasp on tadarida's behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d1d19-b15f-4d3e-b09e-13588d5c5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tadarida predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(path=example_files, iou_threshold=(0, 1, 0.1))\n",
    "def plot_results_file_results(path=crowded_recording, iou_threshold=0.5):\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Tadarida precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f2bc0-e583-416c-91ce-6a6a61e3790c",
   "metadata": {},
   "source": [
    "### BatDetect2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f767d-7a3e-486f-84ac-128fbc76b7ce",
   "metadata": {},
   "source": [
    "As we have seen the performance of Tadarida has room for improvement. We can improve performance by using a specialised machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83711ad2-c991-47af-af15-034764405700",
   "metadata": {},
   "source": [
    "Now we will use **BatDetect2**, a deep learning model for simultaneous\n",
    "detection and classification of bat calls. Although the model was trained on a\n",
    "bat calls of UK bats, we can test its detection performance on the dataset of\n",
    "bats from the Yucat√°n peninsula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a405e-1603-4ec0-8c46-2bd4d43ed67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "batdetect2 /content/week4_data/data/yucatan/audio /content/week4_data/data/yucatan/predictions 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e7cdb-e1c3-4ff4-ac21-40a6044feaf3",
   "metadata": {},
   "source": [
    "The **BatDetect2** model can predict multiple bounding boxes for each recording.\n",
    "Unlike Tadarida, each bounding box has a **score**, a predicted species and a\n",
    "confidence score for the species.\n",
    "\n",
    "We will throw out the predicted species and confidence score, and only use the\n",
    "bounding box score. The **score** is the probability that the bounding box contains\n",
    "a bat call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c2e63-14f9-404c-8d0e-068f7ac34752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all prediction files\n",
    "files = glob.glob(\"/content/week4_data/data/yucatan/predictions/*.csv\")\n",
    "\n",
    "# Read each prediction file\n",
    "batdetect2_predictions = []\n",
    "for path in files:\n",
    "    df = pd.read_csv(path).drop(columns=[\"id\", \"class\", \"class_prob\"])\n",
    "    df[\"recording_id\"] = os.path.basename(path)[:-4]\n",
    "    batdetect2_predictions.append(df)\n",
    "\n",
    "# And concatenate them into a single dataframe\n",
    "batdetect2_predictions = pd.concat(batdetect2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43a94a-ffb0-4086-9cf0-4b5fe1936996",
   "metadata": {},
   "source": [
    "We can then use the same evaluation procedure as before to compute the\n",
    "precision and recall, except now we can select detections with a score greater\n",
    "than some customizable threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57886490-4c53-4d58-aaec-c01309e92f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "batdetect2_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea57ec7-431b-486f-a5f0-061db014a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.3\n",
    "\n",
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    batdetect2_predictions[batdetect2_predictions.det_prob > score_threshold],\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3b2f8-b55d-4e71-ac11-9367dd711ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Batdetect2 predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    path=example_files,\n",
    "    iou_threshold=(0, 1, 0.1),\n",
    "    score_threshold=(0, 1, 0.1),\n",
    ")\n",
    "def plot_batdetect2_results_file_results(\n",
    "    path=crowded_recording,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.3,\n",
    "):\n",
    "    confident_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.det_prob > score_threshold\n",
    "    ]\n",
    "\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Batdetect2 precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "        linewidth=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda0185-bafe-4675-9fe6-7b70df32af96",
   "metadata": {},
   "source": [
    "### Exercise: Compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99346cc8-4bf3-4c24-a409-580b1deb4c80",
   "metadata": {},
   "source": [
    "## Part 2: Identifying Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f6e1-6464-47a9-9814-4883ff6ae9a5",
   "metadata": {},
   "source": [
    "- In the previous section we saw how to detect sounds in a recording. But we still need to identify the species that produced the sound.\n",
    "\n",
    "- Generally, classification is more challenging than detection, as the sounds produced by different species can be very similar (**interspecific overlap**).\n",
    "\n",
    "- A single species can have flexible vocalisations, think humans or mimic birds such as starling (**intraspecific variation**).\n",
    "\n",
    "- Bioacoustic data presents similar challenges to the camera trap datasets as recordings can be:\n",
    "    - **Ocluded** (Simultaneous sounds)\n",
    "    - **Appear in varying ambient condition**s (rain/wind/thunder)\n",
    "    - **Partial** (Only captured half the sound)\n",
    "    - **Noisy** (Saturation and faulty sensor)\n",
    "    - **Quiet or very loud** (depending on animal size, distance, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264ce12-0a2a-418d-90b5-36bbd9d0f623",
   "metadata": {},
   "source": [
    "For the rest of this notebook we will focus on **5** bat species present in the Yucat√°n dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19b242-1165-426b-aac0-d20ca9bab747",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES = [\n",
    "    \"Eumops auripendulus\",\n",
    "    \"Mormoops megalophylla\",\n",
    "    \"Eumops ferox\",\n",
    "    \"Myotis keaysi\",\n",
    "    \"Saccopteryx bilineata\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bd285-7130-41f0-8c1e-12ae0f1892ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = yucatan_annotations[yucatan_annotations[\"class\"].isin(SPECIES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f472b-f647-414e-a44b-c88fda2feff0",
   "metadata": {},
   "source": [
    "### Bat call features\n",
    "\n",
    "- Previous research on bat call identification was based on hand-crafted features of the bat calls.\n",
    "\n",
    "- Measuring call features used to be a manual process.\n",
    "\n",
    "<img src=\"https://www.elekon.ch/batexplorer2/doc/_images/CallParams.png\" alt=\"call parameters\" width=\"400\"/>\n",
    "\n",
    "> Image taken from the [BatExplorer 2.1 user guide](https://www.elekon.ch/batexplorer2/doc/batcall_params.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4c318-e095-4eb5-866a-10cefd13b476",
   "metadata": {},
   "source": [
    "**Peak frequency [kHz]:**\n",
    "\n",
    ">    The frequency at which the call is loudest (peak in the spectrum display), aka frequency of maximum energy (FME) or main frequency.\n",
    ">    Most important parameter for bat classification because it can easily be measured and is often typical for a certain species or group of species.\n",
    ">    The standard deviation of the peak frequency allows the detection of alternating calling species.\n",
    "    \n",
    "**Max frequency [kHz]**\n",
    "\n",
    ">    The maximum frequency of the call. Often this is equal to the start frequency, for Rhinolophidae typically equal to the peak frequency.\n",
    "    \n",
    "**Min frequency [kHz]**\n",
    "\n",
    ">    The minimum frequency of the call. Often this is equal to the end frequency, for hockey stick calls (e.g. Pipistrelle) it might be lower than the end frequency.\n",
    "    \n",
    "**BW Peak2Min [kHz]**\n",
    "\n",
    ">    Bandwidth Peak2Min = Peak frequency - Min frequency\n",
    ">    Often used to distinguish Myotis and Pipistrelle calls, Myotis mostly have higher bandwidth.\n",
    "    \n",
    "**Call length [ms]**\n",
    "\n",
    ">    Time period of call start to call end in ms. Can be measured most accurately in the oscillogram (wave rise to wave drop).\n",
    ">    Search calls from European bats are usually between one and up to approximately 30 ms (horseshoe bats up to 80 ms).\n",
    "    \n",
    "**Call distance [ms]**\n",
    "\n",
    ">    Time period between two consecutive calls in ms. Can be measured most accurately in the oscillogram (wave rise call A to wave rise call B).\n",
    ">    Often this parameter is not very significant since most bat species have irregular rhythms. But it can be an indicator for behavior.\n",
    ">    Search calls from European bats usually have distances of about 30 to 300 ms, sometimes even longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952d398-e237-4ad2-854c-a2927a5e762f",
   "metadata": {},
   "source": [
    "### Exercise: Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c2358-71af-436e-8ed0-6d3fcd0ec5e0",
   "metadata": {},
   "source": [
    "* Explore 2 calls per species\n",
    "* Measure peak frequency, max frequency, min frequency and call length\n",
    "* Store in a dataframe\n",
    "* Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3378d03-31bb-4579-8653-241cecf60b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_annotate = classification_df.groupby(\"class\").sample(n=2)\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    index=[(f\"{r['class']}_{index}\", i) for i, (index, r) in enumerate(to_annotate.iterrows())]\n",
    ")\n",
    "def manually_extract_features_from_spectrogram(index=0):\n",
    "    row = to_annotate.iloc[index]\n",
    "    return plot_spectrogram_with_plotly(\n",
    "        path=os.path.join(YUCATAN_AUDIO_DIR, row[\"recording_id\"]),\n",
    "        start_time=row[\"start_time\"],\n",
    "        end_time=row[\"end_time\"],\n",
    "        low_freq=row[\"low_freq\"],\n",
    "        high_freq=row[\"high_freq\"],\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09917246-4e51-4d27-b5cb-099c39170a40",
   "metadata": {},
   "source": [
    "### Tadarida automated features\n",
    "\n",
    "- Tadarida extracts a large set of features from each detected sound event.\n",
    "\n",
    "- It is possible to build a pipeline for automated species identification using of automatic feature extraction process.\n",
    "\n",
    "- First we detect the sounds in the recording, then we extract the features, and\n",
    "  finally we classify the sounds.\n",
    "\n",
    "- The classification is done using the extracted features a classifier\n",
    "  algorithm, like Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595237b7-76aa-486f-b635-99cb8ee1a37b",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Build a random forest classifier\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Evaluate performance with features\n",
    "\n",
    "Exercise\n",
    "\n",
    "What species get confused?\n",
    "\n",
    "Which features are more significative? \n",
    "\n",
    "### Universal feature set (AudioSet + Yamnet)\n",
    "\n",
    "Instead of relying on hand-crafted features use acoustic feature extractor trained on 5.8k hours of sound from YouTube (AudioSet).\n",
    "\n",
    "Yamnet was trained to classify sounds into 527 different classes. The features it learned to extract are useful to distinguish and identify a large\n",
    "variety of sounds.\n",
    "\n",
    "Audioset does not contain ultrasonic recordings, and thus is devoid of bat sounds. However, we expect the learnt features to be sufficiently general that it can help identify bat calls.\n",
    "\n",
    "*Process all clips with yamnet*\n",
    "\n",
    "*Use umap to visualize*\n",
    "\n",
    "### Exercise: use YamNet features + RF to create classifier\n",
    "\n",
    "### Train a model from Scratch (Yamnet)\n",
    "\n",
    "*Fine tune Yamnet*\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Modify training parameters.\n",
    "\n",
    "Identify poorly performing species across all methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci",
   "language": "python",
   "name": "sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
