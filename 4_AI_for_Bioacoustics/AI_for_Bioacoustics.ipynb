{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d1c2e1-3a0d-46f1-bb8f-c47b6b23a06b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/4_AI_for_Bioacoustics/AI_for_Bioacoustics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519b66b-7870-4c8e-8ba7-7ceb7814d015",
   "metadata": {},
   "source": [
    "# Week 4 - AI for Bioacoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1c4ab-0475-4ecd-98fe-457c3e4a30fa",
   "metadata": {},
   "source": [
    "## What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf4cc-a14c-437d-8c99-d24edfad0d78",
   "metadata": {},
   "source": [
    "In this weeks practical we will explore computer audition applications in ecology such as automated animal detection and species classification from audio sensor data. Among other stuff we will:\n",
    "\n",
    "1. Learn how to visualize audio data.\n",
    "2. Apply an automated animal detector on a set of audio files.\n",
    "3. Learn to evaluate the performance of a sound detector.\n",
    "4. Use a trained neural network to detect animals.\n",
    "5. Manually extract relevant sound features.\n",
    "6. Use features to create an automated animal sound identifier.\n",
    "7. Compare between hand-crafted feature sets with learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea53d95-0644-42b5-9c7b-93f0ce6bc001",
   "metadata": {},
   "source": [
    "If time permits, we will have some time exploring how to train a neural network for audio classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4acde-b379-4a49-93c2-38a8095be264",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a625520-aa73-4a25-894c-d6dc993f9166",
   "metadata": {},
   "source": [
    "### Computer audition\n",
    "\n",
    "**What is Computer audition?**\n",
    "\n",
    "- Computer audition is the field of research that deals with the automatic\n",
    "  analysis of audio signals.\n",
    "\n",
    "- It intersects with many other fields, including machine learning, signal\n",
    "  processing, and computer vision.\n",
    "\n",
    "**What does a computer hear**?\n",
    "\n",
    "- Audio files are a sequence of numbers representing the amplitude of the sound\n",
    "  wave at a given time.\n",
    "\n",
    "- The number of samples per second is called the **sampling rate**.\n",
    "\n",
    "<img alt=\"audio and sampling rate\" width=\"600\" src=\"https://cdn.shopify.com/s/files/1/1169/2482/files/Sampling_Rate_Cover_image.jpg?v=1654170259\"></img>\n",
    "\n",
    "**What tasks can we do with computer audition?**\n",
    "\n",
    "Computer audition is used in a wide range of applications, including:\n",
    "\n",
    "- Speech recognition: Siri, Alexa, Google Assistant\n",
    "- Music information retrieval: Spotify, Shazam\n",
    "- Audio classification: What is sounding in this audio?\n",
    "- Sound event detection: Transcription of audio into a sequence of events.\n",
    "\n",
    "<img alt=\"Sound event detection\" width=\"400\" src=\"http://d33wubrfki0l68.cloudfront.net/508a62f305652e6d9af853c65ab33ae9900ff38e/17a88/images/tasks/challenge2016/task3_overview.png\"></img>\n",
    "\n",
    "> Taken from the paper: Mesaros, A., Heittola, T., Diment, A., Elizalde, B., Shah, A., Vincent, E., ... & Virtanen, T. (2017, November). DCASE 2017 challenge setup: Tasks, datasets and baseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events.\n",
    "\n",
    "Recently neural network models have taken over the field of computer audition and are being used to solve many of the above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56935d83-a08f-4a2f-9ba6-45a3b3660833",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "\n",
    "**Acoustic sensors** can be used to collect field recordings of animal sounds.\n",
    "\n",
    "Usually, these sensors are deployed statically in the field for a long periods of time and record sounds continuously. This is called **passive acoustic monitoring**.\n",
    "\n",
    "<img alt=\"passive acoustic monitoring\" width=\"400\" src=\"https://wittmann-tours.de/wp-content/uploads/2018/06/AudioMoth.jpg\"></img>\n",
    "\n",
    "Alternatively, recordings are actively directed towards a specific animal species or sound events.\n",
    "\n",
    "<img alt=\"active recording\" width=\"400\" src=\"https://s3.amazonaws.com/cdn.freshdesk.com/data/helpdesk/attachments/production/48032687175/original/xjI7Dy3Q9kaCZinr5vf4ksNxQbjK13Yv3A.jpg?1584552543\"></img>\n",
    "\n",
    "> Taken from the Macaulay Library blog post:\n",
    "> [Sound recording tips](https://support.ebird.org/en/support/solutions/articles/48001064298-sound-recording-tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef333a71-9414-4127-b196-76f37f5f4315",
   "metadata": {},
   "source": [
    "### Acoustics for ecology\n",
    "\n",
    "The sound at a site is a reflection of the species present in the area and other\n",
    "environmental factors.\n",
    "\n",
    "<img alt=\"composition of acoustic space\" width=\"400\" src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs12304-017-9288-5/MediaObjects/12304_2017_9288_Fig1_HTML.gif?as=webp\"></img>\n",
    "\n",
    "> Taken from the paper: Mullet, T.C., Farina, A. & Gage, S.H. The Acoustic\n",
    "> Habitat Hypothesis: An Ecoacoustics Perspective on Species Habitat Selection.\n",
    "> Biosemiotics 10, 319‚Äì336 (2017). https://doi.org/10.1007/s12304-017-9288-5\n",
    "\n",
    "If we could link the sounds to the species, we could use this information to\n",
    "study and monitor the biodiversity of an area.\n",
    "\n",
    "Acoustic sensors produce a lot of data, and it is not always easy to analyse.\n",
    "Can we use computer audition to help us?\n",
    "\n",
    "In this practical we will explore the task of **animal sound detection** and\n",
    "**species classification**, using both manual and automated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386638-3d9b-4e64-8c38-58f86f6946ad",
   "metadata": {},
   "source": [
    "## Setup Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0c85-f597-4e48-ba23-dc19fe24996a",
   "metadata": {},
   "source": [
    "Here we will go through the steps to setup the environment for this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ec3ff-fd0e-466b-af3a-a5b549227de5",
   "metadata": {},
   "source": [
    "### Make sure to use GPU runtime in Colab. \n",
    "\n",
    "Go to `Runtime` -> `Change runtime type` and select `GPU` as the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ceeb1-d929-4570-98d5-d42bc7468552",
   "metadata": {},
   "source": [
    "### Mount your Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c1656-f864-414a-99ad-7e963cb2d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c907e-d1d2-47ae-8786-dbe2ab2fcaa5",
   "metadata": {},
   "source": [
    "Add a shortcut in you drive to this [shared folder](https://drive.google.com/drive/folders/1hbbbsILNBsQghktuj0z_Jq_3iEZQCCbj?usp=share_link).\n",
    "\n",
    "This will allow you to access the data we will use in this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aba172-13cd-4962-8465-40ce5d134346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Extract data into machine\n",
    "!unzip /content/drive/MyDrive/BIO0032_AI4Environment/week4_data.zip -d /content/week4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b1804-eda7-4ea2-9ba7-72e15404950d",
   "metadata": {},
   "source": [
    "### Install and import dependencies. \n",
    "\n",
    "Run the following cell to install the required dependencies. This will take a few minutes. You can omit the outputs of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cafa5-4655-48f2-b7ef-621fc148425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get install libfftw3-dev libicu-dev libsndfile1-dev libqt5core5a\n",
    "!pip install pytadarida git+https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment.git batdetect2 umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a40ea-60a5-4cbb-9f4d-a586c55dd98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import ipywidgets\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import xarray as xa\n",
    "from IPython.display import Audio\n",
    "from librosa import display\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "from bios0032utils.bioacoustics import detection, evaluate_detection, plotting\n",
    "from bios0032utils.bioacoustics.classification import (\n",
    "    TADARIDA_FEATURES,\n",
    "    load_bat_call_audio_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0aaa9-819e-4ed3-a6f7-092b445427e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Detecting Animal Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33734ab6-584a-4827-bf6c-7976b3168c80",
   "metadata": {},
   "source": [
    "- Most of the time, we are not interested in all the sounds in a recording, but only in the sounds of a specific animal species.\n",
    "\n",
    "- Acoustic sensors will indiscriminately record all sounds in the environment, including those of animals, wind, rain, etc. Although some recorders can be triggered by a specific sound, this is not always the case.\n",
    "\n",
    "- Passive acoustic monitoring produces many hours of recordings, and it's hard to identify and explore the sounds of interest.\n",
    "\n",
    "These are similar problems to the ones we have seen in the previous practicals for camera trap images.\n",
    "\n",
    "While not as developed as in **computer vision**, there are some tools for automatically **detecting animal sounds** in recordings. Here we will explore a few of them.\n",
    "\n",
    "But first we need to understand how to visualise and annotate sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6d8ce-4bbc-4e71-b29c-a6b7100d5544",
   "metadata": {},
   "source": [
    "### Animal sounds visualisation\n",
    "\n",
    "- While we can listen to the sounds in a recording, it is often easier to\n",
    "visualise them.\n",
    "\n",
    "- This is especially true when we want to compare sounds from\n",
    "different recordings or navigate quickly without the need to listen.\n",
    "\n",
    "- We can use waveplots and spectrograms to visualise the sounds in a recording.\n",
    "\n",
    "Now we will load a dataset of animal recordings provided by [Avisoft](https://www.avisoft.com/animal-sounds/) and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a1eef-d8a1-4efe-94d1-56a6f610d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/content/week4_data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f7a88-6c4d-4a4e-82c8-94d14703cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVISOFT_AUDIO_DIR = DATA_DIR / \"avisoft\" / \"audio\"\n",
    "AVISOFT_METADATA_FILE = DATA_DIR / \"avisoft\" / \"avisoft_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eac016-30de-49b7-a1bc-3faa987b7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata dataframe\n",
    "avisoft = pd.read_csv(AVISOFT_METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc08b3-6394-493f-9f67-28740719d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few rows\n",
    "avisoft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5db25f-fa7d-4049-a37c-fa34692737b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random file from the dataset\n",
    "random_recording = avisoft.sample(n=1).iloc[0]\n",
    "\n",
    "# read the audio file and import it as a numpy array\n",
    "wav, samplerate = librosa.load(\n",
    "    os.path.join(AVISOFT_AUDIO_DIR, random_recording.wav),\n",
    "    sr=None,\n",
    ")\n",
    "\n",
    "# Compute the duration of audio\n",
    "num_samples = len(wav)  # Number of samples taken by the recorder\n",
    "duration = num_samples / samplerate\n",
    "\n",
    "# Get name of animal\n",
    "animal_name = random_recording.english_name\n",
    "\n",
    "print(f\"File selected = {random_recording.wav}\")\n",
    "print(f\"Samplerate = {samplerate} Hz\")\n",
    "print(f\"Duration = {duration:.2f} s\")\n",
    "print(f\"Species = {animal_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a67a0-10bd-4400-bef5-5a260bf978c3",
   "metadata": {},
   "source": [
    "Lets first listen to the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf5b52-f1e8-4085-81e4-5a547752a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=wav, rate=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633c8a3-6948-4497-a4bf-a4c319bd4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of the waveform\n",
    "times = np.linspace(0, duration, num_samples)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(times, wav)\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.title(f\"Waveform of {animal_name} sound\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eead3-d1d3-41ee-8bc3-970ed31fe704",
   "metadata": {},
   "source": [
    "The **waveform** gives us a visual representation of the sound amplitude over time.\n",
    "\n",
    "However, ff there are multiple simultaneous sounds in the recording, it can be hard to see each individual sound. \n",
    "\n",
    "We can use a **spectrogram** to decompose the sound into **frequencies** and visualise them as a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650df605-e01b-48aa-a436-729aaad2f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the spectrogram with the short time fourier transform (STFT)\n",
    "spectrogram = np.abs(librosa.stft(wav))\n",
    "\n",
    "# Amplitude is best represented in logarithmic scale (decibels)\n",
    "db_spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def0c4b-b15d-4c7a-bf8e-61eb226b91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of spectrogram\n",
    "num_freq_bins, num_time_bins = db_spectrogram.shape\n",
    "times = np.linspace(0, duration, num_time_bins)\n",
    "freqs = np.linspace(0, samplerate / 2, num_freq_bins)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(times, freqs, db_spectrogram, cmap=\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(f\"Spectrogram of {random_recording.english_name} sound\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"freq (Hz)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbde3f-289e-4097-aa94-9abd336bb6e1",
   "metadata": {},
   "source": [
    "### Exercise üêò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72092175-0c8c-440d-82b5-a6d5c75bcb86",
   "metadata": {},
   "source": [
    "The sounds produced by animals can be very different from each other. The\n",
    "transformation used to create the spectrogram, called the **short-time Fourier\n",
    "transform** (STFT), will highlight different features of the sound depending on\n",
    "the parameters used.\n",
    "\n",
    "- Research what the STFT is and how its parameters affect the spectrogram. In\n",
    "  particular, try to understand the effect of the **window size** and the **hop\n",
    "  size** or **overlap**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dad7c-f01f-4c9c-aad4-4021450b9d93",
   "metadata": {},
   "source": [
    "Here you can visualise sounds from different species and see how the STFT\n",
    "parameters affect the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ace48-5613-4dd0-a28d-94f2d5e258d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Interactive spectrogram of animal sounds\n",
    "\n",
    "# @markdown Select the file you wish to visualize. Modify the spectrogram parameters to see its effect on the spectrogram. Change the reproduction speed for interesting effects!\n",
    "\n",
    "# Select some varied sounds from avisoft dataset\n",
    "examples = [\n",
    "    (row.english_name, os.path.join(AVISOFT_AUDIO_DIR, row.wav))\n",
    "    # select one random recording per taxonomic group\n",
    "    for row in avisoft.groupby(\"order\").sample(n=1).itertuples()\n",
    "]\n",
    "\n",
    "# Create interactive plot\n",
    "ipywidgets.interact(\n",
    "    plotting.plot_waveform_with_spectrogram,\n",
    "    hop_length=(32, 1024, 32),\n",
    "    n_fft=(32, 2048, 32),\n",
    "    window=plotting.WINDOW_OPTIONS,\n",
    "    file=examples,\n",
    "    cmap=plotting.COLORMAPS,\n",
    "    speed=[\n",
    "        (\"x1\", 1),\n",
    "        (\"x1.5\", 1.5),\n",
    "        (\"x2\", 2),\n",
    "        (\"x0.5\", 0.5),\n",
    "        (\"x0.2\", 0.2),\n",
    "        (\"x0.1\", 0.1),\n",
    "    ],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228393ff-2f15-4a52-a068-f9b01f20dcd3",
   "metadata": {},
   "source": [
    "- Try changing the parameters and see how they affect sounds from different\n",
    "  species.\n",
    "\n",
    "- Can you see that some choice of parameters are good for some species but\n",
    "  not for others?\n",
    "\n",
    "- How do the parameters affect the computation time and resulting image size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051ea3-bbed-4616-b39c-e5c6052c4863",
   "metadata": {},
   "source": [
    "### Detecting sounds\n",
    "\n",
    "As you can imagine, it is not easy to manually annotate all relevant sounds in a recording.\n",
    "\n",
    "Look at this recording from a bat detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bf7b1-f4a7-4545-b0f3-e4455a204184",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_METADATA = DATA_DIR / \"yucatan\" / \"yucatan_metadata.csv\"\n",
    "\n",
    "YUCATAN_AUDIO_DIR = DATA_DIR / \"yucatan\" / \"audio\"\n",
    "\n",
    "# Load metadata of dataset of bat recordings from the yucatan peninsula\n",
    "yucatan = pd.read_csv(YUCATAN_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e66c5-401a-4dbe-b544-5fa6b7a8c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowded_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[1017])\n",
    "\n",
    "plotting.plot_spectrogram(\n",
    "    crowded_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2511b0-03fc-4fab-a720-276156e602e9",
   "metadata": {},
   "source": [
    "There are many bat calls in this recording, it would be very time consuming to annotate them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1de031-0c92-47f9-a4f5-dfb66dd01f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_recording = os.path.join(YUCATAN_AUDIO_DIR, yucatan.id[205])\n",
    "plotting.plot_spectrogram(\n",
    "    empty_recording,\n",
    "    hop_length=128,\n",
    "    n_fft=512,\n",
    "    figsize=(14, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dacb91-cf8a-48f4-bbf9-96b89c318638",
   "metadata": {},
   "source": [
    "This other recording has a single bat pulse. You still need to review it thoroughly to make sure there are no other sounds of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58cfa0-2b33-476a-95b1-65844dd94a34",
   "metadata": {},
   "source": [
    "### Tadarida\n",
    "\n",
    "Similar to **MegaDetector** for camera traps, there are some tools that automatically detect animal sounds in recordings.\n",
    "\n",
    "Here we will explore the tool **Tadarida**. Tadarida is a non-ML generic detector that uses a set of hand-crafted features to detect sounds. It is based on the work of:\n",
    "\n",
    "> Bas, Y., Bas, D. and Julien, J.-F., 2017. Tadarida: A Toolbox for Animal\n",
    "> Detection on Acoustic Recordings. Journal of Open Research Software, 5(1),\n",
    "> p.6. DOI: http://doi.org/10.5334/jors.154"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08608fac-89f4-4e20-ac23-2b67bed2a601",
   "metadata": {},
   "source": [
    "We will use tadarida to detect bat calls in a recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb899b1-46c1-4e66-95dc-7d68bc022921",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detection.run_tadarida_detection([empty_recording, crowded_recording])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4b7fc-d43c-430a-8f0b-537ac3312da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(crowded_recording, detections);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe222ac1-5b12-495e-96b8-44fcfbeb1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(empty_recording, detections);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a40821-856b-4646-b288-8b89382240df",
   "metadata": {},
   "source": [
    "### Evaluate Detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc807f-30b6-4210-b6e8-ecf3c16d94e7",
   "metadata": {},
   "source": [
    "The calls of this dataset were manually annotated, so we can compare the detections with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f009c8-d64a-48a0-b6e3-5bd62fae3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "YUCATAN_ANNOTATIONS = DATA_DIR / \"yucatan\" / \"yucatan_annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305747f-6f97-4b8b-b615-43ea078a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "yucatan_annotations = pd.read_csv(YUCATAN_ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37062a1-5f2a-4b89-be9e-abba8e181382",
   "metadata": {},
   "source": [
    "Lets first visualize the ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d8c4e-79d9-4ac2-b5a2-dade6aa88d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(empty_recording, yucatan_annotations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd1dc3-1662-43fc-969a-f3b157b48e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_and_detection(crowded_recording, yucatan_annotations);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f9304-ba3d-4628-bb11-7d19d9be82b9",
   "metadata": {},
   "source": [
    "Now we can compare the detections with the ground truth. We can use the\n",
    "Intersection over Union (IoU) to measure the overlap between the detections and\n",
    "the ground truth.\n",
    "\n",
    "<img alt=\"intersection over union\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/Intersection_over_Union_-_visual_equation.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c82c7b-7b3e-4539-a5d1-1c2b5fd43a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the predictions and annotations from the crowded recording\n",
    "file_detections = detections[\n",
    "    detections.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "file_annotations = yucatan_annotations[\n",
    "    yucatan_annotations.recording_id == os.path.basename(crowded_recording)\n",
    "]\n",
    "\n",
    "# Match the bounding boxes by computing the IoU. Discard all matches with IoU less than 0.5\n",
    "pred_boxes = evaluate_detection.bboxes_from_annotations(file_detections)\n",
    "true_boxes = evaluate_detection.bboxes_from_annotations(file_annotations)\n",
    "matches = evaluate_detection.match_bboxes(true_boxes, pred_boxes, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6384941-a2bf-4b63-a71d-3f4c86d50cbc",
   "metadata": {},
   "source": [
    "We select all the detections that have an IoU greater than 0.5 and count them as true positives. All the other detections are false positives. Sound events that are not detected are false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d095903-dcc4-443b-8c6b-1a08ea1cd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of annotated sound events\n",
    "positives = len(file_annotations)\n",
    "\n",
    "num_predictions = len(file_detections)\n",
    "\n",
    "# number of matched prediction boxes\n",
    "true_positives = len(matches)\n",
    "\n",
    "# number of predicted boxes that were not matched\n",
    "false_positives = num_predictions - len(matches)\n",
    "\n",
    "# number of annotated sound events that were not matched\n",
    "false_negatives = positives - len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ca26a-8588-469e-9b8a-7d640b07c923",
   "metadata": {},
   "source": [
    "With this information we can compute the precision and recall of the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d09b3b-7135-46a2-89ab-2a92d9e26ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of predictions that are correct\n",
    "precision = true_positives / num_predictions\n",
    "\n",
    "# Percentage of sound events that were detected\n",
    "recall = true_positives / positives\n",
    "\n",
    "print(\n",
    "    f\"Tadarida precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e1423-6f8c-4c6a-b2c3-88a3de2c2949",
   "metadata": {},
   "source": [
    "Lets plot predictions and annotations at the same time.\n",
    "\n",
    "- red = spurious predicted sound event (false positive)\n",
    "- green = correct prediction (true positive)\n",
    "- white = missed sound event (false negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b48e0-4a7b-48ab-a852-af37516fb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    detections,\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891dec79-2156-4a10-9162-847cd77c0ad5",
   "metadata": {},
   "source": [
    "We can also compute the precision/recall on each file in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f05dd3-e7f9-48bc-a178-d3d759c2f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed tadarida detections to save some time\n",
    "full_tadarida_detections = pd.read_csv(\n",
    "    DATA_DIR / \"yucatan\" / \"yucatan_tadarida_detections.csv\"\n",
    ")\n",
    "\n",
    "# compute the precision and recall for each file\n",
    "td_evaluation = []\n",
    "for filename in yucatan_annotations.recording_id.unique():\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        filename,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=0.5,\n",
    "    )\n",
    "    td_evaluation.append({\"wav\": filename, \"precision\": precision, \"recall\": recall})\n",
    "\n",
    "# store the results in a pandas dataframe\n",
    "td_evaluation = pd.DataFrame(td_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b581d-2ea9-4116-b83d-3c2becd2d077",
   "metadata": {},
   "source": [
    "### Exercise üêã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2df686-90e8-40b3-9a0a-3225a3fd1728",
   "metadata": {},
   "source": [
    "Using the dataframe with precision and recall of tadarida on each file (`td_evaluation`), calculate: \n",
    "\n",
    "- The mean precision and recall across all files.\n",
    "- The percentage of files where all bat calls were missed.\n",
    "- The percentage of files where at least half of the predictions were correct.\n",
    "\n",
    "Run the full evaluation again but change the IoU `threshold` parameter. What do you observe?\n",
    "\n",
    "You can use the following interactive widget to get a better grasp on tadarida's behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d1d19-b15f-4d3e-b09e-13588d5c5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tadarida predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(path=example_files, iou_threshold=(0, 1, 0.1))\n",
    "def plot_results_file_results(path=crowded_recording, iou_threshold=0.5):\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Tadarida precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        full_tadarida_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f2bc0-e583-416c-91ce-6a6a61e3790c",
   "metadata": {},
   "source": [
    "### BatDetect2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f767d-7a3e-486f-84ac-128fbc76b7ce",
   "metadata": {},
   "source": [
    "As we have seen the performance of Tadarida has room for improvement. We can improve performance by using a specialised machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83711ad2-c991-47af-af15-034764405700",
   "metadata": {},
   "source": [
    "Now we will use **BatDetect2**, a deep learning model for simultaneous\n",
    "detection and classification of bat calls. Although the model was trained on a\n",
    "bat calls of UK bats, we can test its detection performance on the dataset of\n",
    "bats from the Yucat√°n peninsula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from batdetect2 import api\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826669fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = glob.glob(str(DATA_DIR / \"yucatan\" / \"audio\" / \"*.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for path in tqdm(recordings):\n",
    "    results.append(api.process_file(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e7cdb-e1c3-4ff4-ac21-40a6044feaf3",
   "metadata": {},
   "source": [
    "The **BatDetect2** model can predict multiple bounding boxes for each recording.\n",
    "Unlike Tadarida, each bounding box has a **score**, a predicted species and a\n",
    "confidence score for the species.\n",
    "\n",
    "We will throw out the predicted species and confidence score, and only use the\n",
    "bounding box score. The **score** is the probability that the bounding box contains\n",
    "a bat call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02170ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c2e63-14f9-404c-8d0e-068f7ac34752",
   "metadata": {},
   "outputs": [],
   "source": [
    "batdetect2_predictions = []\n",
    "for result in results:\n",
    "    data = result[\"pred_dict\"]\n",
    "    for prediction in data[\"annotation\"]:\n",
    "        batdetect2_predictions.append({\n",
    "            'det_prob': prediction[\"det_prob\"],\n",
    "            'start_time': prediction[\"start_time\"],\n",
    "            'end_time': prediction[\"end_time\"],\n",
    "            'high_freq': prediction[\"high_freq\"],\n",
    "            'low_freq': prediction[\"low_freq\"],\n",
    "            'recording_id': data[\"id\"],\n",
    "        })\n",
    "\n",
    "# And collect them into a single dataframe\n",
    "batdetect2_predictions = pd.DataFrame(batdetect2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43a94a-ffb0-4086-9cf0-4b5fe1936996",
   "metadata": {},
   "source": [
    "We can then use the same evaluation procedure as before to compute the\n",
    "precision and recall, except now we can select detections with a score greater\n",
    "than some customizable threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57886490-4c53-4d58-aaec-c01309e92f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "batdetect2_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea57ec7-431b-486f-a5f0-061db014a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.3\n",
    "\n",
    "plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "    crowded_recording,\n",
    "    batdetect2_predictions[batdetect2_predictions.det_prob > score_threshold],\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.3,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3b2f8-b55d-4e71-ac11-9367dd711ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Batdetect2 predictions\n",
    "\n",
    "# @markdown Select a file and an IoU threshold.\n",
    "\n",
    "example_files = [\n",
    "    os.path.join(YUCATAN_AUDIO_DIR, row[\"id\"])\n",
    "    for _, row in yucatan.sample(n=20).iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    path=example_files,\n",
    "    iou_threshold=(0, 1, 0.1),\n",
    "    score_threshold=(0, 1, 0.1),\n",
    ")\n",
    "def plot_batdetect2_results_file_results(\n",
    "    path=crowded_recording,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.3,\n",
    "):\n",
    "    confident_detections = batdetect2_predictions[\n",
    "        batdetect2_predictions.det_prob > score_threshold\n",
    "    ]\n",
    "\n",
    "    precision, recall = evaluate_detection.compute_file_precision_recall(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Batdetect2 precision={precision:.1%} recall={recall:.1%} on file {crowded_recording}\"\n",
    "    )\n",
    "\n",
    "    plotting.plot_spectrogram_with_predictions_and_annotations(\n",
    "        path,\n",
    "        confident_detections,\n",
    "        yucatan_annotations,\n",
    "        iou_threshold=iou_threshold,\n",
    "        linewidth=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31c0c1-1567-4249-aa56-75dddcaf8aec",
   "metadata": {},
   "source": [
    "### Exercise üê∏\n",
    "\n",
    "Here you will compare the performance of the two detectors.\n",
    "\n",
    "1. Use the function `evaluate_detection.compute_detection_metrics` to calculate the number\n",
    "of positive, true positives, false positives and false negatives for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d8307-c427-4889-8a81-ad68144b64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example here is how you use the function\n",
    "example_file = example_files[0]\n",
    "\n",
    "confident_detections = batdetect2_predictions[batdetect2_predictions.det_prob > 0.3]\n",
    "\n",
    "(\n",
    "    positives,\n",
    "    true_positives,\n",
    "    false_positives,\n",
    "    false_negatives,\n",
    ") = evaluate_detection.compute_detection_metrics(\n",
    "    example_file,\n",
    "    confident_detections,\n",
    "    yucatan_annotations,\n",
    "    iou_threshold=0.5,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{positives = }, {true_positives = }, {false_positives = }, {false_negatives = }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19f676-0a8d-4f26-b456-220c4a8bac3f",
   "metadata": {},
   "source": [
    "2. Compute the overall precision and recall for each detector.\n",
    "\n",
    "3. Which detector performs better?\n",
    "\n",
    "4. How does this change if you change the score threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99346cc8-4bf3-4c24-a409-580b1deb4c80",
   "metadata": {},
   "source": [
    "## Part 2: Identifying Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f6e1-6464-47a9-9814-4883ff6ae9a5",
   "metadata": {},
   "source": [
    "- In the previous section we saw how to detect sounds in a recording. But we still need to identify the species that produced the sound.\n",
    "\n",
    "- Generally, classification is more challenging than detection, as the sounds produced by different species can be very similar (**interspecific overlap**).\n",
    "\n",
    "- A single species can have flexible vocalisations, think humans or mimic birds such as starling (**intraspecific variation**).\n",
    "\n",
    "- Bioacoustic data presents similar challenges to the camera trap datasets as recordings can be:\n",
    "    - **Ocluded** (Simultaneous sounds)\n",
    "    - **Appear in varying ambient condition**s (rain/wind/thunder)\n",
    "    - **Partial** (Only captured half the sound)\n",
    "    - **Noisy** (Saturation and faulty sensor)\n",
    "    - **Quiet or very loud** (depending on animal size, distance, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264ce12-0a2a-418d-90b5-36bbd9d0f623",
   "metadata": {},
   "source": [
    "For the rest of this notebook we will focus on **10** bat species present in the Yucat√°n dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19b242-1165-426b-aac0-d20ca9bab747",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES = [\n",
    "    \"Mormoops megalophylla\",\n",
    "    \"Myotis keaysi\",\n",
    "    \"Saccopteryx bilineata\",\n",
    "    \"Pteronotus davyi\",\n",
    "    \"Pteronotus parnellii\",\n",
    "    \"Lasiurus ega\",\n",
    "    \"Pteropteryx macrotis\",\n",
    "    \"Eumops underwoodi\",\n",
    "    \"Rhogeessa aeneus\",\n",
    "    \"Eptesicus furinalis\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bd285-7130-41f0-8c1e-12ae0f1892ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = yucatan_annotations[yucatan_annotations[\"class\"].isin(SPECIES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f472b-f647-414e-a44b-c88fda2feff0",
   "metadata": {},
   "source": [
    "### Bat call features\n",
    "\n",
    "- Previous research on bat call identification was based on hand-crafted features of the bat calls.\n",
    "\n",
    "- Measuring call features used to be a manual process.\n",
    "\n",
    "<img src=\"https://www.elekon.ch/batexplorer2/doc/_images/CallParams.png\" alt=\"call parameters\" width=\"400\"/>\n",
    "\n",
    "> Image taken from the [BatExplorer 2.1 user guide](https://www.elekon.ch/batexplorer2/doc/batcall_params.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4c318-e095-4eb5-866a-10cefd13b476",
   "metadata": {},
   "source": [
    "**Peak frequency [kHz]:**\n",
    "\n",
    ">    The frequency at which the call is loudest (peak in the spectrum display), aka frequency of maximum energy (FME) or main frequency.\n",
    ">    Most important parameter for bat classification because it can easily be measured and is often typical for a certain species or group of species.\n",
    ">    The standard deviation of the peak frequency allows the detection of alternating calling species.\n",
    "    \n",
    "**Max frequency [kHz]**\n",
    "\n",
    ">    The maximum frequency of the call. Often this is equal to the start frequency, for Rhinolophidae typically equal to the peak frequency.\n",
    "    \n",
    "**Min frequency [kHz]**\n",
    "\n",
    ">    The minimum frequency of the call. Often this is equal to the end frequency, for hockey stick calls (e.g. Pipistrelle) it might be lower than the end frequency.\n",
    "    \n",
    "**BW Peak2Min [kHz]**\n",
    "\n",
    ">    Bandwidth Peak2Min = Peak frequency - Min frequency\n",
    ">    Often used to distinguish Myotis and Pipistrelle calls, Myotis mostly have higher bandwidth.\n",
    "    \n",
    "**Call length [ms]**\n",
    "\n",
    ">    Time period of call start to call end in ms. Can be measured most accurately in the oscillogram (wave rise to wave drop).\n",
    ">    Search calls from European bats are usually between one and up to approximately 30 ms (horseshoe bats up to 80 ms).\n",
    "    \n",
    "**Call distance [ms]**\n",
    "\n",
    ">    Time period between two consecutive calls in ms. Can be measured most accurately in the oscillogram (wave rise call A to wave rise call B).\n",
    ">    Often this parameter is not very significant since most bat species have irregular rhythms. But it can be an indicator for behavior.\n",
    ">    Search calls from European bats usually have distances of about 30 to 300 ms, sometimes even longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952d398-e237-4ad2-854c-a2927a5e762f",
   "metadata": {},
   "source": [
    "### Exercise üê¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c2358-71af-436e-8ed0-6d3fcd0ec5e0",
   "metadata": {},
   "source": [
    "* Explore 1 calls per species\n",
    "* Measure peak frequency, max frequency, min frequency and call length\n",
    "* Store in a dataframe\n",
    "* Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3378d03-31bb-4579-8653-241cecf60b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_annotate = classification_df.groupby(\"class\").sample(n=1)\n",
    "\n",
    "\n",
    "@ipywidgets.interact(\n",
    "    index=[\n",
    "        (f\"{r['class']}_{index}\", i)\n",
    "        for i, (index, r) in enumerate(to_annotate.iterrows())\n",
    "    ]\n",
    ")\n",
    "def manually_extract_features_from_spectrogram(index=0):\n",
    "    row = to_annotate.iloc[index]\n",
    "    return plotting.plot_spectrogram_with_plotly(\n",
    "        path=os.path.join(YUCATAN_AUDIO_DIR, row[\"recording_id\"]),\n",
    "        start_time=row[\"start_time\"],\n",
    "        end_time=row[\"end_time\"],\n",
    "        low_freq=row[\"low_freq\"],\n",
    "        high_freq=row[\"high_freq\"],\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09917246-4e51-4d27-b5cb-099c39170a40",
   "metadata": {},
   "source": [
    "### Tadarida automated features\n",
    "\n",
    "- Tadarida extracts a large set of features from each detected sound event.\n",
    "\n",
    "- It is possible to build a pipeline for automated species identification using of automatic feature extraction process.\n",
    "\n",
    "- First we detect the sounds in the recording, then we extract the features, and\n",
    "  finally we classify the sounds.\n",
    "\n",
    "- The classification is done using the extracted features a classifier\n",
    "  algorithm, like Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d0492-0505-48c0-879d-e9df6b6d20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load tadarida extracted features\n",
    "tadarida_features = pd.read_csv(DATA_DIR / \"yucatan\" / \"yucatan_tadarida_features.csv\")\n",
    "\n",
    "# the dataframe contains the class of each detected pulse and\n",
    "# a suite of automatically extracted parameters\n",
    "species = tadarida_features[\"Class\"]\n",
    "\n",
    "print(f\"Number of features = {len(TADARIDA_FEATURES)}\")\n",
    "features = tadarida_features[TADARIDA_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d51a5-e25a-49d5-99d6-ef7367f19c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will split the data into train a test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    species,\n",
    "    test_size=0.3,\n",
    "    stratify=species,  # notice we are using the stratified argument\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181dd8d-961f-4ce5-b9d8-79dc858ca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the number of examples per species in each dataset\n",
    "# Train dataset\n",
    "y_train.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876d4a5-b3e5-4441-95a7-7c1e0c0c45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "y_test.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0dc8c-f141-4df3-86fe-a45052dc2834",
   "metadata": {},
   "source": [
    "Now we are ready to train a classifier to identify the species of the bat calls.\n",
    "\n",
    "We will use a **Random Forest Classifier** from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd5da9-dd8f-4127-81c4-a1d02527dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# fit to training data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71016c0-4a23-4912-80dd-b3f94f953ce2",
   "metadata": {},
   "source": [
    "We can evaluate the performance of the classifier in a similar fashion to previous practicals.\n",
    "\n",
    "There is a useful `classification_report` function from `sklearn.metrics` to compute the precision, recall and F1 score for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7525c-5668-46e7-b371-52a9fef64818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with rf model on evaluation data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917af10-d7b1-4057-b6f1-7f7569a0107d",
   "metadata": {},
   "source": [
    "### Exercise ü¶ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e4a9a-0493-45ad-82ff-580a2efe5fc4",
   "metadata": {},
   "source": [
    "In this exercise you will analyze the model's performance in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbe60c-afff-46ee-871c-7f70f9efdd03",
   "metadata": {},
   "source": [
    "1. Plot the confusion matrix using the `sklearn.metrics.ConfusionMatrixDisplay` from scikit learn.\n",
    "\n",
    "If you are confused it is always helpful to look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) and provided examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad54dc-ec28-4b10-bde1-2afad7899b5b",
   "metadata": {},
   "source": [
    "2. Which are the worst performing species? Can you see why? You might need to go back and see some more examples of bat calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0215c-b423-4fa8-93f2-969624bfdb03",
   "metadata": {},
   "source": [
    "### Universal feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6be64-1e56-440e-939f-ad470e249807",
   "metadata": {},
   "source": [
    "- Is Tadarida's feature set optimal for bat call identification?\n",
    "\n",
    "- There is a huge amount of information in the recorded audio that we are throwing away by using hand-crafted features.\n",
    "\n",
    "- Can we learn a better feature set from the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b535bf6-9d97-4553-bcb7-6fee66366974",
   "metadata": {},
   "source": [
    "Here instead of relying on hand-crafted features we will use acoustic feature extractor called [Yamnet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet).\n",
    "\n",
    "Yamnet was trained on [AudioSet](http://research.google.com/audioset/) a massive dataset of YouTube recordings with more than 5.8 thousand hours of audio.\n",
    "\n",
    "It was trained to classify sounds clips into 527 different classes. The features it learned to extract are thus useful to distinguish and identify a large variety of sounds.\n",
    "\n",
    "<img src=\"http://research.google.com/audioset/resources/histogram.svg\" alt=\"audioset dataset\" width=\"400\"/>\n",
    "\n",
    "Audioset does not contain ultrasonic recordings, and thus is devoid of bat sounds. However, we expect the learnt features to be sufficiently general that it can help identify bat calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68854c3-ef99-4719-9bb4-0774f36226b4",
   "metadata": {},
   "source": [
    "First we need to download the model. Thankfully the model is available in [Tensorflow Hub](https://tfhub.dev/), a repository of pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12eb3bd-88f6-41b2-8df2-1f0d204ef26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will load the model using tensorflow_hub\n",
    "yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f54f0-d0d4-48d8-abf6-b6d16dddd5be",
   "metadata": {},
   "source": [
    "Next we will load all the audio bits from the dataset and extract the features using the model.\n",
    "\n",
    "The yamnet was originally trained with 0.96 second audio clips sampled at 16kHz. \n",
    "\n",
    "All our recordings have a samplerate of 441kHz so we will only feed the model with 34ms of audio at a time. \n",
    "\n",
    "We will center each audio clip on the bat calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81531304-22c4-4d01-b69b-ebe2b5e386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will load the information of where each bat call starts and ends\n",
    "audio_clips = pd.read_csv(DATA_DIR / \"yucatan\" / \"yucatan_species_clips.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2797d-b5a7-4ef9-a533-c816db9e30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for _, row in audio_clips.iterrows():\n",
    "    wav = load_bat_call_audio_data(\n",
    "        str(YUCATAN_AUDIO_DIR / row.recording_id), row.start_time, row.end_time\n",
    "    )\n",
    "    audios.append(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871a436-a5cd-442a-a8c4-1284e00b27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for wav in tqdm(audios):\n",
    "    scores, feats, spectrogram = yamnet(wav)\n",
    "    features.append(feats.numpy().squeeze())\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f028ad-35a5-4fae-93e4-95fccf6b7a10",
   "metadata": {},
   "source": [
    "Notice the yamnet model returns a set of 1024 features for each audio bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da7085-44fb-4121-a93f-19c6101ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8300d-2d82-4c64-b61d-000d263761dc",
   "metadata": {},
   "source": [
    "We can visualize the features using the dimensionality reduction technique\n",
    "explore in the previous practicals, [UMAP](https://umap-learn.readthedocs.io/en/latest/).\n",
    "\n",
    "We will compare tadarida's features with the features extracted by yamnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320ce59-1422-4bdf-bc72-374661bb81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_yamnet = UMAP().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09809281-e4f1-40ee-b145-cc6c1a1b1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = umap_yamnet.T\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X, y=Y, hue=audio_clips[\"class\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc29ee-3327-4726-9c0e-abff268f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tadarida = tadarida_features[TADARIDA_FEATURES]\n",
    "y_tadarida = tadarida_features[\"Class\"]\n",
    "\n",
    "umap_tadarida = UMAP().fit_transform(X_tadarida)\n",
    "X, Y = umap_tadarida.T\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X, y=Y, hue=y_tadarida);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d98d8c-167d-4fbc-b805-fcf0358d0bd1",
   "metadata": {},
   "source": [
    "Its hard to evaluate between the two feature sets by just these plots. In the next exercise we will train a classifier using the features extracted by Yamnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ae5a-a6ff-4b9a-bbaa-596b2244e662",
   "metadata": {},
   "source": [
    "### Exercise üêù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7d1c3-8f84-456a-a1be-874f0ec826bd",
   "metadata": {},
   "source": [
    "Here you will build a random forest classifier using the features extracted by Yamnet.\n",
    "\n",
    "1. Split the data into train and test. You can copy the code from the previous exercise.\n",
    "\n",
    "2. Train a random forest classifier using the training data.\n",
    "\n",
    "3. Evaluate the performance using the classification_report function.\n",
    "\n",
    "4. Plot the confusion matrix.\n",
    "\n",
    "5. Was this better? What are the advantages and disadvantages of using a\n",
    "   pre-trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e4b51-df78-4d7e-b47f-76f3d91554ce",
   "metadata": {},
   "source": [
    "### Exercise ü¶ê (Optional)\n",
    "\n",
    "Fine tune the Yamnet model the bat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4098688-e67b-497e-978d-47e1a1e2b475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "colab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
