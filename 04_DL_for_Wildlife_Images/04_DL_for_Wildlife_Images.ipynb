{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a\n",
    "href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/04_DL_for_Wildlife_Images/04_DL_for_Wildlife_Images.ipynb\"\n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In\n",
    "Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Deep Learning for Wildlife Images\n",
    "\n",
    "Last week we have seen how to design, train, and evaluate a deep learning model in PyTorch. Today,\n",
    "we will use that knowledge to make sense of wildlife images!\n",
    "\n",
    "This notebook will require a little bit less coding from your side, but it contains lots of\n",
    "information that is hopefully going to be useful for your own projects! Also, make sure you have\n",
    "gone through last week's exercise; we will re-use lots of concepts and code from it on training deep\n",
    "learning models again today.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Data: Camera Trap Images](#1-data-camera-trap-images)\n",
    "2. [Introduction to Convolutional Neural Networks](#2-introduction-to-convolutional-neural-networks)\n",
    "3. [CNNs for Camera Trap Imagery](#3-cnns-for-camera-trap-imagery)\n",
    "4. [Object Detection](#4-object-detection)\n",
    "5. [Summary](#5-summary)\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or\n",
    "answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data: Camera Trap Images\n",
    "\n",
    "Before we even talk about models, let us start with the data and problem we want to tackle: camera\n",
    "trap images.\n",
    "\n",
    "As you surely know, camera traps are little devices mounted in the wild (_e.g._, on a tree) that\n",
    "contain a camera, as well as a motion sensor:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/refs/heads/main/04_DL_for_Wildlife_Images/ct.png\" width=\"400\" />\n",
    "\n",
    "They automatically start taking sequences of images once the motion sensor gets triggered, such as\n",
    "by an animal passing in front of the camera trap.\n",
    "\n",
    "In this exercise, we will use parts of a curated dataset of camera trap images over North America,\n",
    "the [Caltech Camera Traps](https://lila.science/datasets/caltech-camera-traps) dataset.\n",
    "\n",
    "Let us import it from Google Drive:\n",
    "\n",
    "1. Open this link:\n",
    "   [https://drive.google.com/file/d/1LyLtfF4ke5yQCkJDMhXaULs_AME4Wn2H/view?usp=sharing](https://drive.google.com/file/d/1LyLtfF4ke5yQCkJDMhXaULs_AME4Wn2H/view?usp=sharing)\n",
    "2. Right-click the zip file, select \"Organise\" > \"Add shortcut\". Then, select \"My Drive\" and click\n",
    "   \"Add\".\n",
    "3. Run the following two code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qqn /content/drive/MyDrive/cct20_data_subset.zip -d /content\n",
    "\n",
    "DATA_FOLDER = '/content/cct20_data_subset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file browser in Google Colab (folder icon to the left) and take a look at the dataset\n",
    "(navigate to `/content/cct20_data_subset`).\n",
    "\n",
    "You will notice a few things:\n",
    "* It already comes divided into train/val/test splits (see Session 2 about that).\n",
    "* File `classes.txt` contains a list of species, line-by-line and in order.\n",
    "* The same species names are in subfolders called \"images\" under the train/val/test folders. This\n",
    "  basically tells us what type of animal each of the images shows therein ‚Äì we can use it as a\n",
    "  ground truth to train and test our model!\n",
    "* Don't bother with the \"labels\" subfolders for now, we will use those later.\n",
    "\n",
    "Let us visualise some random images from the `train/images` folder below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# find all images in training folder\n",
    "images = glob.glob(os.path.join(DATA_FOLDER, 'train', 'images', '**/*.jpg'), recursive=True)\n",
    "\n",
    "\n",
    "# show a few random images\n",
    "img_paths = np.random.choice(images, size=25, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for idx, img_path in enumerate(img_paths):\n",
    "    img = Image.open(img_path)\n",
    "    plt.subplot(5,5,idx+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(pathlib.Path(img_path).parent.name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think about labelling those images, or predicting them with a machine learning model, you\n",
    "should already now see some potential challenges:\n",
    "* Animals may be tiny, partially hidden behind bushes, blurred by movement.\n",
    "* Some images are in greyscale: these have been taken with infrared vision at night.\n",
    "* There is an \"empty\" class: as it turns out, motion sensors of camera traps can easily get\n",
    "  triggered by movement other than caused by animals, such as leaves rustling on a tree.\n",
    "\n",
    "\n",
    "We can also plot the histogram of the number of images per species in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of number of training images per class (species)\n",
    "train_folder = os.path.join(DATA_FOLDER, 'train', 'images')\n",
    "classes = [folder for folder in os.listdir(train_folder) \\\n",
    "           if os.path.isdir(os.path.join(train_folder, folder))]\n",
    "num_imgs = [len(os.listdir(os.path.join(DATA_FOLDER, 'train', 'images', species))) for species in classes]\n",
    "\n",
    "plt.bar(np.arange(len(classes)), num_imgs)\n",
    "_ = plt.xticks(np.arange(len(classes)), classes, rotation=90)\n",
    "plt.ylabel('Image count')\n",
    "\n",
    "print(f'Total number of images: {len(images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some species have extremely few images ‚Äì this **class imbalance** might cause a problem for a model.\n",
    "\n",
    "\n",
    "Nonetheless, we have everything we need to predict those species with a model:\n",
    "* Data/input features $\\mathbf{X}$: our images\n",
    "* Target/ground truth labels $\\mathbf{y}$: the species label\n",
    "* Train/val/test splits\n",
    "\n",
    "All we need to proceed is a model that can handle those images.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Convolutional Neural Networks\n",
    "\n",
    "Let us first briefly recap what we saw in the morning, and talk about Convolutional Neural Networks\n",
    "(CNNs).\n",
    "\n",
    "\n",
    "###¬†2.1 Convolution\n",
    "\n",
    "As you have seen, the key innovation of CNNs is the convolution operation: here, a filter matrix (or\n",
    "kernel) of weights is applied over locations on a regular grid. This is in contrast to the\n",
    "fully-connected layer, where each input tensor value is processed by a dedicated weight.\n",
    "\n",
    "<img src=\"https://tikz.net/janosh/conv2d.png\" width=\"500\" />\n",
    "\n",
    "([image source](https://tikz.net/janosh/conv2d.png))\n",
    "\n",
    "\n",
    "In the above example, our input tensor has $7\\times7=49$ values. Thus, we would have needed 49\n",
    "weight parameters for a linear/fully-connected layer, whereas the convolution layer only needed\n",
    "$3\\times3=9$ values. Of course, this difference becomes more significant the larger the inputs are\n",
    "(think about digital images).\n",
    "\n",
    "\n",
    "Like all other layers, such as Linear, ReLU, _etc._ we have seen last week, convolution layers\n",
    "are available via the [torch.nn](https://pytorch.org/docs/stable/nn.html) subpackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a convolution operation in any dimension ‚Äì we don't have to do it in an image plane,\n",
    "but could also do it across just one or three dimensions, for example.\n",
    "\n",
    "However, since we will be dealing with images below, we will make use of the\n",
    "[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) layer.\n",
    "\n",
    "Like nn.Linear, nn.Conv2d layers takes some basic input/output hyperparameters:\n",
    "* `in_channels`: number of input channels (\"depth\" of the input)\n",
    "* `out_channels`: number of output channels (number of convolution filters we will apply)\n",
    "\n",
    "Unlike nn.Linear, we can specify convolution operations with many other properties, such as kernel\n",
    "size, stride, padding, _etc._ See the documentation linked above for more information.\n",
    "\n",
    "Let us create a new 2D-convolution layer and try it out on a tensor below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# initialise new 2D convolution layer\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=8,\n",
    "                       kernel_size=5,\n",
    "                       stride=1,\n",
    "                       padding=0)\n",
    "\n",
    "\n",
    "# create a random input tensor\n",
    "input_tensor = torch.randn(size=(1, 3, 32, 32))         # BxCxHxW\n",
    "\n",
    "# apply convolution layer to input\n",
    "result = conv_layer(input_tensor)\n",
    "\n",
    "print(f'Output tensor size after convolution: {result.size()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, our `input_tensor` is of size `BxCxHxW`. If we assume this is an image, these dimensions\n",
    "would signify:\n",
    "* `B`: batch size (here: 1 = one image)\n",
    "* `C`: number of channels (here: 3 for red, green, blue)\n",
    "* `H`: image height\n",
    "* `W`: image width\n",
    "\n",
    "\n",
    "üí° PyTorch usually follows this ordering for image recognition models.\n",
    "\n",
    "\n",
    "As you can see, the output size behaves as follows:\n",
    "* The first dimension (batch size) still says 1 ‚Äì we feed one input, we get one output.\n",
    "* The second dimension changed from 3 to 8. Our input tensor had three channels. However, we\n",
    "  specified our `conv_layer` as having eight output channels. What happens in the background here is\n",
    "  that we get eight filter kernels, each of size `3x5x5`: since our input has three channels, each\n",
    "  filter must have a weight for each location _and_ channel. When convolving the input with the\n",
    "  filter, it still multiplies each filter weight with its appropriate input (spatial location _and_\n",
    "  channel), sums all results together, and returns the sum at the centre of the current location (as\n",
    "  in the figure above).\n",
    "* Sizes of the third and fourth dimension (height and width of the feature plane, such as the input\n",
    "  image) depend on the convolution hyperparameters (you have seen the formula in the lecture).\n",
    "\n",
    "\n",
    "üñåÔ∏è Change some of the `nn.Conv2d` hyperparameters (_e.g._, set `stride=2`) and observe the\n",
    "changes in output size they result in. Can you find values that do not work with our input tensor?\n",
    "Can you guess why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n",
    "The input of a convolution layer must at least be as large as the convolution kernel. As soon as the\n",
    "kernel is larger than the input, we get an error, because there are not enough input positions to\n",
    "calculate correlations with. To resolve this issue, `kernel_size` must be reduced or else `padding`\n",
    "increased (although adding too much padding, _e.g._ more than half the kernel size, is not sensible\n",
    "anymore at some point).\n",
    "\n",
    "Moreover, `in_channels` must be exactly the same as the input's number of channels.\n",
    "\n",
    "All of this applies to any layer in the model, and one therefore must be careful in designing it so\n",
    "that it can operate with _e.g._ the right type and size of images and intermediate tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pooling\n",
    "\n",
    "You have seen this one too in the lecture: pooling. Here, we aggregate the values locally, for\n",
    "example by taking the maximum or average, without learning specific parameters.\n",
    "\n",
    "Let's see this for ourselves on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tensor of specific values\n",
    "input_tensor = torch.tensor([[1, 5, 3, 5],\n",
    "                             [8, 2, 3, 3],\n",
    "                             [9, 0, 2, 1],\n",
    "                             [-1, 4, 2, 5]])\n",
    "\n",
    "# make sure the tensor is of correct size\n",
    "input_tensor = input_tensor.view((1, 1, 4, 4))      # again: BxCxHxW\n",
    "\n",
    "# create pooling layer\n",
    "max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "# apply\n",
    "result = max_pool(input_tensor)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to reproduce what we have seen in the lecture.\n",
    "\n",
    "üñåÔ∏è Try changing the kernel size, stride, adding padding, or switching the layer to `nn.AvgPool2d` and\n",
    "observe the output. You should be able to reproduce what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we can chain many convolution, pooling and other layers (_e.g._, ReLU) together to\n",
    "obtain a CNN:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/refs/heads/main/04_DL_for_Wildlife_Images/cnn.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "In the above illustration, you notice that the intermediate feature tensors output by each layer get\n",
    "smaller and smaller in spatial dimension, until they collapse to a linear, one-dimensional vector\n",
    "(output of \"Pooling 2\"). At this point, the model has summarised the whole image contents into this\n",
    "non-spatial vector, so we can add final linear/fully-connected layer that _e.g._ returns logits of\n",
    "the label classes the model is supposed to predict. Many CNNs for image classification do that.\n",
    "\n",
    "We will use one such architecture below ‚Äì you have seen it at the end of the lecture already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†2.3 ResNet\n",
    "\n",
    "There isn't a definitive recipe in designing models; oftentimes people just tried ideas. As a\n",
    "result, various different CNN architectures have been proposed over the years. These are the most\n",
    "noteworthy ones:\n",
    "* [LeNet](https://ieeexplore.ieee.org/abstract/document/726791): the \"original\" from 1998 by LeCun,\n",
    "  designed to classify handwritten digits in postal codes (the MNIST dataset).\n",
    "* [AlexNet](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html):\n",
    "  the architecture that started the Deep Learning hype in 2012.\n",
    "* [Inception](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html):\n",
    "  this employed the idea of intermediate classifiers (in the middle of the model) for better\n",
    "  training.\n",
    "* [ResNet](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html):\n",
    "  this added skip connections and enabled training very deep models, beating humans in accuracy on\n",
    "  ImageNet for the first time.\n",
    "\n",
    "\n",
    "There are others, including particularly lightweight architectures like\n",
    "[MobileNet](https://arxiv.org/abs/1704.04861) and [SqueezeNet](https://arxiv.org/abs/1602.07360)\n",
    "(for _e.g._ on-device computing, such as on a smartphone, drone, or camera trap), but the one we\n",
    "will be looking at below is ResNet.\n",
    "\n",
    "ResNet is so popular that comes built-in with PyTorch. We can call it from the\n",
    "[Torchvision](https://pytorch.org/vision/stable/index.html) sub-package, which, as the name\n",
    "suggests, implements all sorts of useful utilities for applying deep learning on computer vision.\n",
    "\n",
    "ResNet comes in different sizes. Let us create an instance of the smallest one below, ResNet-18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# create ResNet-18 model instance\n",
    "model = models.resnet18()\n",
    "\n",
    "# print the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see all the layers in order in the print output above. Compared to our model from last\n",
    "week, this is way bigger already! Also, you should be familiar with most of the layers listed:\n",
    "* `Conv2d` and `MaxPool2d`: we've just seen those above. Note the hyperparameters like\n",
    "  `kernel_size`.\n",
    "* `ReLU`: nonlinear activation function used by default in ResNet (and many other CNNs). You have\n",
    "  seen this last week.\n",
    "* `BatchNorm2d`: we have talked about this during the lecture. It helps training by learning to\n",
    "  normalise intermediate layer output tensors.\n",
    "* `AdaptiveAvgPool2d`: you may be able to imagine what this does.\n",
    "  [Here](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html) is the\n",
    "  documentation of it.\n",
    "* `Sequential`: this is just a `torch.nn` helper that applies a list of pre-defined layers in order;\n",
    "  nothing special.\n",
    "* `Linear` (last layer): this maps the penultimate, one-dimensional feature vector to 1000 output\n",
    "  class logits.\n",
    "\n",
    "\n",
    "Why 1000 outputs, you may ask? In the lecture, we talked about the\n",
    "[ImageNet](https://www.image-net.org/challenges/LSVRC/index.php) classification challenge, which\n",
    "jump-started CNNs. This has images of 1000 everyday categories that appear in images and that a\n",
    "model has to predict, hence the 1000 outputs. Those categories are listed in a default order, and we\n",
    "can download them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/xmartlabs/caffeflow/refs/heads/master/examples/imagenet/imagenet-classes.txt') as url:\n",
    "    imagenet_classes = url.read().decode('utf-8').split('\\n')\n",
    "\n",
    "imagenet_classes = [item for item in imagenet_classes if len(item) > 0]\n",
    "\n",
    "print(f'Number of ImageNet classes: {len(imagenet_classes)}')\n",
    "print(f'First five classes: {imagenet_classes[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNNs for Camera Trap Imagery\n",
    "\n",
    "### 3.1 From Images to Tensors\n",
    "\n",
    "Okay, let us use the above ResNet-18 model instance to predict an image from our camera trap\n",
    "dataset.\n",
    "\n",
    "Let's load a random image into memory. There are multiple libraries in Python to do so; here, we use\n",
    "the [Python Imaging Library (PIL)](https://pypi.org/project/pillow/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# select an image we want to load\n",
    "img_path = os.path.join(DATA_FOLDER, 'train/images/car/58c34ce5-23d2-11e8-a6a3-ec086b02610b.jpg')\n",
    "\n",
    "# load image\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# image properties\n",
    "print(f'Image width: {img.width}')\n",
    "print(f'Image height: {img.height}')\n",
    "\n",
    "# visualise\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an object `img` in memory. However, a PyTorch model requires a `torch.Tensor` as input,\n",
    "so we need to convert it to that. And it doesn't just stop there: it also needs to be of the right\n",
    "size, and have the right (normalised) pixel values.\n",
    "\n",
    "Torchvision implements various [Transforms](https://pytorch.org/vision/0.9/transforms.html) to help\n",
    "us with that. We can compose multiple of those together to perform all of these steps properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "# compose transforms together\n",
    "transforms = T.Compose([\n",
    "    T.Resize(size=[224, 224]),                  # ResNet requires images of size 224x224 pixels\n",
    "    T.ToTensor(),                               # convert from PIL to torch.Tensor\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])      #¬†z-score images by R, G, B band\n",
    "])\n",
    "\n",
    "# use it to transform PIL image\n",
    "img_tensor = transforms(img)\n",
    "\n",
    "print(f'Tensor size: {img_tensor.size()}')\n",
    "print(f'Tensor data type: {img_tensor.type()}')\n",
    "print(f'Tensor value min/max: {img_tensor.min()}/{img_tensor.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see below why we use such seemingly odd values to normalise our tensor.\n",
    "\n",
    "Let's make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very important: put model in evaluation mode so that BatchNorm & Co. work properly\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(img_tensor.unsqueeze(0))       #¬†unsqueeze: remember the first dimension needs to be our batch size\n",
    "\n",
    "print(f'Prediction size: {pred.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get our 1000 logits for all the ImageNet classes we have seen above. If we normalise them with\n",
    "the softmax, we get class probabilities; taking the $\\arg\\max$ then gives us the class index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = pred.softmax(dim=1)      # dim=1: along second (class) dimension\n",
    "\n",
    "confidence, y_hat = pred_softmax.max(1)\n",
    "\n",
    "# our predicted class now is the ImageNet category at position y_hat\n",
    "pred_class = imagenet_classes[y_hat]\n",
    "\n",
    "print(f'Prediction: {pred_class} ({confidence.item():.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's odd. The class is most certainly wrong, and the confidence might be really low, too.\n",
    "\n",
    "\n",
    "### 3.2 Pre-trained Models\n",
    "\n",
    "Remember how model parameters (weights and biases) are initialised randomly whenever we create a new\n",
    "layer? The same happened above when we defined our ResNet-18. Thus, this model is totally\n",
    "unoptimised and does not know how to make good predictions. Let's change that!\n",
    "\n",
    "Torchvision actually has model states available for us that are pre-trained on ImageNet, for free!\n",
    "All we have to do is initialise our model with an extra argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create our ResNet-18. Notice the \"weights\" argument\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run the above two code cells and watch what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transfer Learning\n",
    "\n",
    "Cool, we now get a more sensible prediction! However, this is just for images with cars, and the\n",
    "confidence isn't really good either. That model will likely fail if the image is too difficult, and\n",
    "will not know what to do for species it doesn't know about. Let's predict some random images from\n",
    "our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a few random images\n",
    "img_paths = np.random.choice(images, size=16, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for iidx, img_path in enumerate(img_paths):\n",
    "    # load image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # transform\n",
    "    img_tensor = transforms(img)\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor.unsqueeze(0))\n",
    "        pred_sm = pred.softmax(dim=1)\n",
    "        conf, y_hat = pred_sm.max(1)\n",
    "        pred_label = imagenet_classes[y_hat]\n",
    "        if len(pred_label) > 12:\n",
    "            # shorten label to fit image title\n",
    "            pred_label = pred_label[:9] + '...'\n",
    "\n",
    "        # get ground truth label from parent folder name\n",
    "        gt_label = pathlib.Path(img_path).parent.name\n",
    "\n",
    "        # visualise with prediction & ground truth\n",
    "        plt.subplot(4,4,iidx+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Pred: {pred_label} ({conf.item():.2%}), actual: {gt_label}')\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obviously need to train our model on our camera trap images to improve that. However, even a\n",
    "small ResNet-18 is actually quite big.\n",
    "\n",
    "Remember that the model's learning capacity can be expressed as its number of learnable parameters\n",
    "(weights and biases)? We can count these for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 0\n",
    "\n",
    "# iterate over our ResNet's layer parameters and accumulate their number of elements\n",
    "for param_tensor in model.parameters(recurse=True):\n",
    "    num_params += param_tensor.numel()\n",
    "\n",
    "print(f'Model {model.__class__.__name__} has {num_params} learnable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over eleven million parameters for the smallest ResNet model! Training such a model \"from scratch\"\n",
    "would be very hard for small datasets.\n",
    "\n",
    "Luckily, there is a trick we can pull off: in the lecture, we have seen that the first layers in a\n",
    "CNN learn more geometric features, such as edges and blobs, while the later ones learn more\n",
    "_semantic_ ones that are important for the task (such as object parts of classes we want to\n",
    "predict). The geometric ones are often similar across tasks.\n",
    "\n",
    "If we could \"warm-start\" the model as trained on a big dataset and then adapt it for our own one, we\n",
    "could save a lot of learning of these early parameters. That is what is known as **transfer\n",
    "learning**: we _transfer_ a learnt model from one task to another.\n",
    "\n",
    "As it turns out, using a model pre-trained on ImageNet is a very popular starting point for transfer\n",
    "learning. Let's visualise the convolution filter kernels for the very first layer of the model\n",
    "learnt on ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "\n",
    "\n",
    "filter_weight = model.conv1.weight\n",
    "n,c,w,h = filter_weight.size()\n",
    "\n",
    "rows = np.min((filter_weight.shape[0] // 8 + 1, 64))    \n",
    "grid = utils.make_grid(filter_weight, nrow=8, normalize=True, padding=2)\n",
    "plt.figure(figsize=(8,rows))\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of those filters extracts geometric features from the image that are then processed by all the\n",
    "remaining layers, eventually useful to distinguish 1000 object categories, better than humans do!\n",
    "\n",
    "However, that alone doesn't help us, because our dataset has different categories than the 1000\n",
    "above.\n",
    "\n",
    "To properly do transfer learning, we also need to adapt the model's output to reflect the number of\n",
    "label classes we have in our own dataset. To do so, we have to replace the trained final\n",
    "classification layer with a new, randomly initialised one. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# replace original fully-connected output layer with a new one of right output size\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our model is ready to predict the desired number of output classes for our camera traps\n",
    "dataset. It just hasn't been trained yet.\n",
    "\n",
    "\n",
    "### 3.4 Dataset Class, DataLoader\n",
    "\n",
    "Remember from last week how we need to load our images in batches? This time, they definitely won't\n",
    "all fit into memory anymore, so we absolutely need to define a Dataset class and use a DataLoader.\n",
    "\n",
    "Recap:\n",
    "* The [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) defines what\n",
    "  data points we have and provides functions `__len__()` (number of data points) and\n",
    "  `__getitem__(idx)` (load features and ground truth for data point at given index `idx`).\n",
    "* The [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) does all\n",
    "  the batching required for training and prediction.\n",
    "\n",
    "We can define our own Dataset class that takes care of the image and ground truth label loading\n",
    "below, just like we did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class CTImageDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_folder,\n",
    "                 split,\n",
    "                 transform):\n",
    "        # load label classes from classes.txt file in order\n",
    "        with open(os.path.join(data_folder, 'classes.txt'), 'r', encoding='utf-8') as f_cls:\n",
    "            classes = [line.strip() for line in f_cls.readlines() if len(line) > 0]\n",
    "        self.label_lut = dict(zip(classes, range(len(classes))))\n",
    "\n",
    "        # load all image paths under split folder\n",
    "        self.images = glob.glob(os.path.join(data_folder, split, 'images', '**/*.jpg'))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load image at given index\n",
    "        img_path = self.images[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # apply transform\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        # ground truth label: name of parent folder\n",
    "        label = pathlib.Path(img_path).parent.name\n",
    "\n",
    "        # ground truth label ordinal: from lookup table\n",
    "        label_ordinal = self.label_lut[label]\n",
    "\n",
    "        return img_tensor, label_ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create dataset and DataLoader instances for _e.g._ our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# create training dataset instance\n",
    "dataset_train = CTImageDataset(DATA_FOLDER,\n",
    "                               'train',\n",
    "                               transforms)\n",
    "\n",
    "# DataLoader\n",
    "dl_train = DataLoader(dataset_train,\n",
    "                      batch_size=8,\n",
    "                      shuffle=True)\n",
    "\n",
    "# let's test it\n",
    "for X, y in dl_train:\n",
    "    print(f'Input tensor size: {X.size()}')\n",
    "    print(f'Label tensor size: {y.size()}')\n",
    "    print(f'Label tensor values:\\n{y}')\n",
    "    break       # stop after the first iteration for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get as output:\n",
    "* Input tensor $X$ of size `BxCxHxW`\n",
    "* Target label tensor $y$ of size `B`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†3.5 Model Training\n",
    "\n",
    "All the rest is, initially, exactly the same as last week: we need an optimiser, loss function\n",
    "(criterion), a training loop over epochs, accuracy calculation, _etc._.\n",
    "\n",
    "Let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm                           # this gives us a handy progress bar\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 0.001\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "\n",
    "# this is just for plotting\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "def plot_vals(loss_train, loss_val, accuracy_train, accuracy_val):\n",
    "    clear_output(wait=True)\n",
    "    epochs = np.arange(1, len(loss_train)+1, dtype=int)\n",
    "    plt.clf()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, loss_train, 'b-')\n",
    "    plt.plot(epochs, loss_val, 'r-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, accuracy_train, 'b-', label='train')\n",
    "    plt.plot(epochs, accuracy_val, 'r-', label='val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# create dataset and DataLoader instances for the training and validation splits\n",
    "dataset_train = CTImageDataset(DATA_FOLDER,\n",
    "                               'train',\n",
    "                               transforms)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0)\n",
    "\n",
    "dataset_val = CTImageDataset(DATA_FOLDER,\n",
    "                             'val',\n",
    "                             transforms)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "# create model and adapt for our purposes\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=len(dataset_train.label_lut))\n",
    "\n",
    "# create optimiser for model we defined above\n",
    "optimiser = SGD(model.parameters(),\n",
    "                lr=LEARNING_RATE,\n",
    "                weight_decay=WEIGHT_DECAY,\n",
    "                momentum=MOMENTUM)\n",
    "\n",
    "# loss function/criterion: multi-class cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# keep track of loss and accuracy values over epochs\n",
    "loss_train, accuracy_train = [], []\n",
    "loss_val, accuracy_val = [], []\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # train\n",
    "    model.train()\n",
    "\n",
    "    loss_train_epoch, accuracy_train_epoch = 0.0, 0.0\n",
    "\n",
    "    with tqdm(dataloader_train) as progress_bar:\n",
    "        for idx, (data, target) in enumerate(dataloader_train):\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, target)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loss_train_epoch += loss.item()\n",
    "            accuracy_train_epoch += torch.mean((pred.argmax(1) == target).float()).item()\n",
    "\n",
    "            progress_bar.set_description(\n",
    "                f'[Epoch {epoch}/{NUM_EPOCHS} train] Loss: {loss_train_epoch/(idx+1):.2f}, ' + \\\n",
    "                    f'Accuracy: {accuracy_train_epoch/(idx+1):.2%}')\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        loss_train.append(loss_train_epoch / len(dataloader_train))\n",
    "        accuracy_train.append(accuracy_train_epoch / len(dataloader_train))\n",
    "\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "\n",
    "    loss_val_epoch, accuracy_val_epoch = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader_val) as progress_bar:\n",
    "            for idx, (data, target) in enumerate(dataloader_val):\n",
    "                pred = model(data)\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "                loss_val_epoch += loss.item()\n",
    "                accuracy_val_epoch += torch.mean((pred.argmax(1) == target).float()).item()\n",
    "\n",
    "                progress_bar.set_description(\n",
    "                    f'[Epoch {epoch}/{NUM_EPOCHS}   val] Loss: {loss_val_epoch/(idx+1):.2f}, ' + \\\n",
    "                        f'Accuracy: {accuracy_val_epoch/(idx+1):.2%}')\n",
    "                progress_bar.update(1)\n",
    "            \n",
    "            loss_val.append(loss_val_epoch / len(dataloader_val))\n",
    "            accuracy_val.append(accuracy_val_epoch / len(dataloader_val))\n",
    "    \n",
    "    # plot loss and accuracy curves after each epoch\n",
    "    plot_vals(loss_train, loss_val, accuracy_train, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a good moment to go through the above code block. Much of it should be familiar to you ‚Äì you\n",
    "have seen all the relevant parts in the exercise last week.\n",
    "\n",
    "Then, go ahead and execute the cell. While it is running, read the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†3.6 GPUs, TPUs and Parallel Accelerators\n",
    "\n",
    "As you run the model training loop above, you notice one primary problem: it is _extremely slow_.\n",
    "Now we really can feel the \"big\" images and eleven million parameters of our model; the computer in\n",
    "the Google Colab cloud has a lot to calculate.\n",
    "\n",
    "We now finally get to see what parallel computing hardware can do for us! One of the most common\n",
    "(and traditional) devices to do this is the **Graphics Processing Unit (GPU)**. As the name\n",
    "suggests, a GPU actually is a graphics card to draw an image on your screen. However, through a\n",
    "library called [CUDA](https://developer.nvidia.com/cuda-toolkit), we can use it to perform\n",
    "non-graphical computations, too.\n",
    "\n",
    "\n",
    "The first step is to make a GPU available:\n",
    "\n",
    "üñåÔ∏è In the menu bar at the top of the browser window, click \"Runtime\" > \"Change Runtime Type\".\n",
    "Choose an available GPU option (_e.g._, \"T4 GPU\").\n",
    "\n",
    "We can now use a GPU to perform calculations. However, PyTorch doesn't automatically do this; by\n",
    "default, it still only uses the Central Processing Unit (CPU).\n",
    "\n",
    "Instead, we need to tell PyTorch to perform computations on the GPU. This includes moving tensors\n",
    "over to the correct device. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create a new random tensor\n",
    "my_tensor = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "# here's the device the tensor is currently on\n",
    "print(f'Tensor device: {my_tensor.device}')\n",
    "\n",
    "# we can move the tensor to a CUDA-capable GPU. Let's first test whether CUDA is available:\n",
    "print(f'CUDA is available: {torch.cuda.is_available()}')\n",
    "print(f'Number of CUDA devices (GPUs): {torch.cuda.device_count()}')\n",
    "\n",
    "# if we have a CUDA-capable GPU available, we can move the tensor over:\n",
    "DEVICE = 'cuda'\n",
    "my_tensor = my_tensor.to(DEVICE)\n",
    "print(f'Tensor device: {my_tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run computations on the GPU, we need to make sure all involved tensors, including the model's\n",
    "parameters, are on the exact same device. Therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also move model over to the right device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    pred = model(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once we are done, we have to move tensors back to the system memory (\"CPU\"). This is\n",
    "important if we want to do anything with them outside PyTorch (_e.g._, visualisation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move prediction over to main memory (CPU)\n",
    "pred = pred.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this above!\n",
    "\n",
    "üñåÔ∏è Go back up to the main model training routine and implement this idea: modify the code so that\n",
    "the model, data and label tensors (`X`, `y`) are on the \"cuda\" device. Make sure to modify both\n",
    "training and validation blocks. Then, execute the code cell and observe the change in speed.\n",
    "\n",
    "üí° Tips:\n",
    "- Add an extra hyperparameter `DEVICE = \"cuda\"` at the top and use it (_e.g._, `model =\n",
    "  model.to(DEVICE)`).\n",
    "- If you call `.item()` on a (single-valued) Tensor, you don't need to call `.cpu()` first.\n",
    "- CUDA GPUs aren't the only type of accelerator. For example, if you have a recent Macintosh with\n",
    "  Apple Silicon CPU (Apple M1 or later), try the\n",
    "  [MPS](https://pytorch.org/docs/stable/notes/mps.html) backend ‚Äì you may be surprised what it can\n",
    "  do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical background on parallel accelerators**\n",
    "\n",
    "_Note: this section provides a high-level background on what parallel accelerators do\n",
    "under the bonnet. You can skip it if you are not interested in how this all works technically._\n",
    "\n",
    "The central processing unit (CPU) in a computer is very powerful and versatile: it can be programmed\n",
    "to execute very long chains of operations that ultimately run the code you implement. However, this\n",
    "versatility comes at a cost: CPUs are not designed to run many operations in parallel. Sure, modern\n",
    "CPUs have multiple cores (resp. threads) that can do chunks of work in parallel, but often just a\n",
    "low number of those.\n",
    "\n",
    "In deep learning, however, we often need to compute _the exact same operation_ for many values: for\n",
    "example, if you apply a fully-connected layer to a tensor of 1000 elements, you need to perform the\n",
    "same operation (multiplication) 1000 times. In simplified terms, a CPU will have to do those\n",
    "sequentially, one-by-one, in a for-loop. That takes 1000 compute cycles, which is expensive.\n",
    "\n",
    "This is where parallel accelerators like GPUs come in. These are the opposite of CPUs in some form:\n",
    "they only offer very shallow processing pipelines, that is, you cannot program them to compute lots\n",
    "of operations one after the other in a single run. However, you can tell them to _e.g._ do a\n",
    "multiplication ‚Äì but across many thousand cores at once and in parallel. Here is an image showing\n",
    "the architecture of a CUDA GPU:\n",
    "\n",
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2018/09/image2.jpg\" width=\"800\" />\n",
    "\n",
    "It might be difficult to see, but each of these green rectangles contains many operation cores. You\n",
    "can program them to receive individual values each, but all perform the same operation in parallel.\n",
    "For example, the NVIDIA RTX 5090 has over 24,000 CUDA cores\n",
    "([source](https://www.tomshardware.com/pc-components/gpus/rtx-5090-prototype-allegedly-has-24-576-cuda-cores-and-800w-tdp-two-16-pin-connectors-present)),\n",
    "which means that you could perform as many multiplications as those in a single compute cycle! Now\n",
    "you can see why GPUs & Co. are so useful for deep learning: pretty much every operation can be\n",
    "computed in isolation for each value in a tensor, be it linear transformation, nonlinear activation,\n",
    "convolution, pooling, _etc._\n",
    "\n",
    "Finally, parallel accelerators like GPUs usually have their own memory, because system RAM isn't\n",
    "fast enough. Hence, we need to copy over the tensors to the right device first, hence the\n",
    "`.to(DEVICE)` calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Data Augmentation\n",
    "\n",
    "Above, we talked about transforms. Torchvision has a lot more of those implemented\n",
    "([documentation](https://pytorch.org/vision/0.9/transforms.html)). You will notice that some of them\n",
    "do rather interesting things, often randomly. Why?\n",
    "\n",
    "If you think about it, we could _e.g._ horizontally flip _any_ of our camera trap images and they\n",
    "still would show us the exact same object category: a car is a car, whether it's turned left-right\n",
    "or right-left. Likewise, some of our images are in greyscale because of night vision; we could just\n",
    "as well turn colour images greyscale and they would still show the same thing.\n",
    "\n",
    "We can use this information to **augment** our training images. By randomly applying such data\n",
    "augmentation transforms to an image, we can artificially expand the complexity of the data that our\n",
    "model sees. As a result, the model learns to be more invariant to _e.g._ left-right orientations,\n",
    "colour jitterings, random greyscale, _etc._ ‚Äì it becomes better and more robust!\n",
    "\n",
    "üñåÔ∏è Implement data augmentation above by altering the `transform` the datasets receive as argument.\n",
    "Then, run the code cell to train the model.\n",
    "\n",
    "**Notes:**\n",
    "- Only apply random augmentations to the training images, not the validation or test (can you figure\n",
    "  out why?).\n",
    "- Think carefully which augmentations you want to use, and with which hyperparameters. For example,\n",
    "  a random _vertical_ image flip makes no sense for a camera trap image (having the sky at the\n",
    "  bottom is unrealistic), but could be useful for _e.g._ remote sensing imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model at current epoch if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    'model': model.state_dict(),        # parameters\n",
    "    'loss_train': loss_train,           # we'll also save training stats to identify the model later\n",
    "    'loss_val': loss_val,\n",
    "    'accuracy_train': accuracy_train,\n",
    "    'accuracy_val': accuracy_val\n",
    "}\n",
    "\n",
    "with open(f'model_epoch_{epoch}.pt', 'wb') as f_out:\n",
    "    torch.save(model_data, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use it to predict our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = CTImageDataset(DATA_FOLDER,\n",
    "                              'test',\n",
    "                              transforms)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "\n",
    "\n",
    "# put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "loss_test_epoch, accuracy_test_epoch = 0.0, 0.0\n",
    "\n",
    "# let's store predictions and ground truth for accuracy evaluation later\n",
    "predictions_test, groundtruth_test = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(dataloader_test) as progress_bar:\n",
    "        for idx, (data, target) in enumerate(dataloader_test):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            loss_test_epoch += loss.item()\n",
    "            accuracy_test_epoch += torch.mean((pred.argmax(1) == target).float()).item()\n",
    "\n",
    "            predictions_test.append(pred.cpu())\n",
    "            groundtruth_test.append(target.cpu())\n",
    "\n",
    "            progress_bar.set_description(\n",
    "                f'[Epoch {epoch+1}/{NUM_EPOCHS}  test] Loss: {loss_test_epoch/(idx+1):.2f}, ' + \\\n",
    "                    f'Accuracy: {accuracy_test_epoch/(idx+1):.2%}')\n",
    "            progress_bar.update(1)\n",
    "\n",
    "\n",
    "predictions_test, groundtruth_test = torch.cat(predictions_test, 0), torch.cat(groundtruth_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well your model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=groundtruth_test.numpy(),\n",
    "                                        y_pred=predictions_test.argmax(1).numpy(),\n",
    "                                        ax=plt.gca())\n",
    "_ = plt.yticks(range(len(classes)), classes)\n",
    "_ = plt.xticks(range(len(classes)), classes, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Bonus: Deep Features\n",
    "\n",
    "_Please check out this part last (after Section 3) and only if you have time._\n",
    "\n",
    "If we go back to the architecture diagram of our CNN, we can see that it uses convolution + pooling\n",
    "layers to condense all spatial information down into a one-dimensional feature vector, and then\n",
    "employs a final, fully-connected layer to make class predictions. We have also seen that those\n",
    "intermediate output features change their significance: the first ones are more geometric (blobs,\n",
    "gradients), but as we go through the layers, they become more _semantic_, useful for classifying\n",
    "images.\n",
    "\n",
    "Thus, this second-to-last, one-dimensional feature vector just before the final layer is often very\n",
    "interesting, because it summarises the image contents non-spatially. We can use this for many things\n",
    "in deep learning, one of which is to see how well the model manages to separate images by label\n",
    "class.\n",
    "\n",
    "Let us first extract those feature vectors from the validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, feature_vectors = [], [], []\n",
    "\n",
    "# this allows us to capture the final, linear layer's input (i.e., our feature vector)\n",
    "def store_feature(module, input, output):\n",
    "    global feature_vectors\n",
    "    feature_vectors.append(input[0].cpu())\n",
    "\n",
    "hook = None\n",
    "if len(model.fc._forward_hooks) == 0:\n",
    "    hook = model.fc.register_forward_hook(store_feature)\n",
    "\n",
    "# iterate over validation set\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(dataloader_val):\n",
    "        labels.append(y)\n",
    "        pred = model(X.to(device))\n",
    "        y_hat = pred.argmax(dim=1)\n",
    "        predictions.append(y_hat.cpu())\n",
    "\n",
    "# combine our outputs\n",
    "predictions = torch.cat(predictions, 0)\n",
    "labels = torch.cat(labels, 0)\n",
    "feature_vectors = torch.cat(feature_vectors, 0)\n",
    "\n",
    "# clean up\n",
    "if hook is not None:\n",
    "    hook.remove()\n",
    "\n",
    "print(f'Size of tensor with combined feature vectors: {feature_vectors.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18 produces a 512-dimensional feature vector before the last layer, and we have one for each\n",
    "validation image (hence this specific size). So what can we do with this now?\n",
    "\n",
    "Remember dimensionality reduction methods from Session 2, such as PCA, t-SNE and UMAP? Let's apply\n",
    "one of those to compress the 512 dimensions down to two, and plot the points coloured by their\n",
    "ground truth label accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_embedded = tsne.fit_transform(feature_vectors.numpy())\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, 'classes.txt'), 'r', encoding='utf-8') as f_cls:\n",
    "    classes = [line.strip() for line in f_cls.readlines() if len(line) > 0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], c=labels, cmap='tab20')\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticks(range(len(classes)), classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†4. Object Detection\n",
    "\n",
    "Above, we used a CNN to classify camera trap images based on the animal species they showed. This\n",
    "can be useful, but also has its limitations:\n",
    "* Some of the animals can be really hard to spot because they may occupy only a very small portion\n",
    "  of the image. An image-level classifier might not be very helpful for those cases.\n",
    "* Worse, it has no means of identifying two or more animals (from different species) occurring in\n",
    "  the same image.\n",
    "\n",
    "\n",
    "This is where object detection comes in. Detection consists in predicting _bounding boxes_ for every\n",
    "_instance_ of an object category in an image. An object detection model thus has to return the\n",
    "following information for every image:\n",
    "* A list (tensor, _etc._) of $N\\times4$ values, denoting bounding box coordinates (_e.g._, `[x, y,\n",
    "  width, height]`) for $N$ object instances.\n",
    "* A list (tensor, _etc._) of $N\\times C$ logits, denoting model outputs for all $C$ categories\n",
    "  (classes) for each of the $N$ object instances.\n",
    "\n",
    "\n",
    "### 4.1 Manual Image Annotation\n",
    "\n",
    "The idea above provides us with a lot more information about the image contents, but it comes at a\n",
    "cost: it is much more expensive to manually annotate, because one has to draw bounding boxes _and_\n",
    "assign the correct species for every individual in an image.\n",
    "\n",
    "üñåÔ∏è Let us see this for real. Execute the code block below. You will be presented with an image.\n",
    "Your task is to draw bounding boxes around all animals you see in the image. To do so, select the\n",
    "right species from the list and click and drag to draw a box. Once you are finished, click \"Next\"\n",
    "(or \"Skip\" if there's no animal in the image) and continue with the next image. Note that the time\n",
    "required for you to label each image will be recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# load label class order for ground truth\n",
    "with open(os.path.join(DATA_FOLDER, 'classes.txt'), 'r', encoding='utf-8') as f_classes:\n",
    "    box_classes = [line.strip() for line in f_classes.readlines() if len(line.strip()) > 0]\n",
    "\n",
    "\n",
    "#TODO\n",
    "images_subset = np.random.choice(images, 15, replace=False)     #TODO\n",
    "\n",
    "# outputs\n",
    "times, boxes_user = [], []\n",
    "\n",
    "\n",
    "w_progress = widgets.IntProgress(value=0, max=len(images_subset)-1, description='Progress')\n",
    "\n",
    "w_boxes = BBoxWidget(\n",
    "    image=images_subset[0],\n",
    "    classes=box_classes,\n",
    ")\n",
    "w_container = widgets.VBox([\n",
    "    w_progress,\n",
    "    w_boxes,\n",
    "])\n",
    "\n",
    "# start recording time\n",
    "tic = time.time()\n",
    "\n",
    "def next():\n",
    "    global tic\n",
    "    if w_progress.value >= len(images_subset)-1:\n",
    "        # end reached\n",
    "        w_container.close()\n",
    "        finish()\n",
    "        return\n",
    "\n",
    "    # go to next image and reset properties\n",
    "    w_progress.value += 1\n",
    "    image_file = images_subset[w_progress.value]\n",
    "    w_boxes.image = image_file\n",
    "    w_boxes.bboxes = []\n",
    "\n",
    "    # reset timer\n",
    "    tic = time.time()\n",
    "\n",
    "\n",
    "@w_boxes.on_skip\n",
    "def skip():\n",
    "    global tic\n",
    "    # append time and empty lists for labels and boxes\n",
    "    times.append(time.time() - tic)\n",
    "    boxes_user.append([])\n",
    "\n",
    "    next()\n",
    "\n",
    "@w_boxes.on_submit\n",
    "def submit():\n",
    "    global tic\n",
    "    # append time and lists for labels and boxes\n",
    "    times.append(time.time() - tic)\n",
    "    boxes_user.append(w_boxes.bboxes)\n",
    "\n",
    "    next()\n",
    "\n",
    "\n",
    "def finish():\n",
    "    # show times required (TODO)\n",
    "    plt.bar(np.arange(1, len(times)+1), times)\n",
    "    plt.xlabel('Image number')\n",
    "    plt.ylabel('Time [s]')\n",
    "    plt.title('Time required to annotate')\n",
    "    plt.show()\n",
    "\n",
    "w_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have finished, you should see a bar chart that shows you how much time you needed for each\n",
    "image to annotate. As you can see, this is a rather time-consuming endeavour.\n",
    "\n",
    "Also, you may have found it rather difficult to label the images: some of the animals are really\n",
    "tiny, some are blurry, and some may be hard to identify. Imagine distinguishing between two related\n",
    "and visually similar species ‚Äì that's hard and requires lots of taxonomic knowledge!\n",
    "\n",
    "Let's compare your annotations to those made by experts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def draw_side_by_side(images,\n",
    "                      classes_a,\n",
    "                      classes_b,\n",
    "                      boxes_a,\n",
    "                      boxes_b,\n",
    "                      title_a,\n",
    "                      title_b):\n",
    "\n",
    "    w_progress = widgets.IntProgress(value=0, max=len(images)-1, description='Image')\n",
    "    w_boxes_a = BBoxWidget(\n",
    "        classes=classes_a,\n",
    "        view_only=True,\n",
    "        hide_buttons=True\n",
    "    )\n",
    "    w_boxes_b = BBoxWidget(\n",
    "        classes=classes_b,\n",
    "        view_only=True,\n",
    "        hide_buttons=True\n",
    "    )\n",
    "\n",
    "    def draw_image():\n",
    "        img_path = images[w_progress.value]\n",
    "        w_boxes_a.image = img_path\n",
    "        w_boxes_b.image = img_path\n",
    "\n",
    "        w_boxes_a.bboxes = boxes_a[w_progress.value]\n",
    "        w_boxes_b.bboxes = boxes_b[w_progress.value]\n",
    "\n",
    "    def prev(event=None):\n",
    "        w_progress.value -= 1\n",
    "        draw_image()\n",
    "\n",
    "    def next(event=None):\n",
    "        w_progress.value += 1\n",
    "        draw_image()\n",
    "\n",
    "    btn_prev = widgets.Button(description='<')\n",
    "    btn_prev.on_click(prev)\n",
    "\n",
    "    btn_next = widgets.Button(description='>')\n",
    "    btn_next.on_click(next)\n",
    "\n",
    "    draw_image()\n",
    "\n",
    "    output = widgets.VBox([\n",
    "        widgets.HBox([btn_prev, btn_next]),\n",
    "        w_progress,\n",
    "        widgets.HBox([widgets.HTML(f'{title_a} (left),'), widgets.HTML(f'{title_b} (right)')]),\n",
    "        widgets.HBox([w_boxes_a, w_boxes_b])\n",
    "    ])\n",
    "    display(output)\n",
    "\n",
    "\n",
    "# load ground truth boxes for the sample images\n",
    "gt_boxes = []\n",
    "for img_path in images_subset:\n",
    "    anno_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt')\n",
    "    if not os.path.exists(anno_path):\n",
    "        gt_boxes.append([])\n",
    "        continue\n",
    "    with open(anno_path, 'r', encoding='utf-8') as f_anno:\n",
    "        lines = f_anno.readlines()\n",
    "    tokens = [line.strip().split(' ') for line in lines]\n",
    "    boxes = torch.tensor([[float(val) for val in box[1:]] for box in tokens])\n",
    "\n",
    "    # convert to absolute xywh format\n",
    "    boxes *= torch.tensor([[1024, 768]]).repeat((1, 2))\n",
    "\n",
    "    labels = [box_classes[int(box[0])] for box in tokens]\n",
    "\n",
    "    bboxes = [{'x': box[0], 'y': box[1], 'width': box[2], 'height': box[3], 'label': labels[bidx]}\n",
    "            for bidx, box in enumerate(boxes.numpy().tolist())]\n",
    "    gt_boxes.append(bboxes)\n",
    "\n",
    "\n",
    "# display yours along with ground truth boxes side-by-side\n",
    "draw_side_by_side(images_subset,\n",
    "                  [],\n",
    "                  [],\n",
    "                  gt_boxes,\n",
    "                  boxes_user,\n",
    "                  'Ground Truth',\n",
    "                  'Your Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MegaDetector\n",
    "\n",
    "One of the most famous animal detection model for camera trap images is\n",
    "[MegaDetector](https://arxiv.org/abs/1907.06772). It nowadays is implemented in two repositories:\n",
    "- [MegaDetector](https://github.com/agentmorris/MegaDetector): original implementation\n",
    "- [PytorchWildlife](https://github.com/microsoft/CameraTraps): new fork\n",
    "\n",
    "Below, we will use the PytorchWildlife implementation and apply a pre-trained MegaDetector on our\n",
    "images. First, we need to install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PytorchWildlife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the\n",
    "[documentation](https://github.com/microsoft/CameraTraps?tab=readme-ov-file#mag-model-zoo-and-release-schedules),\n",
    "you can see that PytorchWildlife comes with lots of pre-trained versions built-in (\"Model Zoo\").\n",
    "Let's load one of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PytorchWildlife.models import detection as pw_detection\n",
    "detection_model = pw_detection.MegaDetectorV6(version='MDV6-yolov10-e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what that model returns for a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = np.random.choice(images_subset, size=1)[0]\n",
    "\n",
    "result = detection_model.single_image_detection(img_path,\n",
    "                                                det_conf_thres=0.2)\n",
    "\n",
    "print(f'MegaDetector output for image \"{img_path}\":')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° All the work on image loading, transforms, normalisation, _etc._, is already built into the\n",
    "library for you and happening deep within the framework.\n",
    "\n",
    "Let us see what this looks like for all our images when visualised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# predict all images\n",
    "detections = []\n",
    "for img_path in images_subset:\n",
    "    result = detection_model.single_image_detection(img_path,\n",
    "                                                    det_conf_thres=0.2)\n",
    "    bboxes = result['detections'].xyxy\n",
    "    detections.append([{\n",
    "        'x': box[0],\n",
    "        'y': box[1],\n",
    "        'width': box[2]-box[0],\n",
    "        'height': box[3]-box[1],\n",
    "        'label': result['labels'][bidx]\n",
    "    } for bidx, box in enumerate(bboxes)])\n",
    "\n",
    "\n",
    "# visualise\n",
    "draw_side_by_side(images_subset,\n",
    "                  [],\n",
    "                  [],\n",
    "                  detections,\n",
    "                  gt_boxes,\n",
    "                  'MegaDetector predictions',\n",
    "                  'Ground Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, MegaDetector does not know anything about our species and only predicts \"animal\" (as well\n",
    "as \"human\" and \"car\").\n",
    "\n",
    "However, this should make you think about how you could create a more powerful model by _e.g._\n",
    "stringing MegaDetector and our ResNet-18 classifier from above together:\n",
    "1. Predict animals with MegaDetector\n",
    "2. Classify each bounding box for its species\n",
    "\n",
    "This could also improve the (currently rather weak) performance of our classifier, since it would\n",
    "only receive the part of the image that actually contains the animal!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "We have seen various things today:\n",
    "* Basic working principles of a Convolutional Neural Network (CNN).\n",
    "* How to perform computer vision prediction tasks with deep learning, including image classification\n",
    "  and object detection.\n",
    "* All of that applied to camera trap species classification and prediction, including challenges\n",
    "  (tedium of manual annotation, difficulty for a model to do the task) and potential improvements\n",
    "  (detection followed by classification).\n",
    "\n",
    "You can imagine that a massive amount of research has gone into improving such models (and still is\n",
    "ongoing). We cannot cover everything in this module, but you should be able to see where this can\n",
    "go.\n",
    "\n",
    "Ultimately, ecological research does not stop at the prediction of species and individuals in an\n",
    "image, but then uses this information to make _inference_ about biotic processes in space and time,\n",
    "such as:\n",
    "* Prediction of habitat suitability (just like we did last week, but using species observations\n",
    "  identified in camera trap imagery).\n",
    "* Assessment of predator-prey interactions in space and time (for example, you may find that rabbits\n",
    "  and wolves have different diurnal times of activity, or else spatial territoriality).\n",
    "* Other behavioural traits (_e.g._, observing the habits of a mother bear walking by with offspring\n",
    "  cubs).\n",
    "* Population counts (we can do this with occupancy models, which we will take a look at in the\n",
    "  second half of the module).\n",
    "\n",
    "You can see that lots of ecological research questions can be informed with large-scale sensor and\n",
    "data inputs, such as from camera traps. Using machine learning to predict species in their imagery\n",
    "makes sense to increase the scale of prediction ‚Äì rather than labelling a few images by hand, you\n",
    "can now use the predictions across millions of images in space and time!\n",
    "\n",
    "As is often the case in ecology, however, we are most interested in the _rare_ class, such as the\n",
    "data-deficient species. As you have seen above, deep learning does not cope well with those. Thus,\n",
    "there is still lots of research to be done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
