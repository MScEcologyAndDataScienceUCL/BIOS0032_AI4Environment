{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/03_Deep_Learning/03_Deep_Learning_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: PyTorch and Deep Learning\n",
    "\n",
    "This week, we will dip our toes into the world of deep learning!\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction to PyTorch](#1-introduction-to-pytorch)\n",
    "2. [Gradients and Autograd](#2-gradients-and-autograd)\n",
    "3. [From Functions to Layers](#3-from-functions-to-layers)\n",
    "4. [From Layers to Neural Networks](#4-from-layers-to-neural-networks)\n",
    "5. [Loss Function](#5-loss-function)\n",
    "6. [Model Loading and Saving](#6-model-loading-and-saving)\n",
    "7. [Exercise: Species Distribution Modelling with Deep Learning](#7-exercise-species-distribution-modelling-with-deep-learning)\n",
    "8. [Summary](#8-summary)\n",
    "\n",
    "\n",
    "\n",
    "## Background on Deep Learning Libraries\n",
    "\n",
    "Like for other machine learning models, there exist a number of libraries that allow you to\n",
    "implement deep learning models easily. For Python, some of the most popular ones are:\n",
    "1. [PyTorch](https://pytorch.org/) (by Meta/facebook; we will use it in all of this course)\n",
    "2. [TensorFlow](https://www.tensorflow.org/) (by Google)\n",
    "3. [Jax](https://jax.readthedocs.io/en/latest/) (an ecosystem that supports deep learning frameworks\n",
    "   among other mathematical implementations)\n",
    "4. [Keras](https://keras.io/) (not a dedicated deep learning library itself, but a high-level layer\n",
    "   on top of PyTorch, TensorFlow or Jax)\n",
    "\n",
    "There are many others, but not all are still maintained (or as professionally done as these). Since\n",
    "deep learning is a very fast moving idea, it is highly recommended to use proper and up-to-date\n",
    "frameworks like the ones mentioned above. These will allow you to do anything reliably, from\n",
    "learning over prototyping and doing research to implementing deep learning-based apps for your\n",
    "computer or smartphone.\n",
    "\n",
    "For completeness, the following deep learning libraries have been _deprecated_ and should not be\n",
    "used anymore: Apache MXNet, Microsoft CMTK, Caffe, Theano.\n",
    "\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "- If a line starts with the fountain pen symbol (ðŸ–Œï¸), it asks you to implement a code part or\n",
    "answer a question.\n",
    "- Lines starting with the light bulb symbol (ðŸ’¡) provide important information or tips and tricks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PyTorch\n",
    "\n",
    "\n",
    "PyTorch should come pre-installed in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch (`torch` base package in Python) is primarily a deep learning library these days but can also be used for general mathematical operations, similar to NumPy.\n",
    "\n",
    "It actually builds on NumPy's `ndarray` and introduces a new type of multidimensional matrix, called _tensor_.\n",
    "In deep learning language, a tensor is a multi-dimensional array of a single data type (integer, float, _etc._) that holds deep learning model inputs, calculations, and outputs ([PyTorch documentation](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)).\n",
    "\n",
    "We will see later why PyTorch introduces a new data type, but for the time being, all you need to know is that `torch.tensor` operates similarly to `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a new Torch tensor\n",
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f'Values:\\n{my_tensor}\\n')\n",
    "\n",
    "print(f'Length: {len(my_tensor)}')\n",
    "print(f'Size: {my_tensor.size()}')                      # like np.array.shape\n",
    "print(f'Number of dimensions: {my_tensor.dim()}')       # like np.array.ndim\n",
    "print(f'Number of elements: {my_tensor.numel()}')       # like np.array.size\n",
    "print(f'Number type: {my_tensor.type()}')               # like np.array.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily convert between `torch.Tensor` and `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "my_numpy_array = my_tensor.numpy()                      # convert to NumPy ndarray\n",
    "\n",
    "other_tensor = torch.from_numpy(np.arange(10))          # convert from NumPy ndarray to Torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion across number types is also easily done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current number type: {my_tensor.type()}')\n",
    "\n",
    "my_tensor = my_tensor.float()\n",
    "\n",
    "print(f'After calling .float(): {my_tensor.type()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be initialised with a specific number type, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.ones(3, dtype=torch.bool)         # tensor of ones of size 3, initialised as bool\n",
    "\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [documentation](https://pytorch.org/docs/stable/tensors.html) for all supported data types.\n",
    "For most of our work, we'll be using `.float()` (32-bit floating point), `.long()` (64-bit\n",
    "integer) and `.bool()` (boolean; True/False)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have useful built-in functions, similar to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand((3, 4))                      # tensor with random values of size 3x4\n",
    "\n",
    "print(f'Sum of tensor:\\n{my_tensor.sum()}\\n')\n",
    "print(f'Sum of tensor along first dimension:\\n{my_tensor.sum(0)}\\n')\n",
    "print(f'Mean of tensor along second dimension:\\n{my_tensor.mean(1)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many NumPy functionalities to generate new tensors are also available in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tensor with zeros of size 2x3:\\n{torch.zeros(2, 3)}\\n')\n",
    "print(f'Tensor with ones of size 2x3:\\n{torch.ones(2, 3)}\\n')\n",
    "print(f'Tensor with increasing values from 2 to 50 in steps of 3:\\n{torch.arange(2, 50, 3)}\\n')\n",
    "print(f'Tensor with 10 linearly spaced values from 0 to 1:\\n{torch.linspace(0, 1, 10)}\\n')\n",
    "print(f'Tensor with uniform random values between 0 and 1 of size 2x4:\\n{torch.rand(size=(2,4))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access and assignment of values works exactly the same way as in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.zeros((5, 5), dtype=torch.int)\n",
    "\n",
    "my_tensor[1, 2] = 3         # second element in first dimension, third element in second dimension\n",
    "my_tensor[3, 1:4] = 1       # ranges (1:4 = second to fourth element in dimension)\n",
    "my_tensor[:, -1] = 5        # all elements of first dimension (:) and\n",
    "                            # negative indices (-1 = last element in dimension) for second dimension\n",
    "\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.zeros((3, 4), dtype=torch.int)\n",
    "tensor_b = 10 * torch.ones((5, 4), dtype=torch.int)\n",
    "\n",
    "tensor_cat = torch.cat((tensor_a, tensor_b), dim=0)         # torch.cat: similar to np.concatenate\n",
    "\n",
    "print(f'Concatenated tensor:\\n{tensor_cat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor only has a single element, you can access it with the `.item()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_single = torch.tensor([3.141])\n",
    "\n",
    "print(f'Tensor with single value:\\n{tensor_single}\\n')\n",
    "print(f'Size of tensor:\\n{tensor_single.size()}\\n')\n",
    "\n",
    "print(f'Output of calling .item() on tensor:\\n{tensor_single.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like NumPy arrays, Tensors can have many dimensions as you have seen.\n",
    "You can add/remove dimensions and/or reshape and transpose them in any way that is feasible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.randn(size=(1, 3, 4))\n",
    "\n",
    "print(f'Values of my_tensor:\\n{my_tensor}\\n')\n",
    "\n",
    "print(f'Size of my_tensor:\\n{my_tensor.size()}\\n')\n",
    "\n",
    "# our tensor has three dimensions, with the first one containing only one entry. We can get rid of\n",
    "# the first dimension if we need to:\n",
    "my_tensor = my_tensor.squeeze()             # .squeeze(): remove single-length dimensions\n",
    "print(f'Size after calling .squeeze():\\n{my_tensor.size()}\\n')\n",
    "\n",
    "# the opposite, i.e., adding new dimensions, is also possible:\n",
    "my_tensor = my_tensor.unsqueeze(0)          # .unsqueeze(): add new dimension (at given position)\n",
    "print(f'Size after calling .unsqueeze():\\n{my_tensor.size()}\\n')\n",
    "\n",
    "# we can reshape the tensor into any other shape that contains the same number of elements:\n",
    "my_tensor = my_tensor.reshape((6, 2))       # 6x2 has the same number of elements as 1x3x4\n",
    "print(f'Size after calling .reshape():\\n{my_tensor.size()}\\n')\n",
    "\n",
    "# we can also transpose the tensor. Note that this effectively swaps the ordering of values!\n",
    "print(f'Values right now:\\n{my_tensor}\\n')\n",
    "my_tensor = my_tensor.transpose(1, 0)       # swap ordering: first dimension 1, then dimension 0\n",
    "print(f'Values after calling .transpose():\\n{my_tensor}\\n')\n",
    "\n",
    "my_tensor = my_tensor.T                     # .T: same as .transpose(1, 0) for 2-D tensors\n",
    "print(f'Values after calling .T:\\n{my_tensor}\\n')\n",
    "\n",
    "# we can \"flatten\" the tensor into a one-dimensional one, like a vector:\n",
    "my_tensor = my_tensor.flatten()             # same as np.ravel()\n",
    "print(f'Size after calling .flatten():\\n{my_tensor.size()}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Torch.Tensors build on NumPy arrays, many functions that accept the latter will also work with\n",
    "the former. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.randn((100))                      # randn: random values from  standard normal distribution\n",
    "\n",
    "vals, bins = torch.histogram(my_tensor, bins=10)    # create histogram with 10 bins\n",
    "\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(bins[1:], vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib usually expects NumPy arrays, but it worked well above with Torch Tensors.\n",
    "\n",
    "However, if you need to perform any calculations outside of PyTorch it is good practice to first\n",
    "call `.numpy()` on the tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradients and Autograd\n",
    "\n",
    "At this point, you may ask why we have to work with yet another library and data type.\n",
    "\n",
    "The reason for this is that PyTorch brings extra functionality that other libraries like NumPy do\n",
    "not have, but which is extremely important for deep learning.\n",
    "\n",
    "One of those is the automated calculation of gradients. You should remember from the lecture that\n",
    "gradients are fundamental to training a deep learning model. Let us recap this briefly with the help\n",
    "of an imaginary toy dataset.\n",
    "\n",
    "\n",
    "**ðŸŒ² Toy example: tree age prediction**\n",
    "\n",
    "Imagine the following question: given the measured height of a tree in metres, can you predict how\n",
    "old it is?\n",
    "\n",
    "Let us assume that you have gone around and measured the height of a few trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_heights = torch.tensor([40.0, 12.5, 18.6, 55.2, 34.9])         # tree heights in metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By some means, you also know how old each one of the measured trees is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_ages = torch.tensor([60, 32, 36, 92, 51])                      # tree ages in years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tree_heights, tree_ages)\n",
    "plt.xlabel('Tree height [m]')\n",
    "plt.ylabel('Tree age [y]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It probably would make sense to fit a linear model through this, right?\n",
    "\n",
    "$$\n",
    "\\hat{y} = ax + b\n",
    "$$\n",
    "\n",
    "In other words, we could try and predict the tree age ($\\hat{y}$) from the tree height ($x$).\n",
    "\n",
    "Let's try one with some random values for model parameters $a$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.0)                           # slope\n",
    "b = torch.tensor(30.0)                          # intercept\n",
    "\n",
    "# predict tree age across the range of heights with linear model y_hat = ax + b\n",
    "x = torch.linspace(tree_heights.min(), tree_heights.max(), 100)\n",
    "y_hat_init = a*x + b\n",
    "\n",
    "plt.scatter(tree_heights.numpy(), tree_ages.numpy())\n",
    "plt.plot(x, y_hat_init, 'r--')                       #Â plot prediction\n",
    "plt.xlabel('Tree height [m]')\n",
    "plt.ylabel('Tree age [y]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's a start, but it doesn't really seem to work that well. How can we tell? By calculating\n",
    "the **loss value**.\n",
    "\n",
    "We're performing a regression here, so a viable loss would be the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) =  \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y_i} - y_i)^2\n",
    "$$\n",
    "\n",
    "We can easily do this in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict tree age for all our data points, using initial guess of parameters a and b\n",
    "y_hat = a * tree_heights + b\n",
    "\n",
    "# calculate squared error for each data point\n",
    "squared_error = (y_hat - tree_ages) ** 2\n",
    "print(f'Squared error per data point:\\n{squared_error}\\n')\n",
    "\n",
    "# calculate MSE (mean squared error across all data points)\n",
    "mean_squared_error = torch.mean(squared_error)\n",
    "print(f'Mean squared error: {mean_squared_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're pretty off from the measured values, as you can see. How can we improve our model?\n",
    "\n",
    "Well, we set the model parameters, $a$ and $b$, by ourselves, and they obviously aren't a good fit.\n",
    "What if we could learn them based on our observations and measurements â€“ like in a machine learning\n",
    "model?\n",
    "\n",
    "If you remember from the lecture, **gradients** (first-order derivatives) can tell us about the\n",
    "direction of descent of a function. What if we calculated gradients for our loss function, the mean\n",
    "squared error? This would tell us how to descend it, that is, to move towards a solution where the\n",
    "MSE value is minimal.\n",
    "\n",
    "Thus, if we could calculate the gradients of the loss function with respect to our learnable\n",
    "parameters, $a$ and $b$, we could figure out how to adjust them so that the loss becomes minimal. In\n",
    "other words, we could fit the model to the data!\n",
    "\n",
    "To do so, we need to calculate the first-order derivative of our loss, MSE, with respect to the\n",
    "prediction $\\hat{y}$:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\hat{y}} = \\frac{1}{N}\\sum_{i=1}^{N}2\\hat{y_i} - 2y_i\n",
    "$$\n",
    "\n",
    "\n",
    "Read: the partial derivative of the loss $\\mathcal{L}$ with respect to the prediction $\\hat{y}$ is:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient (first-order derivative) of loss w.r.t. prediction y_hat\n",
    "gradient = (2*y_hat - 2*tree_ages)\n",
    "\n",
    "print(f'Gradient: {gradient.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the gradients of the loss $\\mathcal{L}$ with respect to prediction $\\hat{y}$. That is\n",
    "interesting, but it doesn't yet tell us how to adjust model parameters $a$ and $b$. \n",
    "\n",
    "The reason for this is that we didn't just calculate the loss â€“ we also first calculated the\n",
    "prediction. Our ultimate function, from measured tree height to the loss, thus is the following:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = \\frac{1}{N}\\sum_{i=1}^N ((ax_i + b) - y_i)^2\n",
    "$$\n",
    "\n",
    "We thus have to calculate gradients not just for the loss, but one step further back, through the\n",
    "linear prediction $ax_i + b$.\n",
    "\n",
    "\n",
    "\n",
    "Remember from the lecture that we can apply the chain rule for this:\n",
    "$$\n",
    "    p(q(x))' = p'(q(x)) * q'(x)\n",
    "$$\n",
    "\n",
    "Here, $q()$ is our prediction function ($\\hat{y} = ax + b$) and $p()$ is our MSE loss.\n",
    "\n",
    "The first-order derivatives of our prediction function w.r.t. $a$ and $b$ are:\n",
    "$$\n",
    "\\frac{\\partial\\hat{y}}{\\partial a} = x\n",
    "$$\n",
    "and:\n",
    "$$\n",
    "\\frac{\\partial\\hat{y}}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "If we put those pieces together in the chain rule, we can therefore obtain gradients for $a$ and\n",
    "$b$ based on the loss value we got:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial a} = \\frac{\\partial\\mathcal{L}}{\\partial\\hat{y}} * x\n",
    "$$\n",
    "\n",
    "and:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b} = \\frac{\\partial\\mathcal{L}}{\\partial\\hat{y}} * 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_a = (gradient * tree_heights).mean()\n",
    "\n",
    "print(f'Gradient for parameter a (slope):\\t{gradient_a}')\n",
    "\n",
    "gradient_b = (gradient * 1).mean()\n",
    "\n",
    "print(f'Gradient for parameter b (intercept):\\t{gradient_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we now know how we have to adjust $a$ a lot and $b$ a little bit. We also know in which\n",
    "direction: both need to be adjusted negatively (remember that the _negative_ gradient tells us the\n",
    "direct path to the loss minimum). If we now multiply our learnable parameters $a$ and $b$ with their\n",
    "respective negative gradients, we should get a model that fits our dataset slightly better.\n",
    "\n",
    "Gradients are often too large, and we usually want to reduce their magnitude (also so that single\n",
    "data points don't have too much influence over others). We can do this with an attenuation factor,\n",
    "also known as the **learning rate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "\n",
    "# adjust model parameters a and b based on their gradients\n",
    "a = a - gradient_a * learning_rate\n",
    "b = b - gradient_b * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict tree age again with our new parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_new = a * tree_heights + b\n",
    "\n",
    "# calculate MSE loss\n",
    "mse_loss = torch.mean((y_hat_new - tree_ages)**2)\n",
    "print(f'MSE loss for updated model: {mse_loss}')\n",
    "\n",
    "\n",
    "# plot\n",
    "x = torch.linspace(tree_heights.min(), tree_heights.max(), 100)\n",
    "y_hat_new = a*x + b\n",
    "plt.scatter(tree_heights.numpy(), tree_ages.numpy())\n",
    "plt.plot(x, y_hat_init, 'r--', label='original model')      # prediction with initial parameters\n",
    "plt.plot(x, y_hat_new, 'b--', label='updated model')        #Â with updated parameters\n",
    "plt.legend()\n",
    "plt.xlabel('Tree height [m]')\n",
    "plt.ylabel('Tree age [y]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better overall! Our MSE loss has gone down, simply because we have calculated its\n",
    "gradients and adjusted model parameters $a$ and $b$ accordingly. It's still not a perfect fit, but\n",
    "we could repeat the fitting step a few times until the gradients become near-zero, at which point we\n",
    "would have reached the optimum (convergence) and we would have a good prediction model.\n",
    "\n",
    "\n",
    "But this procedure was very cumbersome. The above was just a simple linear model, imagine how\n",
    "complicated this would become for a big deep learning model. Wouldn't it be nice if we didn't have\n",
    "to calculate gradients and first-order derivatives ourselves?\n",
    "\n",
    "That's exactly what modern deep learning frameworks like PyTorch offer: automated gradient\n",
    "computation ([Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)). Let's\n",
    "repeat the above experiment, but let PyTorch calculate gradients for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our initial model parameters again\n",
    "a = torch.tensor(1.0, requires_grad=True)                           # slope\n",
    "b = torch.tensor(30.0, requires_grad=True)                          # intercept\n",
    "\n",
    "# obtain prediction\n",
    "y_hat = a * tree_heights + b\n",
    "\n",
    "# calculate MSE loss\n",
    "loss = torch.mean((y_hat - tree_ages)**2)\n",
    "\n",
    "# hmm, what's that?\n",
    "loss.backward()\n",
    "\n",
    "# let's see what PyTorch gives us for gradients:\n",
    "print(f'Gradient for parameter a (slope):\\t{a.grad}')\n",
    "print(f'Gradient for parameter b (intercept):\\t{b.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these values with the gradients we calculated manually above: they're identical!\n",
    "\n",
    "The only steps we had to implement to get these values were:\n",
    "1. Specify which tensors we wanted to calculate gradients for (`requires_grad=True`).\n",
    "2. Call `.backward()` on the loss value.\n",
    "\n",
    "In other words, by doing so, PyTorch automatically computes all relevant gradients for us with a\n",
    "single line: it does the **backward pass** (or backpropagation) for us, with chain rule and\n",
    "everything! This works for _any_ PyTorch function, because under the bonnet, all first-order\n",
    "derivatives are implemented already for you.\n",
    "\n",
    "Thus, if this section sounded a bit intimidating to you â€“ don't worry! You will very, very likely\n",
    "_never_ have to calculate or implement derivatives and gradients by yourselves. But I hope you now\n",
    "understand how deep learning model training works. ðŸ¤“\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Functions to Layers\n",
    "\n",
    "Well, it gets even cooler.\n",
    "\n",
    "Above, we specified, and implemented, a linear operation manually:\n",
    "```python\n",
    "y_hat = a * tree_heights + b\n",
    "```\n",
    "\n",
    "If we have to do this for every computation in a deep learning model, it would get really annoying.\n",
    "\n",
    "As you have seen in the lectures, deep learning models consist of _layers_ that each perform a\n",
    "specific operation, such as a linear transformation or a non-linear activation function, on their\n",
    "respective inputs.\n",
    "\n",
    "\n",
    "PyTorch also helps us here, because it has many of the typical deep learning layers already built-in\n",
    "and ready to use for you. All PyTorch layers are defined in the\n",
    "[torch.nn](https://pytorch.org/docs/stable/nn.html) sub-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, a linear prediction layer is implemented â€“ it's the\n",
    "[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer.\n",
    "\n",
    "To make use of it, we first have to create a new linear layer with its specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(in_features=1, out_features=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above object we created is a linear layer that accepts a single feature as input\n",
    "(`in_features=1`) and returns a single prediction value as output (`out_features=1`).\n",
    "\n",
    "\n",
    "ðŸ’¡ You can imagine what happens if you change these hyperparameters:\n",
    "* Setting `in_features` greater than 1 results in a _multivariate_ linear regression.\n",
    "* Setting `out_features` greater than 1 results in _multiple_ linear regressions.\n",
    "\n",
    "\n",
    "We can apply the layer we just defined above on a tensor to obtain a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain prediction with newly created linear layer\n",
    "y_hat = linear_layer(tree_heights.unsqueeze(-1))\n",
    "\n",
    "print(f'Output of linear layer:\\n{y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice two quirks here:\n",
    "1. We had to call `.unsqueeze(-1)` on the input. This is because the linear layer expects an input\n",
    "   tensor of size $B\\times N$. Here, $B$ is the batch size (the number of data points) and $N$ is\n",
    "   the number of features. In our case, we have five data points and only one feature (tree height),\n",
    "   so we need to reshape the tensor into size $5\\times1$.\n",
    "2. The output of the layer seems to contain just random values. That is because the layer's model\n",
    "   parameters, $a$ and $b$, are initialised randomly when a new layer is created. We can access\n",
    "   these and verify that the calculations are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weight (a) and bias (b) from linear layer and calculate output manually (for verification)\n",
    "y_hat_manual = linear_layer.weight * tree_heights + linear_layer.bias\n",
    "\n",
    "print(f'Manual calculation of output, with layer parameters:\\n{y_hat_manual}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ In deep learning jargon, \"weight\" is the same as slope and \"bias\" is the intercept (in a linear\n",
    "model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a brief look at the [torch.nn](https://pytorch.org/docs/stable/nn.html) package to see what\n",
    "kind of layers PyTorch comes with already built-in.\n",
    "\n",
    "For example, we also have non-linear **activation functions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_layer = nn.Sigmoid()        # sigmoid: y_hat = 1/(1+exp(-x))\n",
    "relu_layer = nn.ReLU()              # rectified linear unit: y_hat = max[0, x]\n",
    "\n",
    "\n",
    "random_tensor = torch.randn((5))\n",
    "print(f'Tensor values:\\n{random_tensor}\\n')\n",
    "print(f'After sigmoid:\\n{sigmoid_layer(random_tensor)}\\n')\n",
    "print(f'After ReLU:\\n{relu_layer(random_tensor)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how sigmoid transforms everything into the $[0,1]$ range and ReLU truncates all values at zero,\n",
    "exactly as we would expect.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From Layers to Neural Networks\n",
    "\n",
    "From the above, it should be clear how we can construct a deep learning model: by creating, and\n",
    "applying, various linear and non-linear layers in a pre-defined sequence.\n",
    "\n",
    "And again, PyTorch has an ace up its sleeve for us here.\n",
    "\n",
    "Note how every layer performs a specific operation. To do so, it is _callable_, that is, you can\n",
    "give it an input and it returns an output? Isn't an entire artificial neural network the same?\n",
    "\n",
    "PyTorch implements this by defining a _base class_, `nn.Module`, which offers exactly that and can\n",
    "be specified as a layer, a series of layers, or anything else you like â€“ including an entire model!\n",
    "Thus, we can create our own variants of `nn.Module` that do whatever we want, be it perform an\n",
    "operation as part of a single neural network layer, or an entire network itself.\n",
    "\n",
    "Let us define a small artificial neural network as an `nn.Module` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # define model's layers here\n",
    "        self.linear1 = nn.Linear(in_features, 16)\n",
    "        #TODO: implement rest of the layers here\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO: implement forward pass of model here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot is going on above:\n",
    "* Keyword `class` denotes an object **definition**, a blueprint of what `MyModel` looks like. It\n",
    "  specifies the properties of `MyModel` and what it can do (functions), but\n",
    "  _not_ its actual values (_e.g._, weights and biases of the layers).\n",
    "\n",
    "* Function `__init__` is the object class **constructor**: this method is called whenever you\n",
    "  _initialise_ a new _instance_ of `MyModel`. Here, you define the model's properties, such as its\n",
    "  layers.\n",
    "\n",
    "* Finally, function `forward` is required by the `nn.Module` _base class_ and defines the _forward\n",
    "  pass_, _i.e._, how `MyModel` predicts an output based on a given input. Note the function has two\n",
    "  input arguments: `self`, which is a reference to the current instance of `MyModel` and allows you\n",
    "  to access its properties (_e.g._, layers), and `x`, which is the input tensor. The function\n",
    "  eventually must `return` an output tensor.\n",
    "\n",
    "\n",
    "This concept is called **object-oriented programming** (OOP): you define an object _class_ (the\n",
    "blueprint) that specifies the class' own properties (weights, biases, anything else you need) as\n",
    "well as functions (forward(), _etc._), and then you can initialise an object _instance_ from it.\n",
    "OOP isn't exclusive to PyTorch â€“ you have seen it already without knowing it! Remember last\n",
    "week when you constructed _e.g._ a Random Forest classifier and specified properties like number of\n",
    "trees, before you could call .fit(X, y) on it? Well, you constructed an object instance of\n",
    "RandomForestClassifier with hyperparameters and then called its `fit` function to train it. Neat!\n",
    "\n",
    "In fact, OOP isn't even exclusive to Python â€“ it plays a big role in many other languages. But we\n",
    "don't need to go into much more detail about it here; this is computer science territory.\n",
    "\n",
    "\n",
    "\n",
    "As-is, the class definition of `MyModel` above is not complete. Please implement the remaining\n",
    "parts:\n",
    "\n",
    "ðŸ–Œï¸ Implement an artificial neural network with the following architecture:\n",
    "  1. Linear (fully-connected), mapping from input tensor of dimension `in_features` to 16 output\n",
    "     dimensions.\n",
    "  2. Sigmoid\n",
    "  3. Linear, mapping from 16 input to 32 output dimensions.\n",
    "  4. Sigmoid\n",
    "  5. Linear, mapping from 32 input to `num_classes` output dimensions.\n",
    "\n",
    "ðŸ’¡ As you can see, the model's `__init__` function already has an unfinished line `self.linear1 =\n",
    "nn.Linear(...)`. This is an example of how to define a property of `MyModel`. You can initialise\n",
    "this and the other layers in such fashion here.\n",
    "\n",
    "ðŸ’¡ Likewise, the `forward` function already contains a return statement, although this currently\n",
    "just returns the input `x`. Here, you need to apply the layers to the input `x`, or else the\n",
    "previous layer's output, in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_ndim = 5\n",
    "num_classes = 6\n",
    "\n",
    "# create random input tensor\n",
    "my_tensor = torch.randn((batch_size, input_ndim))\n",
    "\n",
    "\n",
    "# initialise an instance of MyModel that you have defined above\n",
    "my_model = MyModel(input_ndim, num_classes)\n",
    "\n",
    "\n",
    "# apply model instance on input tensor (obtain prediction; forward pass)\n",
    "pred = my_model(my_tensor)\n",
    "\n",
    "# the following lines test whether your model output is a Tensor with correct size\n",
    "assert isinstance(pred, torch.Tensor), \\\n",
    "    f'Incorrect implementation: your model should return a torch.Tensor, but it returned {pred}.'\n",
    "assert pred.dim() == 2 and (pred.size(0) == batch_size and pred.size(1) == num_classes), \\\n",
    "    'Incorrect implementation: your model should return a tensor of size ' + \\\n",
    "    f'{batch_size}x{num_classes}, but it returned {pred.size()}.'\n",
    "\n",
    "print(f'Output size: {pred.size()}')\n",
    "\n",
    "print(f'Output values:\\n{pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are those outputs? They look a bit weird, don't they?\n",
    "\n",
    "It says that the size of the model output is $4\\times6$. If you check the code carefully, you will\n",
    "see that this corresponds to our batch size of 4 (_i.e._, four data points) and the number of\n",
    "classes being 6. Why do we get so many outputs, and not just four values?\n",
    "\n",
    "Lots of machine learning is based on _probabilities_. Most ML models don't just\n",
    "predict the most likely label class, but also provide some form of **confidence**. For example, a\n",
    "random forest will tell you how many of its trees predicted the most frequent class; if 3 out of 5\n",
    "trees predicted the same class, the model confidence is 3/5 = 60% for the predicted class.\n",
    "\n",
    "Deep learning does exactly the same; it can give us confidences for all outputs and all classes. But\n",
    "the above values don't look like confidences, right? These are raw model outputs, also known as\n",
    "**logits**.\n",
    "\n",
    "Remember from the lecture that we need a final activation function for classification, known as the\n",
    "**softmax**? Watch what happens if we apply softmax on these outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = torch.softmax(pred, dim=1)           # apply softmax along second (class) dimension\n",
    "\n",
    "print(f'Output values after softmax:\\n{pred_softmax}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sum these values along the second dimension, _i.e._, along all the label classes individually\n",
    "for each data point, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Softmax-activated values summed along label dimension:\\n{pred_softmax.sum(1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ones! In other words, thanks to softmax, we now have something like probabilities for each class\n",
    "and each data point that we can use: a softmax-activated output value of 0 means \"zero probability\n",
    "that it's this class\", and 1 means that the model is 100% sure about the class.\n",
    "\n",
    "The predicted class then becomes the position (index) of the largest predicted probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pred.argmax(dim=1)          # argmax: take the argument (position) that has maximum value\n",
    "\n",
    "print(f'Arg max:\\n{y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we have to know which class is at which position (we can't expect the model to return\n",
    "class names). The same applies for our target/ground truth labels, $y$.\n",
    "\n",
    "Cool, we now have a deep learning model that, given a number of input points with certain number of\n",
    "features, predicts an output for a specified number of classes. We can make predictions!\n",
    "\n",
    "Let's talk about the remaining ingredients required to train such a model, so that we can apply it\n",
    "to a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "### 5.1 Loss Function\n",
    "\n",
    "To train a deep learning model, we need to have a criterion, a measure of how good its prediction is\n",
    "with respect to a _target_ (label, ground truth). This is the **loss function**. Above, you have\n",
    "seen the mean squared error (MSE) for regression.\n",
    "\n",
    "You may remember from the lecture what we use for classification: the cross-entropy loss. You may\n",
    "also remember that we can technically represent a loss function just like another layer. Thus, it\n",
    "probably makes sense to also implement it as a `torch.nn` object class that we can call on inputs,\n",
    "right?\n",
    "\n",
    "PyTorch does exactly that: see the\n",
    "[documentation](https://pytorch.org/docs/stable/nn.html#loss-functions) for loss functions. Below,\n",
    "we will take a look at\n",
    "[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Cross-Entropy Loss object instance (from the torch.nn sub-package)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define random tensor of targets (ground truth label indices) for demonstration purposes\n",
    "target = torch.randint(low=0,\n",
    "                       high=num_classes,\n",
    "                       size=(batch_size,))\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "print(f'Loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ CrossEntropyLoss takes the **raw model output** (logits) as input, NOT the softmax-activated\n",
    "ones. It applies softmax by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Backpropagation\n",
    "\n",
    "Remember Autograd, the automated gradient computation, from Section 2 above? Let's use it on our\n",
    "loss value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just like that, gradients are calculated.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ’¡ We can only call `.backward()` once (try running the above cell a second time if you want to see\n",
    "what happens). This is a security feature that prevents you from calculating gradients twice,\n",
    "inflating the learning signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optimiser\n",
    "\n",
    "After calling `loss.backward()`, we now have gradients for each model parameter. Now, we need to\n",
    "apply the gradients to actually adjust the parameters.\n",
    "\n",
    "In Section 2 above, we have done this manually. This was a very crude way of doing it and not\n",
    "realistic to use. Instead, you may remember from the lecture that we can use an extra entity for\n",
    "this, known as the **optimiser**.\n",
    "\n",
    "Multiple optimiser types have been proposed, and PyTorch has a pretty complete set of the common\n",
    "ones. The most basic (and widely useful) is _stochastic gradient descent_ (SGD).\n",
    "\n",
    "ðŸ–Œï¸ Read the [SGD documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) and\n",
    "initialise an optimiser instance for the model instance (`my_model`) we defined above by completing\n",
    "the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD         # import stochastic gradient descent from subpackage\n",
    "\n",
    "\n",
    "# create instance of SGD\n",
    "optimiser = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this optimiser to modify all learnable model parameters.\n",
    "\n",
    "### 5.4 Training Iteration\n",
    "\n",
    "Putting all pieces together, this is one training step for a deep learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (forward pass)\n",
    "pred = my_model(my_tensor)\n",
    "\n",
    "# calculate loss\n",
    "loss = criterion(pred, target)\n",
    "\n",
    "# reset any gradients to zero calculated before\n",
    "optimiser.zero_grad()\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "\n",
    "# apply gradients: modify model parameters to better fit target\n",
    "optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "Of course, all the other ML principles we have seen last week, including train/val/test split,\n",
    "under-/overfitting, hyperparameter tuning, _etc._, apply to deep learning models as well. We'll see\n",
    "this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Loading and Saving\n",
    "\n",
    "Once you have trained a model that you like, you may want to save it to disk and be able to re-use\n",
    "it later for prediction.\n",
    "\n",
    "PyTorch allows us to do that very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract model parameters\n",
    "params = my_model.parameters()\n",
    "\n",
    "# save parameters to file\n",
    "with open('model_state.pt', 'wb') as f:\n",
    "    torch.save(params, f)\n",
    "\n",
    "# to load model parameters again for a new model\n",
    "my_model = MyModel(input_ndim, num_classes)             # create new model instance first\n",
    "\n",
    "with open('model_state.pt', 'rb') as f:\n",
    "    params = torch.load(f)\n",
    "    my_model.load_state_dict(params)                    # load trainable parameters from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Tips and tricks:\n",
    "- The `wb` and `rb` indicate \"write binary file\" and \"read binary file\", respectively.\n",
    "- You do have to know how to initialise a new model instance before loading saved parameters. For\n",
    "  example, in the model above, `input_ndim` and `num_classes` must have _exactly_ the same values as\n",
    "  the original model you saved â€“ you cannot just \"invent\" a new label class, for example.\n",
    "- Likewise, the class definition of `MyModel` must be the same.\n",
    "- You can save multiple parameters to an output, for example with a `dict` file.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise: Species Distribution Modelling with Deep Learning\n",
    "\n",
    "We now have pretty much all parts together to train a deep learning model for real! ðŸ˜ƒ\n",
    "\n",
    "Let us use the same dataset we have used last week, where we try to predict which species we can\n",
    "observe in space based on environmental features.\n",
    "\n",
    "The first step, loading the data, is the same as for any ML model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load observations\n",
    "obs = pd.read_csv('https://raw.githubusercontent.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/refs/heads/main/data/north_american_mammals/locations.csv')\n",
    "\n",
    "# load covariates (which we will use as input features)\n",
    "cov = pd.read_csv('https://raw.githubusercontent.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/refs/heads/main/data/north_american_mammals/covariates.csv')\n",
    "\n",
    "# join them together\n",
    "data = pd.merge(obs, cov, on='id')\n",
    "\n",
    "# remove invalid (NaN) values\n",
    "data = data.dropna()\n",
    "\n",
    "# keep track of all covariate names\n",
    "cov_cols = list(frozenset(cov.columns).difference(frozenset(['id', 'lon', 'lat', 'species'])))\n",
    "\n",
    "# show the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, things start to change a bit. Remember that we cannot feed a species name like\n",
    "\"Lasiurus_cinereus\" into a deep learning model. We need to convert names to ordinals (integer\n",
    "numbers from 0) instead, so that each data point with the same species gets assigned the same\n",
    "ordinal. For example:\n",
    "\n",
    "```\n",
    "\"Lasiurus_cinereus\" --> 0\n",
    "\"Ammospermophilus_leucurus\" --> 1\n",
    "_etc._\n",
    "```\n",
    "\n",
    "If we keep this assignment the same throughout, we will know that a point with observed species\n",
    "\"Ammospermophilus_leucurus\" will have a ground truth label of 1, and the model output at position 1\n",
    "will encode the logits (or confidence, with softmax) for the same species.\n",
    "\n",
    "\n",
    "ðŸ–Œï¸ Implement the conversion to ordinals below. You will have to append a new column\n",
    "\"species_ordinal\" to the dataframe, and also store the order of species in a second variable called\n",
    "`species_order`. Tip: take a look at\n",
    "[Pandas.factorize](https://pandas.pydata.org/docs/reference/api/pandas.factorize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_ordinals = ...\n",
    "species_order = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to standardise our covariate/input feature values. At the moment, their magnitude\n",
    "varies massively (summerEVI goes beyond 3,000, while topoRugged is below 100). You can imagine\n",
    "that gradients calculated for big values can easily overpower those for small ones. To facilitate\n",
    "training, we will thus apply z-score normalisation, same as we have seen last week: $$\\hat{x} =\n",
    "\\frac{x - mean(x)}{std(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cov_cols] = (data[cov_cols] - data[cov_cols].mean()) / data[cov_cols].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split the data into training, validation, and test sets. We'll use 60% for training, 10% for validation, and 30% for testing.\n",
    "\n",
    "You should remember this from last week's exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(data,\n",
    "                                         test_size=0.4,\n",
    "                                         stratify=data['species'],\n",
    "                                         random_state=42)\n",
    "data_train, data_val = train_test_split(data_train,\n",
    "                                        test_size=0.16,\n",
    "                                        stratify=data_train['species'],\n",
    "                                        random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now separate the three datasets into their respective input features $\\mathbf{X}$ and target labels $\\mathbf{y}$. Crucially, these need to be `torch.Tensor` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(data_train[cov_cols].to_numpy()).float()\n",
    "y_train = torch.from_numpy(data_train['species_ordinal'].to_numpy()).long()\n",
    "\n",
    "X_val = torch.from_numpy(data_val[cov_cols].to_numpy()).float()\n",
    "y_val = torch.from_numpy(data_val['species_ordinal'].to_numpy()).long()\n",
    "\n",
    "X_test = torch.from_numpy(data_test[cov_cols].to_numpy()).float()\n",
    "y_test = torch.from_numpy(data_test['species_ordinal'].to_numpy()).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that we need data (`X`) in 32-bit float (`.float()`) and label ordinals (`y`) in\n",
    "long integer (`.long()`) format.\n",
    "\n",
    "\n",
    "\n",
    "Datasets used to train deep learning models are usually very large, and input features very big\n",
    "(_e.g._, images). We often cannot fit the entire dataset into memory. Therefore, we often train deep\n",
    "learning models in **minibatches**, that is, we randomly load a fixed number of data points (_e.g._,\n",
    "32) at a time and calculate and apply gradients for those only.\n",
    "\n",
    "Our dataset here isn't really big and would fit into memory, but it's still good practice to train\n",
    "in batches here, too. Plus, it gives us the opportunity to talk about a final piece of the puzzle:\n",
    "[Datasets and DataLoaders](https://pytorch.org/docs/stable/data.html).\n",
    "\n",
    "These are helpers that allow us to load big datasets in batches efficiently. Let us first implement\n",
    "our own Dataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx,:], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `MyDataset` offers two key functions, `__len__` and `__getitem__`. The first one\n",
    "returns the number of data points we have, and the second returns our two properties for the data\n",
    "point at position `idx`, that is, its features `X` and label `y`.\n",
    "\n",
    "We can now create instances of `MyDataset` for our training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataset(X_train, y_train)\n",
    "dataset_val = MyDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem overkill for our purpose, but it is very useful for large datasets and complex\n",
    "inputs like images, as explained. For example, you can create a Dataset class that receives a list\n",
    "of 10,000 image paths, and only decide to actually load the image from disk once `__getitem__` is\n",
    "called. This is much, much more memory-efficient than loading all the images in advance.\n",
    "\n",
    "Function `__getitem__` only returns a single item, the one at position `idx`. But we need them in\n",
    "minibatches, right? This job is done by the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8      # the DataLoader assembles the minibatches, so we define their sizes here\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we set `shuffle=True` for the training DataLoader. This randomises the elements that get\n",
    "put together into a minibatch for each epoch, which improves model training.\n",
    "\n",
    "We can then iterate over each DataLoader instance to obtain our minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in dataloader_train:\n",
    "    print(f'Size of X: {X.size()}.')\n",
    "    print(f'Size of y: {y.size()}')\n",
    "\n",
    "    print(f'Values of X:\\n{X}\\n')\n",
    "    print(f'Values of y:\\n{y}\\n')\n",
    "    break       # abort loop after first iteration (for demonstration purposes only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, we get an input feature tensor `X` of size $8\\times9$ (batch size times number of\n",
    "features), and a label tensor `y` of size 8 (batch size). We can use that to train and test our\n",
    "model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our data ready, so the next step is to create our model. We will reuse our\n",
    "implementation of `MyModel` above.\n",
    "\n",
    "ðŸ–Œï¸ Initialise a new instance of `MyModel` with the right properties (_i.e._, number of input\n",
    "features and number of output classes). You can calculate these from `X_train` and `species_order`\n",
    "accordingly. You can copy-paste your definition below for modifications afterwards if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "my_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ–Œï¸ Next, initialise all the other elements we need: optimiser, criterion/loss function.\n",
    "For the optimiser, start with a learning rate of 0.001 and no other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = ...\n",
    "\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us implement the training loop, where we train the model for a given number of epochs.\n",
    "\n",
    "During each epoch, gradients get calculated for the training data and model parameters adjusted by\n",
    "the optimiser. As a result, the model is supposed to get a little bit better after each epoch on the\n",
    "training set, measured by the loss value going down. During each epoch, we can also do two more\n",
    "things:\n",
    "1. Calculate classification accuracy in addition to the loss value;\n",
    "2. do the same (calculation of loss and accuracy) for the validation set to test for overfitting.\n",
    "\n",
    "Thus, it is common practice to design a training routine as follows:\n",
    "\n",
    "\n",
    "```\n",
    "for epoch in num_epochs:\n",
    "    for X, y in training data loader:\n",
    "        # 1. predict (forward pass)\n",
    "        # 2. calculate loss\n",
    "        # 3. set any previously computed gradients to zero (zero_grad)\n",
    "        # 4. backward pass\n",
    "        # 5. apply gradients to model parameters with learning rate (optimiser step)\n",
    "        # 6. calculate accuracy\n",
    "    \n",
    "    for X, y in validation data loader:\n",
    "        # 1. predict (forward pass)\n",
    "        # 2. calculate loss\n",
    "        # 3. calculate accuracy\n",
    "```\n",
    "\n",
    "\n",
    "ðŸ–Œï¸  Let us implement this now. Complete the code cell below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                optimiser,\n",
    "                dataloader_train,\n",
    "                dataloader_val,\n",
    "                num_epochs):\n",
    "\n",
    "    # prepare lists for overall training and validation loss and accuracy values per epoch\n",
    "    loss_train, loss_val = [], []\n",
    "    accuracy_train, accuracy_val = [], []\n",
    "\n",
    "\n",
    "    # iterate over epoch\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # training\n",
    "        model.train()            # see explanation below about this line\n",
    "\n",
    "        # we will accumulate loss and accuracy values over the minibatches here\n",
    "        loss_train_epoch, accuracy_train_epoch = 0.0, 0.0\n",
    "\n",
    "        # iterate over training DataLoader instance, that is, over all the minibatches\n",
    "        for X, y in dataloader_train:\n",
    "            #TODO: implement training routine here\n",
    "            ...\n",
    "\n",
    "        # append averaged statistics\n",
    "        loss_train.append(loss_train_epoch / len(dataloader_train))\n",
    "        accuracy_train.append(accuracy_train_epoch / len(dataloader_train))\n",
    "\n",
    "\n",
    "        # validate\n",
    "        model.eval()             # see explanation below about this line\n",
    "\n",
    "        #TODO: implement validation routine here\n",
    "\n",
    "    # return the trained model and training statistics at the end\n",
    "    return model, loss_train, loss_val, accuracy_train, accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Infos:\n",
    "- Functions `my_model.train()` and `my_model.eval()` put the model into training, respectively\n",
    "  evaluation/prediction mode. This is very important because some deep learning layers may behave\n",
    "  differently during training compared to validation. We will see examples of this next week.\n",
    "- Autograd, PyTorch's gradient computation engine, needs to keep track of all the history of\n",
    "  functions applied to a tensor to be able to calculate gradients. To do so, it creates a\n",
    "  computation _graph_ by default. This can be rather expensive, both computation- and memory-wise.\n",
    "  If we only want to make predictions, we don't need such a graph, because we will never compute\n",
    "  gradients. Therefore, we can place any operation where we don't need gradients under `with\n",
    "  torch.no_grad():`, which makes things easier and more lightweight.\n",
    "\n",
    "\n",
    "After having completed the above code cell, execute it. Then, let us plot the loss and overall\n",
    "accuracy values over the epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "my_model, loss_train, loss_val, accuracy_train, accuracy_val = train_model(my_model,\n",
    "            optimiser,\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(loss_train, loss_val, accuracy_train, accuracy_val):\n",
    "    epochs = np.arange(1, len(loss_train)+1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, loss_train, 'b-', label='train')\n",
    "    plt.plot(epochs, loss_val, 'r-', label='val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, accuracy_train, 'b-', label='train')\n",
    "    plt.plot(epochs, accuracy_val, 'r-', label='val')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Overall Accuracy')\n",
    "\n",
    "\n",
    "plot_result(loss_train, loss_val, accuracy_train, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you started with the recommended configuration, you might notice two things:\n",
    "1. The training and validation loss curves are going down overall, which means that the model seems\n",
    "   to be learning something; nice!\n",
    "2. The overall accuracy is very low, though, even at the 5th epoch.\n",
    "\n",
    "\n",
    "Well, the loss going down is encouraging, but you see that it hasn't really reached a plateau at the\n",
    "end of training. Remember that gradient descent is an iterative process, adjusting the parameters\n",
    "just a little bit each time. Maybe training hasn't _converged_ yet, and we need to train for longer?\n",
    "\n",
    "\n",
    "ðŸ–Œï¸ Increase the number of epochs to 100 below. Re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialise model instance and optimiser â€“ this is important so that you can start anew\n",
    "my_model = MyModel(...)     #TODO\n",
    "\n",
    "optimiser = ...         \n",
    "\n",
    "# train model\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "my_model, loss_train, loss_val, accuracy_train, accuracy_val = train_model(my_model,\n",
    "            optimiser,\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            NUM_EPOCHS)\n",
    "\n",
    "# plot training stats\n",
    "plot_result(loss_train, loss_val, accuracy_train, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that took a bit longer, but now the loss curves have gone down a lot more. But the accuracy is\n",
    "still terrible. The dataset isn't the problem (remember we were able to\n",
    "train _e.g._ a random forest with up to 60% accuracy last week), so perhaps the hyperparameters are\n",
    "the culprit.\n",
    "\n",
    "As you can imagine, there is no \"single\" deep learning model, but a seemingly infinite number of\n",
    "ways to design, and train, such models. Each of these design choices can be seen as a\n",
    "hyperparameter.\n",
    "\n",
    "Let us try and improve performance now. We'll start with the simplest: the learning rate. Remember\n",
    "from the lecture that too small learning rates can \"trap\" the model in a local, suboptimal minimum;\n",
    "the gradient signal isn't strong enough to overcome small hills in the loss function anymore. Maybe\n",
    "that's the problem?\n",
    "\n",
    "ðŸ–Œï¸ Increase the learning rate from 0.001 to 0.1. Re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialise model instance and optimiser â€“ this is important so that you can start anew\n",
    "my_model = MyModel(...)     #TODO\n",
    "\n",
    "#TODO: see above to train model and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, your overall accuracy should have shot up to around 60% â€“ awesome! We indeed got\n",
    "trapped in a local minimum before.\n",
    "\n",
    "But the loss and accuracy curves appear rather squiggly, jumping up and down (especially for the\n",
    "validation set). The model seems to oscillate.\n",
    "\n",
    "Remember how we're creating minibatches of multiple data points? If they all pull into different\n",
    "directions (in terms of gradients), then the model gets \"thrown around\" and cannot find a good\n",
    "consensus to adjust its parameters to. We can \"iron out\" those minute differences across points by\n",
    "adding more of them at a time â€“ in other words, by increasing the size of the minibatches.\n",
    "\n",
    "ðŸ–Œï¸ Increase the batch size to 64 and re-train the model below. You will have to re-initialise the\n",
    "training and validation set data loaders accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialise data loaders with new batch size\n",
    "dataloader_train = DataLoader(...)\n",
    "\n",
    "dataloader_val = DataLoader(...)\n",
    "\n",
    "# re-initialise model instance and optimiser â€“ this is important so that you can start anew\n",
    "my_model = MyModel(...)     #TODO\n",
    "\n",
    "#TODO: see above to train model and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better.\n",
    "\n",
    "\n",
    "\n",
    "However, you might notice that the training loss (blue) is slightly lower than the validation loss\n",
    "(red) towards the end of the training. This difference is not a problem at all, but too much of it\n",
    "and we have an overfitting problem.\n",
    "\n",
    "Remember how to combat overfitting? One way to do so is weight decay, which shrinks the model's\n",
    "learnable parameters towards zero. This has the effect that the model cannot make crazy outlandish\n",
    "predictions anymore, but has to find a \"mean\" consensus, even if single data points in the training\n",
    "set tell it differently (through large gradient values). As with the learning rate, we can control\n",
    "how much we want the weights to decay, so it's another hyperparameter.\n",
    "\n",
    "We can apply weight decay via the optimiser. Let's try it.\n",
    "\n",
    "ðŸ–Œï¸ Add weight decay of 0.001 to the optimiser. Re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialise model instance and optimiser â€“ this is important so that you can start anew\n",
    "my_model = MyModel(...)     #TODO\n",
    "\n",
    "#TODO: see above to train model and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the gap between the training and validation loss curves close, but too much and the\n",
    "overall accuracy doesn't reach the same level as before (trade-off between over- and underfitting).\n",
    "\n",
    "Once you're happy, you can apply your trained model on the test set to obtain a final prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    loss_test = 0.0\n",
    "\n",
    "    # store per-data point predictions and labels for later\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss_test += loss.item()\n",
    "            predictions.append(y_hat)\n",
    "            labels.append(y)\n",
    "    \n",
    "    # combine prediction and label tensors\n",
    "    predictions, labels = torch.cat(predictions, 0), torch.cat(labels, 0)\n",
    "\n",
    "    # return results\n",
    "    return loss_test/len(dataloader), predictions, labels\n",
    "\n",
    "\n",
    "# create test set dataloader\n",
    "dataset_test = MyDataset(X_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# predict on test set with model\n",
    "loss, pred_test, labels_test = test_model(my_model, dataloader_test)\n",
    "\n",
    "# report\n",
    "oa_test = torch.mean((pred_test.argmax(1) == labels_test).float())\n",
    "print(f'Test set loss: {loss:.2f}, overall accuracy: {oa_test:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ–Œï¸ Bonus: try adjusting other hyperparameters, for example:\n",
    "* Increase the number of linear layers in the model\n",
    "* Increase or decrease the `out_channels` of the model's intermediate layers\n",
    "* Replace `nn.Sigmoid()` with another non-linearity, _e.g._ `nn.ReLU()`\n",
    "* Try out different combinations of learning rate, weight decay, batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "We have seen how deep learning models work, how they get trained, and have designed an entire\n",
    "workflow in PyTorch to do so.\n",
    "\n",
    "You might be disappointed to see that all of the above, including lots of coding, hyperparameter\n",
    "search, and longer training times led to a test accuracy that is at best as good as what a random\n",
    "forest could do for us last week.\n",
    "\n",
    "If you've tried the bonus above, you probably also noticed that bigger models don't really help but\n",
    "can make things worse. You may see now why deep learning had fallen out of fashion in the late 90s\n",
    "to early 2000s: models like Support Vector Machine and Random Forest were just as powerful, but\n",
    "lighter, quicker, and easier to develop.\n",
    "\n",
    "\n",
    "\n",
    "### So what's the big deal with deep learning?\n",
    "\n",
    "The data we collect today is orders of magnitude vaster and more complex than back in the 90s. Even\n",
    "a single, tiny image of 800x600 pixels contains over 1.4 Million features. Using such data to make\n",
    "semantically advanced predictions, such as which species is in an image, is an extremely difficult\n",
    "task. Designing _e.g._ a Random Forest for this is feasible, but requires very complicated\n",
    "engineering.\n",
    "\n",
    "This is where deep learning shines: if the features and/or task are complex, and datasets are huge.\n",
    "\n",
    "Our SDM example seems to be neither. However, starting next week we will look into problems where\n",
    "deep learning really does make sense. Crucially, the principles you have seen today form the basis\n",
    "for any supervised deep learning model â€“ up to large language models like GPT! So stay tuned for\n",
    "what's to come!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
