{"cells":[{"cell_type":"markdown","metadata":{"id":"8pe0gNk7iIOu"},"source":["# Week 10: Species Distribution Modelling\n","\n","In this final exercise of the course we will be looking at species distribution modelling (SDM)! As\n","you have seen in the slides, there's generally two flavours of SDMs:\n","* _Mechanistic:_ here, the relationship rules between a habitat and its suitability for a species\n","  are manually/explicitly defined in a mathematical framework. For example: suitability for penguins\n","  = (annual temperature < 10 °C) * (latitude < -70 degrees). You can imagine that this becomes\n","  hard very quickly once we go beyond such dummy examples.\n","* _Correlative:_ hence, this form is more common. Here, we attempt to automatically identify\n","  patterns in the data and compare them to observed species occurrences; in other words, we cast it\n","  as a machine learning problem!\n","\n","\n","SDMs are the \"bread and butter\" in ecology and found everywhere – in fact, you have already used\n","some form of it in the last week (occupancy modelling).\n","\n","\n","Throughout the course _BIOS0032: AI for the Environment_, you have encountered and learnt to use\n","many models for a plethora of prediction tasks and datasets. Given that they are all one or another\n","flavour of machine learning model, they share the same basic principles that need to be heeded.\n","Recall figure from week 2:\n","\n","![ml\n","workflow](https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/2_Intro_to_ML/ml_workflow.png?raw=true)\n","\n","> Simplified workflow from: S. Amershi et al., [\"Software Engineering for Machine Learning: A Case\n","> Study,\"](https://ieeexplore.ieee.org/abstract/document/8804457) 2019 IEEE/ACM 41st International\n","> Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), Montreal, QC,\n","> Canada, 2019, pp. 291-300, doi: 10.1109/ICSE-SEIP.2019.00042.\n","\n","\n","Hence, in this exercise we will address SDMs from a machine learning perspective here, going into\n","the details and pitfalls of training and using them. The main exercise uses random forests, but\n","there's also some deep learning at the end if you're interested!"]},{"cell_type":"markdown","metadata":{"id":"vtzJedigiIOw"},"source":["# 0. Setup\n","\n","## 0.1 Dependencies"]},{"cell_type":"code","source":["# installation of packages not present in Google Colab\n","%pip install rasterio"],"metadata":{"id":"YOg6j6oTiNG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXwrMl9EiIOw"},"outputs":[],"source":["from typing import Tuple, Dict\n","import os\n","import sys\n","import time\n","import hashlib\n","from google.colab import drive, output\n","import numpy as np\n","import pandas as pd\n","import geopandas as gpd\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import rasterio\n","from shapely.geometry import Point\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","\n","\n","RANDOM_SEED = 42\n","\n","output.enable_custom_widget_manager()"]},{"cell_type":"markdown","source":["## 0.2 Mount Google drive\n","\n","Data used in this exercise is available in the public Google drive folder with this URL:\n","https://drive.google.com/drive/folders/1R81gwnmn5MiMuW2pQrkXFRVq9Yzlyg0g?usp=sharing\n","\n","Follow these steps to use the dataset in this notebook:\n","1. Open the link and log in to your Google account (if needed).\n","2. Click the folder name in the heading, then \"Organize\" > \"Add shortcut\".\n","3. Select \"My Drive\", then click \"Add\"."],"metadata":{"id":"FT60U8fJkJDX"}},{"cell_type":"code","source":["# mount Google drive\n","drive.mount('/content/drive')\n","\n","# folder where we'll try to access files from\n","BASE_FOLDER = '/content/drive/MyDrive/UCL0032_AI4Environment/week_10/week10_exercise/data'\n","\n","# we'll be using an external Python script down below\n","sys.path.append(os.path.join(BASE_FOLDER, 'misc'))"],"metadata":{"id":"me0S1TuXjvSd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FUhXnRmiIOw"},"source":["## 1. Data preparation\n","\n","The dataset we'll be using in this exercise comes from the following paper:\n","> Winner, K., Ingenloff, K., Sandall, E., Sica, YV, Marsh, C., Cohen, J., Ranipeta, A., Killion, A.,\n","> Jetz, W.: _High Resolution Species Distribution Models of North American Biodiversity_. In\n","> preparation.\n","\n","We'll be using a subset of three mammalian species occurrences over North America.\n","\n","\n","### 1.1 Observation data\n","\n","Our observations come from GBIF\n","([10.15468/DL.ZP2JZF](https://www.gbif.org/occurrence/download/0291299-200613084148143)) and have\n","been pre-processed already, including removal of outliers and duplicates, and binning into 1km^2\n","resolution grid cells. Let us first load them from the CSV file into memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGKM89nMiIOw"},"outputs":[],"source":["# load data\n","observations = pd.read_csv(os.path.join(BASE_FOLDER,\n","                                        'observations/observations.csv'))\n","\n","# keep tuple (= unmodifiable list) of species names\n","species_names = observations['species'].unique()\n","species_names = tuple(species_names)\n","\n","print('First few rows of observations:')\n","print(observations.head())"]},{"cell_type":"markdown","metadata":{"id":"XzrZRAYXiIOx"},"source":["It's always a good idea to visualise your data. This facilitates finding potential outliers and\n","errors originating _e.g._ from any data pre-processing step. If your data includes\n","crowdsourcing-derived geospatial point records, these are often susceptible to inaccuracies due to\n","low-quality GPS recorders, or simply due to user error.\n","\n","Let's plot these points on a map now. Just like for R, there's libraries available for Python that\n","can conveniently do this, such as GeoPandas (see [week 8](https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/8_AI_and_Movement_Data/)) and Plotly (which we'll use here)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCwJawdRiIOx"},"outputs":[],"source":["fig = px.scatter_mapbox(observations,\n","                        lat='lat',\n","                        lon='lon',\n","                        title='Observations',\n","                        hover_name='species',\n","                        color='species',\n","                        height=600,\n","                        width=800,\n","                        zoom=2)\n","fig.update_layout(mapbox_style='open-street-map')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"BJqsQkPliIOx"},"source":["#### ❓\n","\n","Read up on the three species regarding their habitat preferences and assumed distribution. A good\n","source is for example [Map of Life](https://mol.org/).\n","\n","- Do these observations appropriately reflect these species' expected habitat range?\n","- Could you see any issues for modelling the three species together, for example in a joint SDM?"]},{"cell_type":"markdown","metadata":{"id":"EGt1IW_viIOx"},"source":["### 1.2 Pseudo-absences\n","\n","As is often the case, we only have (opportunistically sampled) observations, but no confirmed\n","absences of species (which are much harder to obtain). This causes problems for _both_ training and\n","testing habitat suitability models.\n","\n","A standard approach in this case is to sample additional locations in a random or semi-random\n","manner, and designate these as _pseudo-absences_. This may seem counterintuitive to do so, but our\n","chances of accidentally including suitable locations as pseudo-absences is surprisingly low. Yet,\n","the quality of these pseudo-absences still depends on the way they are sampled.\n","\n","For this exercise, you are given three sets of differently sampled pseudo-absences\n","(`absences_a.csv`, `absences_b.csv`, `absences_c.csv`), which get loaded and visualised in the code\n","below.\n","\n","\n","#### ❓\n","- Can you identify the differences between those three sets? Tip: it might help to display\n","  observations _vs._ pseudo-absences for one species at a time; feel free to modify the code to this\n","  end.\n","- Imagine you had to write your own code to sample pseudo-absences. What do you need to do to get\n","  meaningful ones? List the main steps you deem necessary (no need for code or pseudocode)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2r00H_TqiIOx"},"outputs":[],"source":["# read CSV file of predefined pseudo-absences\n","pseudo_absences = pd.read_csv(os.path.join(BASE_FOLDER,\n","                                           'observations',\n","                                           'pseudo_absences_c.csv'))\n","\n","# add extra column to differentiate between observations (presence points)\n","# and pseudo-absences\n","pseudo_absences['is_presence'] = False\n","\n","# merge observations & pseudo-absences data frames into a new one\n","observations['is_presence'] = True\n","data = observations.copy()._append(pseudo_absences)\n","\n","# plot\n","fig = px.scatter_mapbox(data,\n","                        lat='lat',\n","                        lon='lon',\n","                        title='Observations & pseudo-absences',\n","                        hover_name='species',\n","                        color='is_presence',\n","                        height=600,\n","                        width=800,\n","                        zoom=2)\n","fig.update_layout(mapbox_style='open-street-map')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"1OYjmrL1iIOx"},"source":["### 1.3 Covariates\n","\n","We'll be using the following thematic layers as covariates for this exercise:\n","* `meanAnnualTemp`: mean annual air temperature [CHELSA]\n","* `seasonalityPrecip`: precipitation seasonality [CHELSA]\n","* `accumPrecip`: accumulated precipitation on growing season days [CHELSA]\n","* `cloudCoverAnnualVar`: cloud cover intra-annual variation [EarthEnv]\n","* `topoRugged`: topographic ruggedness index [EarthEnv]\n","* `varEVI`: spatial variation of the enhanced vegetation index (EVI) [EarthEnv]\n","* `summerEVI`: mean summer EVI (Jun-Aug) [MODIS]\n","* `winterEVI`: mean winter EVI (Nov-Feb) [MODIS]\n","\n","Sources:\n","* [CHELSA] https://chelsa-climate.org/\n","* [EarthEnv] https://www.earthenv.org/\n","* [MODIS] https://lpdaac.usgs.gov/products/mod13q1v006/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8EN-0H7iIOx"},"outputs":[],"source":["# let us first define a tuple (= fixed list) of the covariates, in order\n","covariate_ids = (\n","    'meanAnnualTemp',\n","    'seasonalityPrecip',\n","    'accumPrecip',\n","    'cloudCoverAnnualVar',\n","    'topoRugged',\n","    'varEVI',\n","    'summerEVI',\n","    'winterEVI'\n",")"]},{"cell_type":"markdown","metadata":{"id":"Gb6Bl8wDiIOy"},"source":["In [week 9](https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/9_AItoEcologicalModels) we've seen how to sample covariate values at given geospatial locations using R. Naturally, we can do the same in Python, for example using the Rasterio library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1OaH2aXiIOy"},"outputs":[],"source":["def sample_covariates(locations: pd.DataFrame,\n","                      geotiff_path: str) -> Tuple[float]:\n","    '''\n","        Receives geospatial coordinates (lat, lon) and a path to a GeoTIFF file and samples values\n","        from the GeoTIFF.\n","\n","        Inputs:\n","        - \"locations\": Pandas.DataFrame, containing (at least) \"lat\" and \"lon\" columns with\n","          coordinates.\n","        - \"geotiff_path\": str, path to GeoTIFF file that needs to be sampled.\n","\n","        Returns:\n","        - tuple of floats, containing per-point sampled values in order of \"locations\"\n","    '''\n","    # prepare locations by converting lat+lon columns into list of (lat, lon) tuples\n","    # tip: read up on Python's \"zip\" command\n","    coordinates = zip(locations['lon'], locations['lat'])\n","\n","    # load GeoTIFF in rasterio\n","    with rasterio.open(geotiff_path, 'r') as f_raster:\n","      values = np.array(tuple(f_raster.sample(coordinates))).squeeze()\n","\n","      # mask no data values if present\n","      if f_raster.nodata is not None:\n","        invalid = values == f_raster.nodata\n","        values = values.astype(float)\n","        values[invalid] = float('nan')\n","    return values\n","\n","\n","# sample covariate values by iterating over GeoTIFFs in our specified order\n","for cov_id in tqdm(covariate_ids):\n","  # GeoTIFF file path\n","  geotiff_path = os.path.join(BASE_FOLDER, 'rasters', f'{cov_id}.tiff')\n","\n","  # sample values on locations defined in \"obs_merged\" and append to covariates DataFrame\n","  sampled_values = sample_covariates(data, geotiff_path)\n","  data[cov_id] = sampled_values\n","\n","\n","# remove rows with NaN values\n","print(f'# samples: {len(data)}.')\n","data.dropna(axis=0,\n","            inplace=True)\n","print(f'# samples after dropping NaN values: {len(data)}.\\n')\n","\n","\n","# display the first few rows (this now contains both observations+pseudo-absences *and* covariates)\n","print(data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01RBVRqaiIOy"},"outputs":[],"source":["# plot covariate histograms for each species\n","for species_name in species_names:\n","    valid_rows = data['species'] == species_name\n","\n","    plt.figure(figsize=(14,10))\n","    for cidx, covariate_id in enumerate(covariate_ids):\n","        plt.subplot(4,4,cidx+1)\n","        plt.hist(data.loc[valid_rows,covariate_id])\n","        plt.title(covariate_id)\n","    plt.suptitle(species_name)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"V8SJdgXOiIOy"},"source":["## 2. Data splitting\n","\n","Since our species distribution modelling procedure is based on correlative measurements obtained\n","from known data points, it is a machine learning (ML) problem. As such, it has to follow all the\n","typical ML requirements and prerequisites, one of which is a means to test for generalisation\n","inability, or overfitting (see [week\n","2](https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/2_Intro_to_ML)).\n","To test for this, we basically need three datasetes (recap):\n","* a _training_ dataset to fit the model to (tune model parameters);\n","* a _validation_ dataset to tune the model's **hyper**parameters for maximum generalisation out of\n","  the known set of samples (training set);\n","* at the very end, a _test_ set to get a final performance score on data the model truly has never\n","  seen.\n","\n","The idea then is to iteratively train-test the model on the training and validation sets, and tune\n","its parameters and hyperparameters before eventually applying it on the test set.\n","\n","In ecology, one often sees multiple runs of training-validation on different partitionings (folds),\n","known as k-fold crossvalidation. This is a good idea to increase the confidence in generalisation\n","ability, but it should not replace a dedicated test set (in theory...).\n","\n","Naturally, all three datasets should follow the same underlying distribution. This is where species\n","distribution modelling becomes difficult: not only do the _covariates_ be equally sampled, but also\n","the _labels_ (observations).\n","\n","In the following, we will split the data into three sets (training, validation, test) based on\n","pre-defined percentages (stratified by species), and sampled randomly.\n","\n","#### ❓\n","- Is this an adequate way of splitting the data? Why so, resp. why not?\n","- We use single-species distribution models below. Do you see additional problems if we were to use\n","  a multi- or joint SDM instead?\n","- Likewise, what would be the problem with this idea if we used spatial context for covariates\n","  instead of single point values?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SlawZbYiIOy"},"outputs":[],"source":["def split_random(data: pd.DataFrame,\n","                 percentage_train: float=60.0,\n","                 percentage_test: float=30.0) -> Tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n","    '''\n","        Receives an observations table and percentage values for the training and test set\n","        (remainder is used for validation set). Splits the observations into train, val, and test\n","        sets at random.\n","\n","        Inputs:\n","\n","        - \"observations\": Pandas.DataFrame, containing locations and column \"species\" with observed\n","          labels\n","        - \"percentage_train\": float, percent of observation rows assigned randomly to training set\n","        - \"percentage_test\": float, the same for the validation set\n","\n","        Returns:\n","\n","        - three Pandas.DataFrame instances containing train, val, test subsets of input\n","          \"observations\" Pandas.DataFrame.\n","    '''\n","    percentage_val = max(0.0, 100 - percentage_train - percentage_test)\n","\n","    train, val, test = [], [], []\n","\n","    # split observations and pseudo-absences separately\n","    for abs_val in (False, True):\n","      subset = data[data['is_presence'] == abs_val]\n","\n","      # split at random\n","      train_sub, test_sub = train_test_split(subset,\n","                                             test_size=(percentage_test+percentage_val)/100.0,\n","                                             train_size=percentage_train/100.0,\n","                                             shuffle=True,\n","                                             random_state=RANDOM_SEED,\n","                                             stratify=subset['species'])\n","\n","      # split test set again for validation set\n","      val_sub, test_sub = train_test_split(test_sub,\n","                                  test_size=percentage_test/(percentage_test+percentage_val),\n","                                  train_size=percentage_val/(percentage_test+percentage_val),\n","                                  shuffle=True,\n","                                  random_state=RANDOM_SEED,\n","                                  stratify=test_sub['species'])\n","\n","      # append\n","      train.append(train_sub)\n","      val.append(val_sub)\n","      test.append(test_sub)\n","\n","    # concatenate observation and pseudo-absence subsets into data frames\n","    train = pd.concat(train, axis=0)\n","    val = pd.concat(val, axis=0)\n","    test = pd.concat(test, axis=0)\n","\n","    return train, val, test\n","\n","\n","# do the splitting\n","data_train, data_val, data_test = split_random(data,\n","                                               60.0,\n","                                               30.0)\n","\n","\n","# visualise\n","data_train['split'] = 'train'\n","data_val['split'] = 'val'\n","data_test['split'] = 'test'\n","data = pd.concat((data_train, data_val, data_test),\n","                 axis=0)\n","fig = px.scatter_mapbox(data,\n","                        lat='lat',\n","                        lon='lon',\n","                        hover_name='split',\n","                        color='split',\n","                        height=600,\n","                        width=800,\n","                        zoom=2)\n","fig.update_layout(mapbox_style='open-street-map')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"ElLB-TsfiIOy"},"source":["## 3. Hyperparameter tuning\n","\n","We can now think about training our SDMs, including tuning their hyperparameters. This includes the\n","following steps:\n","\n","For each hyperparameter combination:\n","1. Train model with parameters on training set.\n","2. Obtain predictions by applying trained models to validation set.\n","3. Evaluate model performance on validation set.\n","Then, pick the hyperparameter combination of choice.\n","\n","\n","With that in mind, let us first implement three generic functions for (i.) model training, (ii.)\n","prediction, and (iii.) performance evaluation. We shall use random forests as SDMs for this\n","exercise. You have seen those before in [week\n","2](https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/tree/main/2_Intro_to_ML).\n","\n","\n","#### ❓\n","- Complete the gaps in the code below.\n","- For performance evaluation, you will have to choose a metric. There are many options, not all of\n","  which are adequate here. Tip: read through and choose from what\n","  [scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html) has to offer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXkf1SaoiIOy"},"outputs":[],"source":["def train(data: pd.DataFrame,\n","          hyperparameters: dict=None) -> Dict[str, RandomForestClassifier]:\n","    '''\n","        Main training function for random forest.\n","    '''\n","    if hyperparameters is None:\n","        hyperparameters = {}\n","\n","    models = {}     # dict of species: model instances\n","\n","    # iterate over all species\n","    for species in species_names:\n","        # subset data for species\n","        valid = data['species'] == species\n","\n","        # train model\n","        model = ❓\n","        model.fit(❓)\n","\n","        # add to dict\n","        models[species] = model\n","\n","    return models\n","\n","\n","\n","def predict(data: pd.DataFrame,\n","            models: Dict[str, RandomForestClassifier]) -> pd.DataFrame:\n","    '''\n","        Main prediction function for random forest.\n","    '''\n","    # initialise empty dict of predictions\n","    predictions = {}\n","\n","    # iterate over all species\n","    for species in species_names:\n","        model = models[species]\n","\n","        # get predicted probabilities\n","        pred = ❓\n","\n","        # we have two classes (is_presence = False or True), so let's extract the \"True\" column vals\n","        col = np.where(model.classes_ == True)[0]\n","        pred = pred[:,col].squeeze()\n","        predictions[species] = pred\n","\n","    # convert to DataFrame\n","    predictions = pd.DataFrame(predictions)\n","    predictions.index = data.index\n","    return predictions\n","\n","\n","\n","def evaluate(predictions: pd.DataFrame,\n","             labels: pd.DataFrame) -> Dict[str,float]:\n","    '''\n","        Main evaluation function for any form of predictions with pseudo-probabilities.\n","    '''\n","    # initialise empty dict of performance scores\n","    scores = {}\n","\n","    # iterate over all species\n","    for species in species_names:\n","        # subset labels accordingly\n","        valid = np.where(labels['species']==species)[0]\n","\n","        labels_sub = labels['is_presence'].iloc[valid]\n","        pred_sub = predictions[species].iloc[valid]\n","\n","        # calculate metric\n","        metric = ❓\n","        scores[species] = metric\n","\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"mwV-aNbviIOz"},"source":["We now have all the ingredients to tune our random forest's hyperparameters.\n","\n","#### ❓\n","Read up on the random forest as provided in the `Scikit-learn` library.\n","\n","- Which hyperparameters does it offer, and which of those make sense to be tuned?\n","- Pick your two favourites and perform a grid search on the best combination of them. Do so by\n","  completing the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCsmboILiIOz"},"outputs":[],"source":["# define your hyperparameters (argument and list of values) here.\n","# The format is 'argument name': [val1, val2, val3, ...]\n","# Use precise random forest arguments as names (check the scikit-learn\n","# documentation). So, for example, if scikit-learn's Random Forest\n","# implementation accepts an argument named \"n_jobs\" and you want to try out\n","# values 1, 2, and 4, you can define it as follows:\n","#\n","#   'n_jobs': [1, 2, 4]\n","#\n","HYPERPARAMETERS = {\n","    '❓': [❓, ❓, ❓]\n","}\n","\n","# prepare matrix of all combinations of both hyperparameters (cartesian product)\n","param_a, param_b = tuple(HYPERPARAMETERS.keys())\n","vals_a, vals_b = np.meshgrid(HYPERPARAMETERS[param_b], HYPERPARAMETERS[param_a])\n","\n","# prepare matrix of crossvalidation results: here, we measure both accuracy and training time\n","accuracies, times = np.zeros_like(vals_a, dtype=float), np.zeros_like(vals_a, dtype=float)\n","\n","# iterate over all possible combinations of both hyperparameters\n","print('Starting crossvalidation...')\n","with tqdm(range(len(HYPERPARAMETERS[param_a])*len(HYPERPARAMETERS[param_b]))) as pbar:\n","    for idx_a, val_a in enumerate(HYPERPARAMETERS[param_a]):\n","        for idx_b, val_b in enumerate(HYPERPARAMETERS[param_b]):\n","            hyperparameters = {\n","                param_a: val_a,\n","                param_b: val_b\n","            }\n","\n","            # 0. start time\n","            tic = time.time()\n","\n","            # 1. train Random Forests on training set using current set of hyperparameters\n","            models = ❓   # call your \"train\" function from above here...\n","\n","            # 2. use trained models to predict validation set\n","            predictions = ❓    # ...and so on\n","\n","            # 3. evaluate predictions using accuracy measure\n","            scores = ❓\n","            score_avg = np.mean(list(scores.values()))          # average across all species\n","\n","            # 4. stop time\n","            toc = time.time()\n","            duration = toc - tic\n","\n","            # store values\n","            accuracies[idx_a,idx_b] = score_avg\n","            times[idx_a,idx_b] = duration\n","\n","            pbar.set_description(f'[{param_a}: {val_a}; {param_b}: {val_b}] ' + \\\n","                    f'score: {score_avg:.2f}, time: {duration:.2f} s')\n","            pbar.update(1)\n","\n","\n","# plot result as matrices\n","ticks_x, ticks_y = range(len(HYPERPARAMETERS[param_b])), range(len(HYPERPARAMETERS[param_a]))\n","\n","_, axes = plt.subplots(1,2, sharey=True)\n","plot_a = axes[0].matshow(accuracies.astype(float))\n","axes[0].set_xticks(ticks_x, HYPERPARAMETERS[param_b])\n","axes[0].set_yticks(ticks_y, HYPERPARAMETERS[param_a])\n","axes[0].set_ylabel(param_a)\n","axes[0].set_xlabel(param_b)\n","axes[0].set_title('accuracies')\n","plt.colorbar(plot_a)\n","plot_b = axes[1].matshow(times.astype(float))\n","axes[1].set_xticks(ticks_x, HYPERPARAMETERS[param_b])\n","axes[1].set_yticks(ticks_y, HYPERPARAMETERS[param_a])\n","axes[1].set_xlabel(param_b)\n","axes[1].set_title('times')\n","plt.colorbar(plot_b)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDgboBpkiIOz"},"outputs":[],"source":["# select your favourite set of hyperparameters here\n","HYPERPARAMETERS = {\n","    'n_estimators': 50,\n","    'max_depth': 10\n","}"]},{"cell_type":"markdown","metadata":{"id":"CRAhK2hriIOz"},"source":["# 4. Train, predict, and evaluate\n","\n","## 4.1 Check performance across splits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-UL_jTGiIOz"},"outputs":[],"source":["# 1. train Random Forests on training set using your final set of hyperparameters\n","models = train(data_train, HYPERPARAMETERS)\n","\n","# 2. use trained models to predict all three sets\n","pred_train = predict(data_train, models)\n","pred_val = predict(data_val, models)\n","pred_test = predict(data_test, models)\n","\n","# 3. evaluate predictions using accuracy measure\n","scores_train = evaluate(pred_train, data_train)\n","scores_val = evaluate(pred_val, data_val)\n","scores_test = evaluate(pred_test, data_test)\n","\n","# combine\n","scores = pd.DataFrame({'train': scores_train,\n","                       'val': scores_val,\n","                       'test': scores_test})\n","\n","# plot\n","plt.figure()\n","for sidx, species in enumerate(species_names):\n","    plt.bar(range(sidx, len(species_names)*4, 4), scores.loc[species], label=species)\n","plt.ylabel('score')\n","plt.xticks(range(1, len(species_names)*4, 4), ('train', 'val', 'test'))\n","plt.legend(loc='lower left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xjIMQg60iIOz"},"source":["## 4.2 Predict map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOFdfYudiIOz"},"outputs":[],"source":["# 1. Define locations on uniform grid across study area\n","landmask = gpd.read_file(os.path.join(BASE_FOLDER,\n","                                      'misc',\n","                                      'NA_land_area.geojson'))\n","\n","# unionise polygons from different continents\n","landmask = gpd.GeoDataFrame(index=[0],\n","                            crs=landmask.crs,\n","                            geometry=[landmask.unary_union])\n","bounds = landmask.bounds\n","\n","# we sample with limited resolution here\n","RESOLUTION = 0.5            # degrees (1 degree ~= 111km)\n","loc_lon = np.arange(bounds.minx[0], bounds.maxx[0], RESOLUTION)\n","loc_lat = np.arange(bounds.miny[0], bounds.maxy[0], RESOLUTION)\n","grid_lon, grid_lat = np.meshgrid(loc_lon, loc_lat)\n","\n","# remove points not on land\n","points = [Point(row) for row in list(zip(grid_lon.ravel(), grid_lat.ravel()))]\n","data_map = gpd.GeoDataFrame(data={'lon': grid_lon.ravel(), 'lat': grid_lat.ravel()},\n","                            geometry=points,\n","                            crs=landmask.crs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1KQVRRWiIOz"},"outputs":[],"source":["# 2. Sample covariates\n","for cov_id in tqdm(covariate_ids):\n","  # GeoTIFF file path\n","  geotiff_path = os.path.join(BASE_FOLDER, 'rasters', f'{cov_id}.tiff')\n","\n","  # sample values on locations defined in \"obs_merged\" and append to covariates DataFrame\n","  sampled_values = sample_covariates(data_map, geotiff_path)\n","  data_map[cov_id] = sampled_values\n","\n","# drop NaNs\n","data_map.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5J2AD-HCiIO0"},"outputs":[],"source":["# 3. Predict using trained Random Forests\n","pred_map = predict(data_map, models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7Dwl94YiIO0"},"outputs":[],"source":["# 4. Plot\n","df_map = pd.concat((pred_map, data_map), axis=1)\n","\n","\n","def plot_predictions(species: str,\n","                     plot_observations: bool=False) -> None:\n","    traces = []\n","    fig_pred = px.scatter_mapbox(df_map,\n","                                 lat='lat',\n","                                 lon='lon',\n","                                 title='Predictions',\n","                                 hover_name=species,\n","                                 color=species,\n","                                 height=600,\n","                                 width=800,\n","                                 zoom=2)\n","    traces.append(fig_pred.data[0])\n","    if plot_observations:\n","        fig2 = px.scatter_mapbox(data.loc[(data['species']==species) * \\\n","                                          (data['is_presence']==True)],\n","                                 lat='lat',\n","                                 lon='lon',\n","                                 title='Observations',\n","                                 hover_name='species',\n","                                 color_discrete_sequence=['green'],\n","                                 height=600,\n","                                 width=800,\n","                                 zoom=2)\n","        traces.append(fig2.data[0])\n","    fig = go.FigureWidget(data=traces)\n","    fig.update_layout(mapbox_style='open-street-map')\n","    fig.show()\n","\n","\n","species_wdgt = widgets.Dropdown(options=species_names, description='species:')\n","show_obs_wdgt = widgets.Checkbox(description='show observations')\n","\n","def redraw_figure(*args, **kwargs):\n","    clear_output(wait=True)\n","    display(species_wdgt, show_obs_wdgt, fig)\n","    plot_predictions(species=species_wdgt.value,\n","                     plot_observations=show_obs_wdgt.value)\n","\n","species_wdgt.observe(redraw_figure, names=['value'])\n","show_obs_wdgt.observe(redraw_figure, names=['value'])\n","\n","\n","redraw_figure()"]},{"cell_type":"markdown","source":["#### ❓\n","\n","* Are you satisfied with the results, and does the visual quality of the map correspond to the accuracy values you obtained?\n","* Repeat model training and testing with a different set of pseudo-absences (`pseudo_absences_b.csv` and `pseudo_absences_c.csv` under Section 1.2). How do model accuracy and map predictions change?\n","* Why do you think that is the case?"],"metadata":{"id":"bnuFodh_uPt2"}},{"cell_type":"markdown","metadata":{"id":"X9m5mIIdiIO0"},"source":["# 5. Bonus: Deep Learning model\n","\n","Remember week 3 where we used deep learning models to classify camera trap images? Let's use them as\n","SDMs here!\n","\n","In the exercise of week 3, you have seen the most important steps (model creation, data\n","augmentation, _etc._). We can do pretty much the same here (augmentation is harder with non-image\n","data, so we'll leave it for now). Like there (and in week 4), you will be provided with functions to\n","actually train and validate deep learning models here. If you are interested you are welcome to\n","check them out [here](TODO). What we will do differently now, however, is that we'll define our very\n","own deep learning model from the ground up!\n","\n","Let us first import some libraries needed:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6SSYkewiIO0"},"outputs":[],"source":["import torch                    # torch (PyTorch) is our deep learning library of choice\n","from torch import nn            # nn defines all the class modules (building blocks) we need\n","\n","torch.manual_seed(RANDOM_SEED)  # like NumPy & Co., also PyTorch has a random number generator"]},{"cell_type":"markdown","metadata":{"id":"5-07iwOqiIO0"},"source":["## 5.1 Define model class\n","\n","Next, we'll define our deep learning model. This includes two aspects:\n","1. Declare _which layers_ the model has;\n","2. Implement _in which order_ the layers are applied.\n","\n","PyTorch does this by means of object-oriented programming. No worries if you don't know what this\n","is; we won't need it for much. Feel free to ask us if you're curious, though! For now, it suffices\n","to say that you add your layers in the first method (`def __init__`) below, and the order in which\n","they are applied is then defined in the second method (`def forward`) underneath that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMKs1tJAiIO0"},"outputs":[],"source":["class Model(nn.Module):\n","    '''\n","        Implementation of our point-based deep learning model. This class subclasses (inherits) from\n","        \"nn.Module\", which includes lots of PyTorch goodies we can use.\n","    '''\n","\n","    def __init__(self,\n","                 num_covariates: int,\n","                 num_classes: int) -> None:\n","        '''\n","            This function (\"__init__\") is known as the \"constructor\" and is called whenever we\n","            create a new \"instance\" of our \"Model\" class. Here, we define all the properties our\n","            model must have, including its layers.\n","        '''\n","        super().__init__()      # call the parent (nn.Module) constructor first\n","\n","        # let's define our layers\n","        self.layers = nn.Sequential(                    # sequential: layers applied one-by-one\n","            nn.Linear(num_covariates, 64),              # first fully-connected: map from no. covs.\n","            nn.BatchNorm1d(64),                         # batch normalisation\n","            nn.ReLU(),                                  # rectified linear unit (ReLU)\n","\n","            # ❓ you can modify the layer stack if you want.\n","            # Check out the PyTorch documentation for this:\n","            # https://pytorch.org/docs/stable/nn.html\n","\n","            nn.Linear(64, 128),                         # second fully-connected\n","            nn.BatchNorm1d(128),                        # second batch normalisation\n","            nn.Linear(128, num_classes)                 # final fully-connected: map to no. classes\n","        )\n","\n","\n","    def forward(self,\n","                data: torch.Tensor) -> torch.Tensor:\n","        '''\n","            The \"forward pass\" function of our model. Above (in the constructor), we only defined\n","            *what* layers the model has; here, we define *how* (i.e., in which order) to use them.\n","        '''\n","        return self.layers(data)                        # we just apply our nn.Sequential list"]},{"cell_type":"markdown","metadata":{"id":"KjEevfbBiIO0"},"source":["## 5.2 Create model class instance\n","\n","What we defined above is the model _class_, something like a blueprint for what our model will look\n","like. However, this isn't a model yet – it has no learnable parameters or other form of state; it's\n","just a template.\n","\n","To create an actual model _instance_, we can execute the command below. This effectively calls the\n","`def __init__` function with the arguments we provide (`num_covariates`, `num_classes`) and then\n","initialises a new model instance (with random parameters as by PyTorch default)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ku8_Y4nqiIO0"},"outputs":[],"source":["# model: let's create an instance of our above \"Model\" class and call its constructor\n","model = Model(num_covariates=len(covariate_ids),\n","              num_classes=len(species_names))"]},{"cell_type":"markdown","metadata":{"id":"ACe6WLoGiIO0"},"source":["We now have a fresh, new instance of our \"Model\" and should be able to already make predictions with\n","it, albeit random ones. In PyTorch, the moniker for data elements is not \"array\", \"matrix\" or\n","\"DataFrame\", but \"Tensor\". PyTorch models generally accept input tensors of sizes like this\n","(examples):\n","\n","```\n","    B x C\n","    B x C x W x H\n","```\n","\n","Where `B` is the _batch size_ (_i.e._, number of data points grouped together in one go during\n","training/prediction), `C` is the number of _channels_ (_e.g._, 3 in a red-green-blue image), and `W`\n","and `H` are the image width and height (if available). We're not dealing with images here, so our\n","tensor is just two-dimensional: `B` is still the batch size (number of observations/pseudo-absences\n","in one batch), but `C` is the number of covariates.\n","\n","You have seen this in weeks 3 and 4. In our case, though, we need to give our model a `B x C`\n","torch.Tensor, as it accepts `B` elements with `C` covariate values each. Let's try it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PqWB0qUiIO0"},"outputs":[],"source":["# let's take the first ten rows (B) and all covariates (C) of our \"data\" DataFrame\n","data_subset = data.loc[:,covariate_ids].iloc[:10,:]\n","\n","# convert subset to a torch.Tensor\n","subset_tensor = torch.from_numpy(data_subset.to_numpy()).float()\n","print(f'Input size: {subset_tensor.size()}')\n","\n","# pass it through the model to obtain predictions (\"forward pass\")\n","pred_subset = model(subset_tensor)\n","\n","print(f'Prediction size: {pred_subset.size()}')\n","print('Predicted values:')\n","print(pred_subset)"]},{"cell_type":"markdown","metadata":{"id":"AHyAME3CiIO1"},"source":["# 5.3 Training\n","\n","Let us now actually train this model for species distribution modelling! As said all the\n","functionality for this is available in a [separate Python file](TODO)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cUYRh3hiIO1"},"outputs":[],"source":["from deep_learning_utils import train_eval_model\n","\n","\n","# hyperparameters for training our model\n","num_epochs = 10         # number of epochs (i.e., passes over the full dataset)\n","batch_size = 16         # batch size: number of data points to group together at a time\n","learning_rate = 0.01    # learning rate: tells by how much parameters are modified w.r.t. gradients\n","weight_decay = 0.001    # l1 regularisation on the model parameters (to prevent overfitting)\n","momentum = 0.9          # integrate gradients from previous steps for more robust descent\n","device = 'cpu'          # GPU acceleration is not really needed for such a small model.\n","                        # However, if you want to use it, switch the Colab\n","                        # runtime to GPU, provide \"cuda\" as device, and re-run the code blocks above.\n","\n","\n","# train and validate our model\n","model, loss_train, loss_val, acc_train, acc_val = train_eval_model(data_train,\n","                                                                   data_val,\n","                                                                   covariate_ids,\n","                                                                   species_names,\n","                                                                   model,       # our model instance\n","                                                                   num_epochs,\n","                                                                   batch_size,\n","                                                                   learning_rate,\n","                                                                   weight_decay,\n","                                                                   momentum,\n","                                                                   device)\n","\n","\n","# visualise loss and accuracy curves over epochs\n","plt.figure()\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(num_epochs), loss_train, 'b-', label='train')\n","plt.plot(np.arange(num_epochs), loss_val, 'r-', label='val')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss value')\n","plt.grid()\n","plt.legend()\n","plt.title('Loss')\n","plt.subplot(1,2,2)\n","plt.plot(np.arange(num_epochs), acc_train, 'b-', label='train')\n","plt.plot(np.arange(num_epochs), acc_val, 'r-', label='val')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy value')\n","plt.grid()\n","plt.legend()\n","plt.title('Accuracy')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pcFBg1OjiIO-"},"source":["#### ❓\n","* Is the model performing well according to the plotted figures you can see? If not, can you think\n","  of what problem may be prevalent, and how to possibly prevent it?\n","* Modify the hyperparameters a bit: increase the learning rate to 0.1. What happens, and why?\n","* Likewise, increase the number of epochs to e.g. 50 (GPU recommended). How much does this help?\n","\n","\n","Once you have a set of hyperparameters you like, you can use that model to predict the test set and\n","map below, as you did for random forests above.\n","\n","Note that we _could_ conduct a hyperparameter grid search with default values like we did above. In\n","practice, this is less done for deep learning models because they are so costly to train."]},{"cell_type":"markdown","metadata":{"id":"3RXuzoCGiIO-"},"source":["## 5.4 Prediction and validation\n","\n","Now that we have trained our model, we can use it to predict our test set, exactly like the random\n","forest above. The basic function for this is also provided and used in the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uU0VuvauiIO-"},"outputs":[],"source":["from deep_learning_utils import predict_model\n","\n","\n","# predict all three sets with final model\n","datasets = {'train': data_train, 'val': data_val, 'test': data_test}\n","scores = {}\n","for split, dataset in datasets.items():\n","    # predict\n","    predictions = predict_model(dataset,\n","                                covariate_ids,\n","                                species_names,\n","                                model,\n","                                batch_size,\n","                                device)\n","\n","    # evaluate\n","    scores[split] = evaluate(predictions, dataset)\n","\n","# combine\n","scores = pd.DataFrame(scores)\n","\n","# plot\n","plt.figure()\n","for sidx, species in enumerate(species_names):\n","    plt.bar(range(sidx, len(species_names)*4, 4), scores.loc[species], label=species)\n","plt.ylabel('score')\n","plt.xticks(range(1, len(species_names)*4, 4), ('train', 'val', 'test'))\n","plt.legend(loc='lower left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"21-9cF77iIO_"},"source":["#### ❓\n","* Compare these results to the accuracy obtained with random forests (Section 4.1 above). Are you\n","  satisfied with these results, resp., did you expect them to be like that?"]},{"cell_type":"markdown","metadata":{"id":"rByj9KaqiIO_"},"source":["## 5.5 Predict map\n","\n","Let us predict and plot points on a map grid like we did for random forests in Section 4.2 above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2IoGCufiIO_"},"outputs":[],"source":["# 1. Make sure to re-run section 4.2 above if you have restarted your runtime with GPU support.\n","\n","# predict map using deep learning model\n","pred_map_dl = predict_model(data_map,\n","                            covariate_ids,\n","                            species_names,\n","                            model,\n","                            batch_size,\n","                            device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJGnnurziIO_"},"outputs":[],"source":["# plot\n","df_map_dl = pd.concat((pred_map_dl, data_map), axis=1)\n","\n","def plot_predictions(species: str,\n","                     plot_observations: bool=False) -> None:\n","    traces = []\n","    fig_pred = px.scatter_mapbox(df_map_dl,\n","                                 lat='lat',\n","                                 lon='lon',\n","                                 title='Predictions',\n","                                 hover_name=species,\n","                                 color=species,\n","                                 height=600,\n","                                 width=800,\n","                                 zoom=2)\n","    traces.append(fig_pred.data[0])\n","    if plot_observations:\n","        fig2 = px.scatter_mapbox(data.loc[(data['species']==species) * \\\n","                                          (data['is_presence']==True)],\n","                                 lat='lat',\n","                                 lon='lon',\n","                                 title='Observations',\n","                                 hover_name='species',\n","                                 color_discrete_sequence=['green'],\n","                                 height=600,\n","                                 width=800,\n","                                 zoom=2)\n","        traces.append(fig2.data[0])\n","    fig = go.FigureWidget(data=traces)\n","    fig.update_layout(mapbox_style='open-street-map')\n","    fig.show()\n","\n","\n","species_wdgt = widgets.Dropdown(options=species_names, description='species:')\n","show_obs_wdgt = widgets.Checkbox(description='show observations')\n","\n","def redraw_figure(*args, **kwargs):\n","    clear_output(wait=True)\n","    display(species_wdgt, show_obs_wdgt, fig)\n","    plot_predictions(species=species_wdgt.value,\n","                     plot_observations=show_obs_wdgt.value)\n","\n","species_wdgt.observe(redraw_figure, names=['value'])\n","show_obs_wdgt.observe(redraw_figure, names=['value'])\n","\n","\n","redraw_figure()"]},{"cell_type":"markdown","metadata":{"id":"e8AW6flgiIO_"},"source":["#### ❓\n","* What patterns can you observe that the deep learning model makes? _Tip:_ compare across species.\n","* Are these predictions what you expected based on _e.g._ the accuracy scores above? Why, resp. why\n","  not?"]}],"metadata":{"kernelspec":{"display_name":"deepSDM","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}